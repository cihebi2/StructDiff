# StructDiff å®Œæ•´é›†æˆå’Œä½¿ç”¨æŒ‡å—

## ç›®å½•

1. [é¡¹ç›®æ¦‚è§ˆ](https://claude.ai/chat/26ea5509-2e87-4974-a214-d245f93cdf52#%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%A7%88)
2. [ç¯å¢ƒè®¾ç½®](https://claude.ai/chat/26ea5509-2e87-4974-a214-d245f93cdf52#%E7%8E%AF%E5%A2%83%E8%AE%BE%E7%BD%AE)
3. [æ•°æ®å‡†å¤‡](https://claude.ai/chat/26ea5509-2e87-4974-a214-d245f93cdf52#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87)
4. [æ¨¡å‹è®­ç»ƒ](https://claude.ai/chat/26ea5509-2e87-4974-a214-d245f93cdf52#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83)
5. [é«˜çº§åŠŸèƒ½ä½¿ç”¨](https://claude.ai/chat/26ea5509-2e87-4974-a214-d245f93cdf52#%E9%AB%98%E7%BA%A7%E5%8A%9F%E8%83%BD%E4%BD%BF%E7%94%A8)
6. [æ€§èƒ½ä¼˜åŒ–](https://claude.ai/chat/26ea5509-2e87-4974-a214-d245f93cdf52#%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96)
7. [å¯è§†åŒ–å’Œåˆ†æ](https://claude.ai/chat/26ea5509-2e87-4974-a214-d245f93cdf52#%E5%8F%AF%E8%A7%86%E5%8C%96%E5%92%8C%E5%88%86%E6%9E%90)
8. [æœ€ä½³å®è·µ](https://claude.ai/chat/26ea5509-2e87-4974-a214-d245f93cdf52#%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5)

## é¡¹ç›®æ¦‚è§ˆ

StructDiff æ˜¯ä¸€ä¸ªå…ˆè¿›çš„åŸºäºæ‰©æ•£æ¨¡å‹çš„è‚½æ®µç”Ÿæˆæ¡†æ¶ï¼Œé›†æˆäº†ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š

* **ESMFold é›†æˆ** ï¼šè‡ªåŠ¨é¢„æµ‹å’Œåˆ©ç”¨ 3D ç»“æ„ä¿¡æ¯
* **LoRA å¾®è°ƒ** ï¼šé«˜æ•ˆçš„å¤§æ¨¡å‹å¾®è°ƒç­–ç•¥
* **å¤šç§é‡‡æ ·æ–¹æ³•** ï¼šDDPMã€DDIMã€PNDMã€DPM-Solverã€EDM ç­‰
* **æ¡ä»¶ç”Ÿæˆ** ï¼šæ”¯æŒå¤šç§çº¦æŸå’Œå±æ€§æ§åˆ¶
* **å†…å­˜ä¼˜åŒ–** ï¼šæ¢¯åº¦ç´¯ç§¯ã€æ··åˆç²¾åº¦ã€é«˜æ•ˆæ•°æ®åŠ è½½
* **å…¨é¢çš„è¯„ä¼°** ï¼šåºåˆ—ã€ç»“æ„ã€åŠŸèƒ½ç­‰å¤šç»´åº¦æŒ‡æ ‡
* **ä¸°å¯Œçš„å¯è§†åŒ–** ï¼šäº¤äº’å¼ä»ªè¡¨æ¿å’Œåˆ†æå·¥å…·

## ç¯å¢ƒè®¾ç½®

### 1. åˆ›å»ºç¯å¢ƒ

```bash
# ä½¿ç”¨ conda
conda env create -f environment.yml
conda activate structdiff

# å®‰è£…é¢å¤–ä¾èµ–
pip install -r requirements-dev.txt
```

### 2. éªŒè¯å®‰è£…

```python
# test_installation.py
import torch
from structdiff import StructDiff
from structdiff.models.esmfold_wrapper import ESMFoldWrapper
from structdiff.models.lora import apply_lora_to_model

print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

# æµ‹è¯• ESMFold
try:
    esmfold = ESMFoldWrapper()
    print("âœ“ ESMFold loaded successfully")
except Exception as e:
    print(f"âœ— ESMFold loading failed: {e}")

print("\nInstallation verified!")
```

## æ•°æ®å‡†å¤‡

### 1. æ•°æ®é¢„å¤„ç†ä¸ç»“æ„é¢„æµ‹

```python
# prepare_data_with_structures.py
from structdiff.data.efficient_loader import CachedPeptideDataset
from structdiff.models.esmfold_wrapper import ESMFoldWrapper
import pandas as pd
from pathlib import Path

# åŠ è½½æ•°æ®
data_path = "data/raw/peptides.csv"
df = pd.read_csv(data_path)

# åˆå§‹åŒ– ESMFold
esmfold = ESMFoldWrapper()

# é¢„æµ‹ç»“æ„å¹¶ç¼“å­˜
cache_dir = Path("data/structure_cache")
cache_dir.mkdir(exist_ok=True)

for idx, row in df.iterrows():
    sequence = row['sequence']
    cache_file = cache_dir / f"{idx}_{sequence[:10]}.pt"
  
    if not cache_file.exists():
        print(f"Predicting structure for {sequence[:20]}...")
        features = esmfold.predict_structure(sequence)
        torch.save(features, cache_file)

# åˆ›å»ºé«˜æ•ˆæ•°æ®é›†
dataset = CachedPeptideDataset(
    data_path,
    cache_dir="data/cache",
    use_mmap=True,
    precompute_features=True,
    num_workers=8
)

print(f"Dataset prepared with {len(dataset)} samples")
```

### 2. æ•°æ®åˆ†æå’Œå¯è§†åŒ–

```python
# analyze_dataset.py
from structdiff.visualization import PeptideVisualizer
import pandas as pd

# åŠ è½½æ•°æ®
df = pd.read_csv("data/processed/train.csv")
sequences = df['sequence'].tolist()

# åˆ›å»ºå¯è§†åŒ–å™¨
viz = PeptideVisualizer(style='publication')

# ç”Ÿæˆåˆ†ææŠ¥å‘Š
viz.plot_property_distribution(sequences, save_path="analysis/properties.png")
viz.plot_sequence_logo(sequences[:100], save_path="analysis/logo.png")
viz.create_summary_report(
    sequences,
    metrics={'diversity': 0.85, 'novelty': 0.92},
    output_path="analysis/dataset_report.html"
)
```

## æ¨¡å‹è®­ç»ƒ

### 1. ä½¿ç”¨ LoRA å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹

```python
# train_with_lora.py
import torch
from omegaconf import OmegaConf
from structdiff.models.structdiff import StructDiff
from structdiff.models.lora import apply_lora_to_model, save_lora_weights
from structdiff.training.gradient_accumulation import DynamicGradientAccumulator
from structdiff.utils.memory_optimization import MemoryMonitor, optimize_model_memory

# åŠ è½½é…ç½®
config = OmegaConf.load("configs/lora_training.yaml")

# åˆ›å»ºæ¨¡å‹
model = StructDiff(config)

# åº”ç”¨ LoRA
lora_modules = apply_lora_to_model(
    model.sequence_encoder,
    target_modules=['query', 'key', 'value', 'dense'],
    rank=16,
    alpha=32,
    dropout=0.1
)

# å†…å­˜ä¼˜åŒ–
model = optimize_model_memory(model)

# è®¾ç½®åŠ¨æ€æ¢¯åº¦ç´¯ç§¯
optimizer = torch.optim.AdamW(
    filter(lambda p: p.requires_grad, model.parameters()),
    lr=config.training.lr
)

accumulator = DynamicGradientAccumulator(
    model=model,
    optimizer=optimizer,
    target_batch_size=128,  # ç›®æ ‡æ‰¹é‡å¤§å°
    min_accumulation_steps=1,
    max_accumulation_steps=8,
    memory_efficient=True
)

# å†…å­˜ç›‘æ§
monitor = MemoryMonitor()

# è®­ç»ƒå¾ªç¯
for epoch in range(config.training.num_epochs):
    for batch in train_loader:
        with monitor.track_memory(f"epoch_{epoch}"):
            # åŠ¨æ€è°ƒæ•´ç´¯ç§¯æ­¥æ•°
            accumulator.adjust_accumulation_steps(batch['sequences'].shape[0])
          
            # è®­ç»ƒæ­¥éª¤
            outputs = model(**batch, return_loss=True)
          
            # æ¢¯åº¦ç´¯ç§¯
            step_performed = accumulator.backward(outputs['total_loss'])
          
            if step_performed:
                print(f"Step completed, loss: {outputs['total_loss'].item():.4f}")
  
    # ä¿å­˜ LoRA æƒé‡
    if epoch % 10 == 0:
        save_lora_weights(
            model,
            f"checkpoints/lora_epoch_{epoch}.pth",
            additional_state={'epoch': epoch, 'config': config}
        )

print("Training completed!")
```

### 2. ä½¿ç”¨é«˜çº§è¯„ä¼°æŒ‡æ ‡

```python
# evaluate_model.py
from structdiff.metrics.advanced_metrics import compute_comprehensive_metrics
from structdiff.models.structdiff import StructDiff
import torch

# åŠ è½½æ¨¡å‹
model = StructDiff.from_pretrained("checkpoints/best_model.pth")
model.eval()

# ç”Ÿæˆæ ·æœ¬
with torch.no_grad():
    samples = model.sample(
        batch_size=100,
        seq_length=20,
        conditions={'peptide_type': torch.tensor([0] * 100)},  # æŠ—èŒè‚½
        guidance_scale=2.0,
        sampling_method='dpm_solver',
        num_inference_steps=20
    )

generated_sequences = samples['sequences']

# è®¡ç®—ç»¼åˆæŒ‡æ ‡
metrics = compute_comprehensive_metrics(
    generated=generated_sequences,
    reference=reference_sequences,
    structures=[s['structures'] for s in samples],
    peptide_type='antimicrobial'
)

# æ‰“å°è¯¦ç»†ç»“æœ
print("\n=== Evaluation Results ===")
for category in ['sequence', 'structure', 'function', 'diversity']:
    print(f"\n{category.upper()} METRICS:")
    for metric, value in metrics.items():
        if category in metric:
            print(f"  {metric}: {value:.4f}")
```

## é«˜çº§åŠŸèƒ½ä½¿ç”¨

### 1. æ¡ä»¶ç”Ÿæˆä¸çº¦æŸ

```python
# conditional_generation_example.py
from structdiff.generation.conditional_generation import (
    ConditionalGenerator, GenerationConstraints
)

# åˆ›å»ºæ¡ä»¶ç”Ÿæˆå™¨
generator = ConditionalGenerator(model, device)

# å®šä¹‰ç”Ÿæˆçº¦æŸ
constraints = GenerationConstraints(
    # åºåˆ—çº¦æŸ
    min_length=15,
    max_length=25,
    required_motifs=['KK', 'RR'],  # å¿…é¡»åŒ…å«çš„æ¨¡ä½“
    forbidden_motifs=['PPP'],      # ç¦æ­¢çš„æ¨¡ä½“
    fixed_positions={0: 'M', -1: 'K'},  # å›ºå®šä½ç½®çš„æ°¨åŸºé…¸
  
    # ç»„æˆçº¦æŸ
    min_charge=3,
    max_charge=8,
    min_hydrophobicity=0.2,
    max_hydrophobicity=0.6,
    required_amino_acids={'K': 2, 'R': 2},  # æœ€å°‘åŒ…å«çš„æ°¨åŸºé…¸
  
    # ç»“æ„çº¦æŸ
    min_helix_content=0.4,
    max_helix_content=0.7,
  
    # åŠŸèƒ½çº¦æŸ
    peptide_type='antimicrobial',
    target_activity_score=0.8
)

# ç”Ÿæˆæ»¡è¶³çº¦æŸçš„è‚½æ®µ
peptides = generator.generate_with_constraints(
    num_samples=50,
    constraints=constraints,
    guidance_scale=3.0,
    temperature=0.8,
    rejection_sampling=False  # ä½¿ç”¨å¼•å¯¼ç”Ÿæˆè€Œéæ‹’ç»é‡‡æ ·
)

print(f"Generated {len(peptides)} peptides meeting all constraints")
```

### 2. ä½¿ç”¨å¤šç§é‡‡æ ·æ–¹æ³•

```python
# advanced_sampling_example.py
from structdiff.sampling.advanced_samplers import (
    DPMSolver, EDMSampler, ConsistencyModel, LatentConsistencyModel
)

# DPM-Solver (å¿«é€Ÿé«˜è´¨é‡)
dpm_solver = DPMSolver(model.diffusion, algorithm="dpmsolver++", order=2)
samples_dpm = dpm_solver.sample(
    model, shape=(10, 20, 768),
    num_inference_steps=20,  # ä»…éœ€ 20 æ­¥
    conditions=conditions
)

# EDM é‡‡æ ·å™¨ (æœ€é«˜è´¨é‡)
edm_sampler = EDMSampler(model.diffusion)
samples_edm = edm_sampler.sample(
    model, shape=(10, 20, 768),
    num_inference_steps=50,
    s_churn=40,  # éšæœºæ€§æ§åˆ¶
    s_noise=1.003
)

# ä¸€è‡´æ€§æ¨¡å‹ (è¶…å¿«é€Ÿ)
consistency_model = LatentConsistencyModel(model, num_inference_steps=4)
samples_lcm = consistency_model.sample_lcm(
    shape=(10, 20, 768),
    conditions=conditions,
    guidance_scale=8.0,
    num_steps=4  # ä»…éœ€ 4 æ­¥ï¼
)

# æ¯”è¾ƒç”Ÿæˆæ—¶é—´å’Œè´¨é‡
import time

methods = {
    'DDPM (1000 steps)': lambda: model.sample(10, 20, sampling_method='ddpm'),
    'DDIM (50 steps)': lambda: model.sample(10, 20, sampling_method='ddim', num_inference_steps=50),
    'DPM-Solver++ (20 steps)': lambda: dpm_solver.sample(model, (10, 20, 768), num_inference_steps=20),
    'LCM (4 steps)': lambda: consistency_model.sample_lcm((10, 20, 768), num_steps=4)
}

for name, method in methods.items():
    start = time.time()
    samples = method()
    elapsed = time.time() - start
    print(f"{name}: {elapsed:.2f}s")
```

### 3. å±æ€§å¼•å¯¼ç”Ÿæˆ

```python
# property_guided_generation.py
from structdiff.generation.conditional_generation import (
    ConditionalSampler, PropertyPredictor
)

# è®­ç»ƒå±æ€§é¢„æµ‹å™¨ï¼ˆå‡è®¾å·²è®­ç»ƒï¼‰
property_predictor = PropertyPredictor(
    input_dim=model.config.model.hidden_dim,
    num_properties=3  # charge, hydrophobicity, helix_content
)
property_predictor.load_state_dict(torch.load("models/property_predictor.pth"))

# åˆ›å»ºæ¡ä»¶é‡‡æ ·å™¨
sampler = ConditionalSampler(model, property_predictor)

# ç”Ÿæˆå…·æœ‰ç‰¹å®šå±æ€§çš„è‚½æ®µ
target_properties = {
    'charge': 5.0,
    'hydrophobicity': 0.4,
    'helix_content': 0.6
}

peptides = sampler.sample_with_property_control(
    num_samples=20,
    target_properties=target_properties,
    property_weights={'charge': 2.0, 'hydrophobicity': 1.0},
    num_iterations=100,
    learning_rate=0.1
)

# éªŒè¯ç”Ÿæˆçš„å±æ€§
for peptide in peptides[:5]:
    props = compute_properties(peptide)
    print(f"{peptide}: charge={props['charge']:.2f}, "
          f"hydro={props['hydrophobicity']:.2f}, "
          f"helix={props['helix_content']:.2f}")
```

## æ€§èƒ½ä¼˜åŒ–

### 1. å†…å­˜ä¼˜åŒ–æœ€ä½³å®è·µ

```python
# memory_optimized_training.py
from structdiff.utils.memory_optimization import (
    MemoryEfficientTrainer, clear_memory, get_model_memory_footprint
)

# åˆ›å»ºå†…å­˜é«˜æ•ˆè®­ç»ƒå™¨
trainer = MemoryEfficientTrainer(
    model=model,
    config=config,
    auto_batch_size=True,
    auto_accumulation=True
)

# è‡ªåŠ¨æ‰¾åˆ°æœ€ä¼˜æ‰¹é‡å¤§å°
optimal_batch_size = trainer.find_optimal_batch_size(
    train_loader,
    min_batch_size=4,
    max_batch_size=128
)
print(f"Optimal batch size: {optimal_batch_size}")

# è·å–æ¨¡å‹å†…å­˜å ç”¨
memory_info = get_model_memory_footprint(model)
print(f"Model memory: {memory_info['total_memory_mb']:.2f} MB")

# è®­ç»ƒæ—¶å®šæœŸæ¸…ç†å†…å­˜
for epoch in range(num_epochs):
    for i, batch in enumerate(train_loader):
        # è®­ç»ƒæ­¥éª¤
        loss = train_step(model, batch, optimizer)
      
        # æ¯ 100 æ­¥æ¸…ç†ä¸€æ¬¡å†…å­˜
        if i % 100 == 0:
            clear_memory()
            print(f"Memory cleared at step {i}")
```

### 2. æ•°æ®åŠ è½½ä¼˜åŒ–

```python
# optimized_data_loading.py
from structdiff.data.efficient_loader import (
    LMDBPeptideDataset, StreamingPeptideDataset, DataPipelineOptimizer
)

# ä½¿ç”¨ LMDB è·å¾—è¶…å¿«é€Ÿåº¦
lmdb_dataset = LMDBPeptideDataset(
    data_path="data/train.csv",
    lmdb_path="data/train.lmdb",
    map_size=50 * 1024 * 1024 * 1024  # 50GB
)

# æµå¼å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†
streaming_dataset = StreamingPeptideDataset(
    data_files=["data/shard_001.jsonl", "data/shard_002.jsonl"],
    buffer_size=10000,
    shuffle=True
)

# ä¼˜åŒ–æ•°æ®ç®¡é“
optimizer = DataPipelineOptimizer(config)
train_loader = optimizer.create_optimized_dataloader(
    lmdb_dataset,
    batch_size=optimal_batch_size,
    num_workers=8
)

# åŸºå‡†æµ‹è¯•
benchmark_results = optimizer.benchmark_dataloader(train_loader)
print(f"Throughput: {benchmark_results['throughput']:.2f} batches/sec")
```

## å¯è§†åŒ–å’Œåˆ†æ

### 1. åˆ›å»ºäº¤äº’å¼ä»ªè¡¨æ¿

```python
# create_dashboard.py
from structdiff.visualization import InteractivePeptideExplorer
import torch

# ç”Ÿæˆè‚½æ®µå’ŒåµŒå…¥
with torch.no_grad():
    samples = model.sample(
        batch_size=500,
        seq_length=20,
        return_embeddings=True
    )

sequences = samples['sequences']
embeddings = samples['embeddings'].mean(dim=1)  # æ± åŒ–

# åˆ›å»ºäº¤äº’å¼æ¢ç´¢å™¨
explorer = InteractivePeptideExplorer(sequences, embeddings)
app = explorer.create_dashboard()

# è¿è¡Œä»ªè¡¨æ¿
if __name__ == '__main__':
    app.run_server(debug=True, port=8050)
```

### 2. ç”Ÿæˆç»¼åˆæŠ¥å‘Š

```python
# generate_report.py
from structdiff.visualization import PeptideVisualizer
from structdiff.metrics.advanced_metrics import compute_comprehensive_metrics

# ç”Ÿæˆå¤§é‡æ ·æœ¬è¿›è¡Œåˆ†æ
sequences = []
conditions_list = [
    {'peptide_type': 0, 'name': 'antimicrobial'},
    {'peptide_type': 1, 'name': 'antifungal'},
    {'peptide_type': 2, 'name': 'antiviral'}
]

for cond in conditions_list:
    samples = model.sample(
        batch_size=100,
        seq_length=20,
        conditions={'peptide_type': torch.tensor([cond['peptide_type']] * 100)},
        guidance_scale=2.0
    )
    sequences.extend([(seq, cond['name']) for seq in samples['sequences']])

# åˆ†ç¦»åºåˆ—å’Œæ ‡ç­¾
seqs, labels = zip(*sequences)

# åˆ›å»ºå¯è§†åŒ–
viz = PeptideVisualizer(style='publication')

# åµŒå…¥ç©ºé—´å¯è§†åŒ–
embeddings = compute_embeddings(seqs)  # ä½¿ç”¨æ¨¡å‹è®¡ç®—åµŒå…¥
viz.plot_embedding_space(
    embeddings,
    labels=labels,
    method='tsne',
    save_path="results/embedding_space.png"
)

# ç›¸ä¼¼æ€§ç½‘ç»œ
viz.plot_sequence_similarity_network(
    seqs[:100],
    labels=labels[:100],
    similarity_threshold=0.6,
    save_path="results/similarity_network.png"
)

# ç”Ÿæˆ HTML æŠ¥å‘Š
metrics = compute_comprehensive_metrics(seqs)
viz.create_summary_report(
    seqs,
    metrics,
    output_path="results/generation_report.html"
)
```

## æœ€ä½³å®è·µ

### 1. å®éªŒç®¡ç†

```python
# experiment_management.py
import wandb
from pathlib import Path
import json

class ExperimentManager:
    def __init__(self, project_name="StructDiff"):
        self.project_name = project_name
        self.exp_dir = Path("experiments")
        self.exp_dir.mkdir(exist_ok=True)
  
    def start_experiment(self, config, tags=None):
        # åˆå§‹åŒ– wandb
        wandb.init(
            project=self.project_name,
            config=config,
            tags=tags or []
        )
      
        # åˆ›å»ºå®éªŒç›®å½•
        exp_name = wandb.run.name
        self.current_exp_dir = self.exp_dir / exp_name
        self.current_exp_dir.mkdir(exist_ok=True)
      
        # ä¿å­˜é…ç½®
        with open(self.current_exp_dir / "config.json", 'w') as f:
            json.dump(config, f, indent=2)
      
        return exp_name
  
    def log_metrics(self, metrics, step=None):
        wandb.log(metrics, step=step)
  
    def save_checkpoint(self, model, optimizer, epoch, metrics):
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'metrics': metrics
        }
      
        path = self.current_exp_dir / f"checkpoint_epoch_{epoch}.pth"
        torch.save(checkpoint, path)
      
        # ä¸Šä¼ åˆ° wandb
        wandb.save(str(path))
  
    def finish_experiment(self):
        wandb.finish()

# ä½¿ç”¨ç¤ºä¾‹
exp_manager = ExperimentManager()
exp_name = exp_manager.start_experiment(
    config=config_dict,
    tags=['lora', 'esmfold', 'antimicrobial']
)

for epoch in range(num_epochs):
    # è®­ç»ƒ
    train_metrics = train_epoch(model, train_loader, optimizer)
    val_metrics = validate(model, val_loader)
  
    # è®°å½•æŒ‡æ ‡
    exp_manager.log_metrics({
        'train_loss': train_metrics['loss'],
        'val_loss': val_metrics['loss'],
        'val_perplexity': val_metrics['perplexity']
    }, step=epoch)
  
    # ä¿å­˜æ£€æŸ¥ç‚¹
    if epoch % 10 == 0:
        exp_manager.save_checkpoint(model, optimizer, epoch, val_metrics)

exp_manager.finish_experiment()
```

### 2. ç”Ÿäº§éƒ¨ç½²

```python
# deployment.py
import torch
from typing import List
import asyncio
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

class GenerationRequest(BaseModel):
    num_samples: int = 10
    peptide_type: str = "antimicrobial"
    min_length: int = 15
    max_length: int = 25
    guidance_scale: float = 2.0
    temperature: float = 1.0

class GenerationResponse(BaseModel):
    sequences: List[str]
    scores: List[float]
    generation_time: float

# åˆ›å»º API
app = FastAPI(title="StructDiff API")

# åŠ è½½æ¨¡å‹ï¼ˆå…¨å±€ï¼‰
model = torch.jit.load("models/structdiff_optimized.pt")
model.eval()

@app.post("/generate", response_model=GenerationResponse)
async def generate_peptides(request: GenerationRequest):
    """ç”Ÿæˆè‚½æ®µçš„ API ç«¯ç‚¹"""
    import time
  
    start_time = time.time()
  
    try:
        # å‡†å¤‡æ¡ä»¶
        type_map = {"antimicrobial": 0, "antifungal": 1, "antiviral": 2}
        peptide_type_id = type_map.get(request.peptide_type, 0)
      
        # å¼‚æ­¥ç”Ÿæˆ
        loop = asyncio.get_event_loop()
        samples = await loop.run_in_executor(
            None,
            lambda: model.sample(
                batch_size=request.num_samples,
                seq_length=(request.min_length + request.max_length) // 2,
                conditions={'peptide_type': torch.tensor([peptide_type_id] * request.num_samples)},
                guidance_scale=request.guidance_scale,
                temperature=request.temperature
            )
        )
      
        generation_time = time.time() - start_time
      
        return GenerationResponse(
            sequences=samples['sequences'],
            scores=samples.get('scores', [1.0] * len(samples['sequences'])),
            generation_time=generation_time
        )
  
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    return {"status": "healthy", "model_loaded": model is not None}

# è¿è¡ŒæœåŠ¡å™¨
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 3. æ¨¡å‹ä¼˜åŒ–å’Œå¯¼å‡º

```python
# optimize_and_export.py
import torch
from torch.quantization import quantize_dynamic

# ä¼˜åŒ–æ¨¡å‹ç”¨äºæ¨ç†
def optimize_for_inference(model):
    """ä¼˜åŒ–æ¨¡å‹ä»¥æé«˜æ¨ç†é€Ÿåº¦"""
    model.eval()
  
    # 1. èåˆæ“ä½œ
    model = torch.jit.script(model)
  
    # 2. é‡åŒ–ï¼ˆå¯é€‰ï¼‰
    quantized_model = quantize_dynamic(
        model,
        {torch.nn.Linear, torch.nn.Conv1d},
        dtype=torch.qint8
    )
  
    # 3. ä¼˜åŒ–å›¾
    optimized_model = torch.jit.optimize_for_inference(quantized_model)
  
    return optimized_model

# å¯¼å‡ºæ¨¡å‹
def export_model(model, export_path="models/structdiff_optimized.pt"):
    """å¯¼å‡ºä¼˜åŒ–åçš„æ¨¡å‹"""
    # åˆ›å»ºç¤ºä¾‹è¾“å…¥
    example_inputs = {
        'batch_size': 1,
        'seq_length': 20,
        'conditions': {'peptide_type': torch.tensor([0])},
        'guidance_scale': 2.0
    }
  
    # è¿½è¸ªæ¨¡å‹
    traced_model = torch.jit.trace(
        model.sample,
        example_inputs=example_inputs
    )
  
    # ä¿å­˜
    torch.jit.save(traced_model, export_path)
    print(f"Model exported to {export_path}")

# åŸºå‡†æµ‹è¯•
def benchmark_inference(original_model, optimized_model):
    """æ¯”è¾ƒåŸå§‹å’Œä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½"""
    import time
  
    # é¢„çƒ­
    for _ in range(5):
        _ = optimized_model.sample(1, 20)
  
    # æµ‹è¯•
    n_runs = 100
  
    # åŸå§‹æ¨¡å‹
    start = time.time()
    for _ in range(n_runs):
        _ = original_model.sample(1, 20)
    original_time = (time.time() - start) / n_runs
  
    # ä¼˜åŒ–æ¨¡å‹
    start = time.time()
    for _ in range(n_runs):
        _ = optimized_model.sample(1, 20)
    optimized_time = (time.time() - start) / n_runs
  
    print(f"Original model: {original_time*1000:.2f} ms/sample")
    print(f"Optimized model: {optimized_time*1000:.2f} ms/sample")
    print(f"Speedup: {original_time/optimized_time:.2f}x")
```

## ç»“è¯­

StructDiff ç°åœ¨æ˜¯ä¸€ä¸ªåŠŸèƒ½å®Œæ•´ã€æ€§èƒ½ä¼˜åŒ–ã€æ˜“äºä½¿ç”¨çš„è‚½æ®µç”Ÿæˆæ¡†æ¶ã€‚é€šè¿‡é›†æˆ ESMFoldã€LoRAã€é«˜çº§é‡‡æ ·æ–¹æ³•å’Œå…¨é¢çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå®ƒèƒ½å¤Ÿæ»¡è¶³ä»ç ”ç©¶åˆ°ç”Ÿäº§çš„å„ç§éœ€æ±‚ã€‚

ä¸»è¦ç‰¹ç‚¹ï¼š

* ğŸš€  **é«˜æ€§èƒ½** ï¼šå¤šç§ä¼˜åŒ–æŠ€æœ¯ï¼Œæ”¯æŒå¤§è§„æ¨¡ç”Ÿæˆ
* ğŸ¯  **ç²¾ç¡®æ§åˆ¶** ï¼šä¸°å¯Œçš„æ¡ä»¶ç”Ÿæˆé€‰é¡¹
* ğŸ“Š  **å…¨é¢è¯„ä¼°** ï¼šå¤šç»´åº¦çš„è¯„ä¼°æŒ‡æ ‡
* ğŸ¨  **ç›´è§‚å¯è§†åŒ–** ï¼šäº¤äº’å¼åˆ†æå·¥å…·
* ğŸ”§  **æ˜“äºæ‰©å±•** ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼Œä¾¿äºå®šåˆ¶

ç»§ç»­å¼€å‘å»ºè®®ï¼š

1. é›†æˆæ›´å¤šé¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚ ESM-3ã€AlphaFoldï¼‰
2. æ·»åŠ å¼ºåŒ–å­¦ä¹ å¾®è°ƒ
3. å®ç°å¤šæ¨¡æ€ç”Ÿæˆï¼ˆåºåˆ—+ç»“æ„è”åˆç”Ÿæˆï¼‰
4. å¼€å‘ Web ç•Œé¢å’Œäº‘æœåŠ¡
5. æ„å»ºè‚½æ®µè®¾è®¡ä¸“ç”¨çš„åŸºå‡†æ•°æ®é›†

å¸Œæœ›è¿™ä¸ªå®Œæ•´çš„å®ç°èƒ½å¸®åŠ©æ‚¨å……åˆ†åˆ©ç”¨ StructDiff çš„æ‰€æœ‰åŠŸèƒ½ï¼

# Updated: 05/31/2025 15:11:07

# Updated: 05/31/2025 15:14:04

# Updated: 05/31/2025 23:30:00

# Updated: 05/31/2025 23:30:18
