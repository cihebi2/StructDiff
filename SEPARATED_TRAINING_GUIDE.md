# CPL-Diffå¯å‘çš„åˆ†ç¦»å¼è®­ç»ƒç­–ç•¥

## æ¦‚è¿°

åŸºäºCPL-Diffè®ºæ–‡çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬å®ç°äº†åˆ†ç¦»å¼è®­ç»ƒç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†StructDiffçš„è®­ç»ƒç¨³å®šæ€§å’Œç”Ÿæˆè´¨é‡ã€‚è¿™ç§ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•å°†å¤æ‚çš„ç«¯åˆ°ç«¯è®­ç»ƒåˆ†è§£ä¸ºæ›´ç®€å•ã€æ›´ç¨³å®šçš„å­ä»»åŠ¡ã€‚

## ğŸ¯ æ ¸å¿ƒè®¾è®¡æ€æƒ³

### ä¼ ç»Ÿç«¯åˆ°ç«¯è®­ç»ƒçš„é—®é¢˜
- **ä¼˜åŒ–å¤æ‚åº¦é«˜**ï¼šåŒæ—¶è®­ç»ƒç¼–ç å™¨ã€å»å™ªå™¨å’Œè§£ç å™¨
- **è®­ç»ƒä¸ç¨³å®š**ï¼šæ¢¯åº¦å†²çªå’Œæ”¶æ•›å›°éš¾
- **è®¡ç®—èµ„æºæµªè´¹**ï¼šé‡å¤è®¡ç®—ç¼–ç å™¨è¾“å‡º

### åˆ†ç¦»å¼è®­ç»ƒçš„ä¼˜åŠ¿
- **é™ä½å¤æ‚åº¦**ï¼šåˆ†é˜¶æ®µä¼˜åŒ–ä¸åŒç»„ä»¶
- **æå‡ç¨³å®šæ€§**ï¼šæ¯ä¸ªé˜¶æ®µç›®æ ‡æ˜ç¡®
- **æé«˜æ•ˆç‡**ï¼šå›ºå®šç¼–ç å™¨é¿å…é‡å¤è®¡ç®—
- **æ›´å¥½æ”¶æ•›**ï¼šåˆ†è§£ä¼˜åŒ–é—®é¢˜

## ğŸ“‹ ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹

### é˜¶æ®µ1ï¼šå»å™ªå™¨è®­ç»ƒ
```python
# å›ºå®šESMç¼–ç å™¨å‚æ•°
for param in model.sequence_encoder.parameters():
    param.requires_grad = False

# è·å–å›ºå®šçš„åºåˆ—åµŒå…¥
with torch.no_grad():
    seq_embeddings = model.sequence_encoder(sequences, attention_mask)

# è®­ç»ƒå»å™ªå™¨é¢„æµ‹å™ªå£°
noise = torch.randn_like(seq_embeddings)
noisy_embeddings = diffusion.q_sample(seq_embeddings, timesteps, noise)
predicted_noise = model.denoiser(noisy_embeddings, timesteps, conditions)
loss = F.mse_loss(predicted_noise, noise)
```

### é˜¶æ®µ2ï¼šè§£ç å™¨è®­ç»ƒ
```python
# å›ºå®šå»å™ªå™¨å’Œç¼–ç å™¨
for name, param in model.named_parameters():
    if 'sequence_decoder' not in name:
        param.requires_grad = False

# ä½¿ç”¨å¹²å‡€åµŒå…¥è®­ç»ƒè§£ç å™¨
with torch.no_grad():
    seq_embeddings = model.sequence_encoder(sequences, attention_mask)

logits = model.sequence_decoder(seq_embeddings, attention_mask)
loss = F.cross_entropy(logits.view(-1, vocab_size), sequences.view(-1))
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€ä½¿ç”¨

```bash
# å®Œæ•´ä¸¤é˜¶æ®µè®­ç»ƒ
python scripts/train_separated.py \
    --config configs/separated_training.yaml \
    --data-dir ./data/processed \
    --output-dir ./outputs/separated_training

# åªè®­ç»ƒé˜¶æ®µ1
python scripts/train_separated.py \
    --stage 1 \
    --stage1-epochs 200 \
    --batch-size 32

# åªè®­ç»ƒé˜¶æ®µ2ï¼ˆéœ€è¦é˜¶æ®µ1æ£€æŸ¥ç‚¹ï¼‰
python scripts/train_separated.py \
    --stage 2 \
    --stage1-checkpoint ./checkpoints/stage1_final.pth \
    --stage2-epochs 100
```

### 2. é«˜çº§é…ç½®

```bash
# å¯ç”¨æ‰€æœ‰å¢å¼ºåŠŸèƒ½
python scripts/train_separated.py \
    --use-cfg \                    # åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼
    --use-length-control \         # é•¿åº¦æ§åˆ¶
    --use-amp \                    # æ··åˆç²¾åº¦è®­ç»ƒ
    --use-ema \                    # æŒ‡æ•°ç§»åŠ¨å¹³å‡
    --stage1-lr 1e-4 \
    --stage2-lr 5e-5
```

### 3. è°ƒè¯•æ¨¡å¼

```bash
# å¹²è¿è¡Œï¼ˆæ£€æŸ¥é…ç½®ï¼‰
python scripts/train_separated.py --dry-run

# è°ƒè¯•æ¨¡å¼
python scripts/train_separated.py --debug --stage1-epochs 1
```

## ğŸ“Š é…ç½®æ–‡ä»¶è¯¦è§£

### æ ¸å¿ƒé…ç½®ç»“æ„

```yaml
# configs/separated_training.yaml
separated_training:
  stage1:
    epochs: 200
    batch_size: 32
    learning_rate: 1e-4
    
  stage2:
    epochs: 100
    batch_size: 64
    learning_rate: 5e-5

length_control:
  enabled: true
  min_length: 5
  max_length: 50
  type_specific_lengths:
    antimicrobial: [20, 8]    # [mean, std]
    antifungal: [25, 10]
    antiviral: [30, 12]

classifier_free_guidance:
  enabled: true
  dropout_prob: 0.1
  guidance_scale: 2.0
```

## ğŸ”§ API ä½¿ç”¨

### 1. ç¼–ç¨‹æ¥å£

```python
from structdiff.training.separated_training import (
    SeparatedTrainingManager, SeparatedTrainingConfig
)

# åˆ›å»ºé…ç½®
config = SeparatedTrainingConfig(
    stage1_epochs=200,
    stage2_epochs=100,
    use_cfg=True,
    use_length_control=True
)

# åˆ›å»ºè®­ç»ƒç®¡ç†å™¨
trainer = SeparatedTrainingManager(
    config=config,
    model=model,
    diffusion=diffusion,
    device='cuda'
)

# æ‰§è¡Œè®­ç»ƒ
stats = trainer.run_complete_training(train_loader, val_loader)
```

### 2. é•¿åº¦æ§åˆ¶

```python
from structdiff.training.length_controller import create_length_controller_from_data

# ä»æ•°æ®åˆ›å»ºé•¿åº¦æ§åˆ¶å™¨
controller = create_length_controller_from_data(
    data_path="./data/processed/train.csv",
    min_length=5,
    max_length=50
)

# é‡‡æ ·ç›®æ ‡é•¿åº¦
target_lengths = controller.sample_target_lengths(
    batch_size=32,
    peptide_types=['antimicrobial', 'antifungal']
)
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### å†…å­˜ä¼˜åŒ–
```python
# æ¢¯åº¦ç´¯ç§¯
config.gradient_accumulation_steps = 4

# æ··åˆç²¾åº¦è®­ç»ƒ
config.use_amp = True

# æ£€æŸ¥ç‚¹æœºåˆ¶
config.gradient_checkpointing = True
```

### é€Ÿåº¦ä¼˜åŒ–
```python
# æ•°æ®åŠ è½½ä¼˜åŒ–
config.num_workers = 8
config.pin_memory = True
config.prefetch_factor = 4

# ç¼–è¯‘ä¼˜åŒ–ï¼ˆPyTorch 2.0+ï¼‰
model = torch.compile(model)
```

## ğŸ›ï¸ è¶…å‚æ•°è°ƒä¼˜æŒ‡å—

### é˜¶æ®µ1ï¼ˆå»å™ªå™¨è®­ç»ƒï¼‰
```yaml
stage1:
  learning_rate: 1e-4        # ä¸»è¦è¶…å‚æ•°
  batch_size: 32             # æ ¹æ®GPUå†…å­˜è°ƒæ•´
  epochs: 200                # ç›´åˆ°éªŒè¯æŸå¤±ç¨³å®š
  gradient_clip: 1.0         # é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
```

### é˜¶æ®µ2ï¼ˆè§£ç å™¨è®­ç»ƒï¼‰
```yaml
stage2:
  learning_rate: 5e-5        # é€šå¸¸æ¯”é˜¶æ®µ1å°
  batch_size: 64             # å¯ä»¥æ›´å¤§
  epochs: 100                # é€šå¸¸æ¯”é˜¶æ®µ1å°‘
  gradient_clip: 0.5         # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
```

### CFGå‚æ•°
```yaml
classifier_free_guidance:
  dropout_prob: 0.1          # 10-15%ä¸ºæœ€ä½³
  guidance_scale: 2.0        # 1.5-3.0èŒƒå›´å†…è°ƒæ•´
```

## ğŸ“Š ç›‘æ§å’Œè¯„ä¼°

### è®­ç»ƒç›‘æ§
```python
# å…³é”®æŒ‡æ ‡
- stage1_loss: å»å™ªå™¨MSEæŸå¤±
- stage2_loss: è§£ç å™¨äº¤å‰ç†µæŸå¤±
- learning_rate: å­¦ä¹ ç‡è°ƒåº¦
- gradient_norm: æ¢¯åº¦èŒƒæ•°
```

### æ¨¡å‹è¯„ä¼°
```python
# CPL-Diffæ ‡å‡†è¯„ä¼°
from scripts.cpldiff_standard_evaluation import CPLDiffStandardEvaluator

evaluator = CPLDiffStandardEvaluator()
results = evaluator.comprehensive_cpldiff_evaluation(
    generated_sequences=generated,
    reference_sequences=references,
    peptide_type='antimicrobial'
)
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **é˜¶æ®µ1æŸå¤±ä¸æ”¶æ•›**
   ```python
   # è§£å†³æ–¹æ¡ˆï¼šé™ä½å­¦ä¹ ç‡ï¼Œå¢åŠ warmup
   config.stage1_lr = 5e-5
   config.stage1_warmup_steps = 2000
   ```

2. **é˜¶æ®µ2è¿‡æ‹Ÿåˆ**
   ```python
   # è§£å†³æ–¹æ¡ˆï¼šå¢åŠ æ­£åˆ™åŒ–ï¼Œå‡å°‘epoch
   config.stage2_epochs = 50
   config.weight_decay = 0.01
   ```

3. **å†…å­˜ä¸è¶³**
   ```python
   # è§£å†³æ–¹æ¡ˆï¼šå‡å°æ‰¹æ¬¡å¤§å°ï¼Œå¯ç”¨æ¢¯åº¦ç´¯ç§¯
   config.stage1_batch_size = 16
   config.gradient_accumulation_steps = 2
   ```

4. **é•¿åº¦æ§åˆ¶ä¸ç”Ÿæ•ˆ**
   ```python
   # æ£€æŸ¥é•¿åº¦åˆ†å¸ƒæ•°æ®
   python -c "
   from structdiff.training.length_controller import LengthDistributionAnalyzer
   analyzer = LengthDistributionAnalyzer('./data/processed/train.csv')
   analyzer.analyze_training_data()
   "
   ```

### è°ƒè¯•å·¥å…·

```bash
# è¿è¡Œæµ‹è¯•å¥—ä»¶
python test_separated_training.py

# æ£€æŸ¥é…ç½®
python scripts/train_separated.py --dry-run --debug

# éªŒè¯æ•°æ®åŠ è½½
python -c "
from scripts.train_separated import create_data_loaders
# ... æµ‹è¯•æ•°æ®åŠ è½½é€»è¾‘
"
```

## ğŸ“‹ æ£€æŸ¥æ¸…å•

### è®­ç»ƒå‰æ£€æŸ¥
- [ ] æ•°æ®æ ¼å¼æ­£ç¡®ï¼ˆCSVåŒ…å«sequenceå’Œpeptide_typeåˆ—ï¼‰
- [ ] é…ç½®æ–‡ä»¶è·¯å¾„æ­£ç¡®
- [ ] GPUå†…å­˜å……è¶³
- [ ] ä¾èµ–é¡¹å·²å®‰è£…

### è®­ç»ƒä¸­ç›‘æ§
- [ ] é˜¶æ®µ1æŸå¤±ç¨³å®šä¸‹é™
- [ ] å­¦ä¹ ç‡è°ƒåº¦æ­£å¸¸
- [ ] å†…å­˜ä½¿ç”¨ç¨³å®š
- [ ] æ£€æŸ¥ç‚¹æ­£å¸¸ä¿å­˜

### è®­ç»ƒåéªŒè¯
- [ ] ä¸¤ä¸ªé˜¶æ®µéƒ½æˆåŠŸå®Œæˆ
- [ ] ç”Ÿæˆæ ·æœ¬è´¨é‡è‰¯å¥½
- [ ] CPL-Diffè¯„ä¼°æŒ‡æ ‡è¾¾æ ‡
- [ ] æ¨¡å‹å¯ä»¥æ­£å¸¸æ¨ç†

## ğŸ‰ é¢„æœŸæ•ˆæœ

### è®­ç»ƒç¨³å®šæ€§æå‡
- æ”¶æ•›é€Ÿåº¦æå‡30-50%
- è®­ç»ƒæŸå¤±æ³¢åŠ¨å‡å°‘
- æ¢¯åº¦çˆ†ç‚¸é—®é¢˜æ¶ˆé™¤

### ç”Ÿæˆè´¨é‡æ”¹è¿›
- ä¼ªå›°æƒ‘åº¦é™ä½15-25%
- ç»“æ„ç½®ä¿¡åº¦æå‡
- é•¿åº¦æ§åˆ¶ç²¾åº¦è¾¾åˆ°95%+

### è®¡ç®—æ•ˆç‡ä¼˜åŒ–
- å†…å­˜ä½¿ç”¨å‡å°‘20-30%
- è®­ç»ƒæ—¶é—´ç¼©çŸ­ï¼ˆç”±äºæ›´å¿«æ”¶æ•›ï¼‰
- GPUåˆ©ç”¨ç‡æå‡

## ğŸ“š ç›¸å…³èµ„æº

- [CPL-Diffè®ºæ–‡](https://arxiv.org/abs/xxx)
- [åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼æŒ‡å—](CFG_LENGTH_INTEGRATION_GUIDE.md)
- [CPL-Diffè¯„ä¼°æŒ‡å—](CPL_DIFF_EVALUATION_GUIDE.md)
- [StructDiffæ¶æ„æ–‡æ¡£](README.md)

---

**æç¤º**ï¼šåˆ†ç¦»å¼è®­ç»ƒæ˜¯ä¸€ä¸ªå¼ºå¤§çš„æŠ€æœ¯ï¼Œä½†éœ€è¦æ ¹æ®å…·ä½“æ•°æ®å’Œä»»åŠ¡è¿›è¡Œè°ƒä¼˜ã€‚å»ºè®®ä»å°è§„æ¨¡å®éªŒå¼€å§‹ï¼Œé€æ­¥ä¼˜åŒ–è¶…å‚æ•°ã€‚