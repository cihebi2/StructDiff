# Default training configuration
experiment:
  name: "structdiff_peptide_generation"
  project: "StructDiff"
  seed: 42
  output_dir: "./outputs"
  checkpoint_dir: "./checkpoints"
  log_dir: "./logs"

model:
  type: "StructDiff"
  sequence_encoder:
    pretrained_model: "facebook/esm2_t6_8M_UR50D"
    freeze_encoder: false
    use_lora: true
    lora_rank: 16
    lora_alpha: 32
    lora_dropout: 0.1
  
  structure_encoder:
    type: "multi_scale"
    hidden_dim: 256
    use_esmfold: false
    local:
      hidden_dim: 256
      num_layers: 3
      kernel_sizes: [3, 5, 7]
      dropout: 0.1
    global:
      hidden_dim: 512
      num_attention_heads: 8
      num_layers: 4
      dropout: 0.1
    fusion:
      method: "attention"
      hidden_dim: 256
  
  denoiser:
    hidden_dim: 768
    num_layers: 12
    num_heads: 12
    dropout: 0.1
    use_cross_attention: true

data:
  train_path: "./data/peptides/train.csv"
  val_path: "./data/peptides/val.csv"
  test_path: "./data/peptides/test.csv"
  structure_dir: "./data/structures"
  
  # Data processing
  max_length: 50
  min_length: 5
  augmentation:
    enabled: true
    noise_level: 0.1
    mask_prob: 0.15
  
  # Dataloader
  batch_size: 32
  num_workers: 4
  pin_memory: true
  prefetch_factor: 2

training:
  num_epochs: 100
  gradient_accumulation_steps: 1
  gradient_clip: 1.0  # Added missing parameter
  
  # Optimizer
  optimizer:
    type: "AdamW"
    lr: 1e-4
    betas: [0.9, 0.999]
    weight_decay: 0.01
    eps: 1e-8
  
  # Scheduler
  scheduler:
    type: "cosine"
    num_warmup_steps: 1000
    num_training_steps: null  # Computed from epochs
    min_lr: 1e-6
  
  # Mixed precision
  use_amp: true
  amp_dtype: "float16"
  
  # EMA
  use_ema: true
  ema_decay: 0.9999
  ema_update_every: 10
  
  # Checkpointing
  save_every: 1000
  validate_every: 500
  log_every: 100
  max_checkpoints: 5
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 10
    metric: "val_loss"
    mode: "min"

diffusion:
  num_timesteps: 1000
  noise_schedule: "sqrt"
  beta_start: 0.0001
  beta_end: 0.02
  
  # Sampling
  sampling_method: "ddpm"
  ddim_steps: 50
  
  # Loss weights
  loss_type: "mse"

# Loss configuration
training_config:
  loss_weights:
    diffusion_loss: 1.0
    structure_consistency_loss: 0.1
    auxiliary_loss: 0.01

evaluation:
  metrics:
    - perplexity
    - accuracy
    - structure_consistency
    - diversity
  
  generation:
    num_samples: 100
    guidance_scale: 1.0
    temperature: 1.0

wandb:
  enabled: true
  project: "StructDiff"
  entity: null  # Your wandb entity
  tags: ["peptide", "diffusion", "structure"]
  
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  save_every: 100  # Save logs every N steps
  log_every: 100   # Log metrics every N steps
# Updated: 05/31/2025 15:11:07

# Updated: 05/31/2025 15:14:04
