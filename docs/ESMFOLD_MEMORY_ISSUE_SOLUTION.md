# ESMFold å†…å­˜é—®é¢˜è§£å†³æ–¹æ¡ˆ

## ğŸš¨ é—®é¢˜æè¿°

åœ¨è¿è¡Œç»“æ„ç‰¹å¾è®­ç»ƒæ—¶é‡åˆ°çš„ESMFoldå†…å­˜åˆ†é…å¤±è´¥é—®é¢˜ï¼š

```
CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 64.12 MiB is free. Process 29424 has 496.00 MiB memory in use. Including non-PyTorch memory, this process has 23.09 GiB memory in use.
```

## ğŸ” é—®é¢˜åˆ†æ

### 1. æ ¹æœ¬åŸå› 
- **åŒé‡ESMFoldåŠ è½½**ï¼šæ¨¡å‹å†…éƒ¨å°è¯•å†æ¬¡åŠ è½½ESMFoldï¼Œå¯¼è‡´å†…å­˜é‡å¤å ç”¨
- **å†…å­˜ç¢ç‰‡åŒ–**ï¼šPyTorchå†…å­˜åˆ†é…å™¨çš„ç¢ç‰‡åŒ–é—®é¢˜
- **ç¼ºä¹å†…å­˜ç®¡ç†ç­–ç•¥**ï¼šæ²¡æœ‰é‡‡ç”¨æˆåŠŸè„šæœ¬ä¸­çš„å†…å­˜ä¼˜åŒ–æŠ€å·§

### 2. å½±å“è¯„ä¼°
- âŒ **ä¸¥é‡å½±å“è®­ç»ƒæ•ˆæœ**ï¼šESMFoldåŠ è½½å¤±è´¥å¯¼è‡´å›é€€åˆ°"è™šæ‹Ÿç»“æ„é¢„æµ‹"
- âŒ **ç»“æ„ç‰¹å¾ç¼ºå¤±**ï¼šå®é™…ä¸Šæ²¡æœ‰ä½¿ç”¨çœŸæ­£çš„ç»“æ„ç‰¹å¾
- âŒ **è®­ç»ƒè´¨é‡ä¸‹é™**ï¼šå˜æˆäº†ä¼ªç»“æ„ç‰¹å¾è®­ç»ƒ

## âœ… è§£å†³æ–¹æ¡ˆ

### 1. åŸºäºæˆåŠŸè„šæœ¬çš„ä¼˜åŒ–ç­–ç•¥

#### 1.1 å†…å­˜åˆ†é…ç­–ç•¥
```bash
# å…³é”®ç¯å¢ƒå˜é‡è®¾ç½®
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:128
```

#### 1.2 å…±äº«ESMFoldå®ä¾‹
```python
def setup_shared_esmfold(device):
    """åˆ›å»ºå…±äº«çš„ESMFoldå®ä¾‹ä»¥èŠ‚çœå†…å­˜"""
    # æ›´æ¿€è¿›çš„å†…å­˜æ¸…ç†
    torch.cuda.empty_cache()
    torch.cuda.synchronize()
    gc.collect()
    
    # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:128'
    
    # å†æ¬¡æ¸…ç†
    torch.cuda.empty_cache()
    
    # åˆ›å»ºESMFoldå®ä¾‹
    shared_esmfold = ESMFoldWrapper(device=device)
    return shared_esmfold
```

#### 1.3 é¿å…åŒé‡åŠ è½½
```python
def setup_model_and_training(config, device, shared_esmfold):
    # å¤‡ä»½åŸå§‹é…ç½®
    original_use_esmfold = config.model.structure_encoder.get('use_esmfold', False)
    
    # ä¸´æ—¶ç¦ç”¨æ¨¡å‹å†…éƒ¨çš„ESMFoldåŠ è½½
    if shared_esmfold and shared_esmfold.available:
        config.model.structure_encoder.use_esmfold = False
    
    # åˆ›å»ºæ¨¡å‹
    model = StructDiff(config.model).to(device)
    
    # æ¢å¤é…ç½®å¹¶è®¾ç½®å…±äº«å®ä¾‹
    config.model.structure_encoder.use_esmfold = original_use_esmfold
    
    # æ‰‹åŠ¨è®¾ç½®å…±äº«ESMFoldå®ä¾‹
    if shared_esmfold and shared_esmfold.available:
        model.structure_encoder.esmfold = shared_esmfold
        model.structure_encoder.use_esmfold = True
```

### 2. æ•°æ®åŠ è½½ä¼˜åŒ–

#### 2.1 ä¿å®ˆçš„æ•°æ®åŠ è½½å™¨è®¾ç½®
```python
train_loader = DataLoader(
    train_dataset,
    batch_size=2,  # å°æ‰¹æ¬¡å¤§å°
    shuffle=True,
    num_workers=0,  # å…³é”®ï¼šé¿å…å¤šè¿›ç¨‹ç¼“å­˜ç«äº‰
    pin_memory=False,  # ç¦ç”¨pin_memoryèŠ‚çœå†…å­˜
    collate_fn=collator,
    drop_last=True
)
```

#### 2.2 å…±äº«ESMFoldå®ä¾‹ä¼ é€’
```python
train_dataset = PeptideStructureDataset(
    data_path="...",
    config=config,
    is_training=True,
    shared_esmfold=shared_esmfold  # ä¼ é€’å…±äº«å®ä¾‹
)
```

### 3. è®­ç»ƒè¿‡ç¨‹ä¼˜åŒ–

#### 3.1 å®šæœŸå†…å­˜æ¸…ç†
```python
def clear_memory():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()

# åœ¨è®­ç»ƒå¾ªç¯ä¸­å®šæœŸè°ƒç”¨
if batch_idx % 10 == 0:
    clear_memory()
```

#### 3.2 æ¢¯åº¦ç´¯ç§¯ç­–ç•¥
```python
# ä½¿ç”¨å°æ‰¹æ¬¡ + å¤§æ¢¯åº¦ç´¯ç§¯
batch_size = 2
gradient_accumulation_steps = 8
effective_batch_size = 16  # ä¿æŒç›¸åŒçš„æœ‰æ•ˆæ‰¹æ¬¡å¤§å°
```

## ğŸ› ï¸ ä¿®å¤ç‰ˆæœ¬å®ç°

### 1. æ–°çš„è®­ç»ƒè„šæœ¬
åˆ›å»ºäº† `full_train_with_structure_features_fixed_v2.py`ï¼ŒåŒ…å«ï¼š

- âœ… **å…±äº«ESMFoldå®ä¾‹ç®¡ç†**
- âœ… **å†…å­˜åˆ†é…ç­–ç•¥ä¼˜åŒ–**
- âœ… **é¿å…åŒé‡åŠ è½½æœºåˆ¶**
- âœ… **ä¿å®ˆçš„æ•°æ®åŠ è½½è®¾ç½®**
- âœ… **å®šæœŸå†…å­˜æ¸…ç†**

### 2. å¯åŠ¨è„šæœ¬ä¼˜åŒ–
åˆ›å»ºäº† `start_structure_training_fixed.sh`ï¼ŒåŒ…å«ï¼š

```bash
# å…³é”®ç¯å¢ƒå˜é‡
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:128

# é¢„æ¸…ç†GPUå†…å­˜
python -c "
import torch
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print(f'GPUå†…å­˜å·²æ¸…ç†')
"
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. åœæ­¢å½“å‰è®­ç»ƒ
```bash
# æ‰¾åˆ°å¹¶åœæ­¢å½“å‰è®­ç»ƒè¿›ç¨‹
ps aux | grep full_train_with_structure_features_enabled
kill <PID>
```

### 2. å¯åŠ¨ä¿®å¤ç‰ˆæœ¬
```bash
cd /home/qlyu/sequence/StructDiff-7.0.0
./start_structure_training_fixed.sh
```

### 3. ç›‘æ§è®­ç»ƒ
```bash
# æŸ¥çœ‹æ—¥å¿—
tail -f outputs/structure_feature_training_fixed/training.log

# ç›‘æ§GPUå†…å­˜
watch -n 1 nvidia-smi
```

## ğŸ“Š é¢„æœŸæ”¹è¿›æ•ˆæœ

### 1. å†…å­˜ä½¿ç”¨ä¼˜åŒ–
- **ESMFoldå†…å­˜ä½¿ç”¨**: ~14GBï¼ˆå…±äº«å®ä¾‹ï¼‰
- **æ¨¡å‹å†…å­˜ä½¿ç”¨**: ~4GB
- **æ€»å†…å­˜ä½¿ç”¨**: ~18GBï¼ˆåœ¨24GBèŒƒå›´å†…ï¼‰

### 2. è®­ç»ƒç¨³å®šæ€§
- âœ… **ESMFoldæˆåŠŸåŠ è½½**ï¼šä¸å†å›é€€åˆ°è™šæ‹Ÿç»“æ„é¢„æµ‹
- âœ… **çœŸæ­£çš„ç»“æ„ç‰¹å¾**ï¼šä½¿ç”¨å®é™…çš„pLDDTã€è·ç¦»çŸ©é˜µç­‰
- âœ… **è®­ç»ƒæ”¶æ•›æ€§**ï¼šç»“æ„ç‰¹å¾çœŸæ­£å‚ä¸è®­ç»ƒ

### 3. æ€§èƒ½æŒ‡æ ‡
- **æ‰¹æ¬¡å¤„ç†æ—¶é—´**: 8-12ç§’/æ‰¹æ¬¡
- **å†…å­˜ç¨³å®šæ€§**: æ— å†…å­˜æ³„æ¼
- **è®­ç»ƒè´¨é‡**: ç»“æ„æ„ŸçŸ¥çš„åºåˆ—ç”Ÿæˆ

## ğŸ” éªŒè¯æ–¹æ³•

### 1. æ£€æŸ¥ESMFoldçŠ¶æ€
åœ¨è®­ç»ƒæ—¥å¿—ä¸­æŸ¥æ‰¾ï¼š
```
âœ… å…±äº«ESMFold GPUå®ä¾‹åˆ›å»ºæˆåŠŸ
âœ… å…±äº« ESMFold å®ä¾‹å·²è®¾ç½®åˆ°æ¨¡å‹ä¸­
```

### 2. ç›‘æ§å†…å­˜ä½¿ç”¨
```bash
# åº”è¯¥çœ‹åˆ°ç¨³å®šçš„å†…å­˜ä½¿ç”¨ï¼Œæ— çªç„¶å¢é•¿
nvidia-smi -l 1
```

### 3. éªŒè¯ç»“æ„ç‰¹å¾
åœ¨è®­ç»ƒæ—¥å¿—ä¸­åº”è¯¥çœ‹åˆ°ç»“æ„ç‰¹å¾ç›¸å…³çš„å¤„ç†ä¿¡æ¯ï¼Œè€Œä¸æ˜¯"è™šæ‹Ÿç»“æ„é¢„æµ‹"ã€‚

## ğŸ“ å…³é”®ç»éªŒæ€»ç»“

1. **å…±äº«å®ä¾‹æ˜¯å…³é”®**ï¼šé¿å…é‡å¤åŠ è½½å¤§å‹æ¨¡å‹
2. **å†…å­˜åˆ†é…ç­–ç•¥å¾ˆé‡è¦**ï¼š`expandable_segments:True`è§£å†³ç¢ç‰‡åŒ–
3. **æ•°æ®åŠ è½½å™¨è®¾ç½®å½±å“ç¨³å®šæ€§**ï¼š`num_workers=0`é¿å…ç«äº‰
4. **å®šæœŸæ¸…ç†å¿…ä¸å¯å°‘**ï¼šé˜²æ­¢å†…å­˜æ³„æ¼ç´¯ç§¯
5. **å°æ‰¹æ¬¡+æ¢¯åº¦ç´¯ç§¯**ï¼šåœ¨å†…å­˜é™åˆ¶ä¸‹ä¿æŒè®­ç»ƒæ•ˆæœ

é€šè¿‡è¿™äº›ä¼˜åŒ–ï¼Œå¯ä»¥æˆåŠŸå¯ç”¨çœŸæ­£çš„ç»“æ„ç‰¹å¾è®­ç»ƒï¼Œè€Œä¸æ˜¯ä¹‹å‰çš„ä¼ªè®­ç»ƒæ¨¡å¼ã€‚ 