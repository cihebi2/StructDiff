# StructDiff ESMFold è®­ç»ƒæ¶æ„è¯¦è§£

## æ¦‚è¿°

`full_train_200_epochs_with_esmfold_fixed.py` æ˜¯ä¸€ä¸ªç»è¿‡ä¼˜åŒ–çš„ StructDiff æ¨¡å‹è®­ç»ƒè„šæœ¬ï¼Œä¸“é—¨è®¾è®¡ç”¨äºç»“åˆ ESMFold ç»“æ„é¢„æµ‹çš„è‚½æ®µåºåˆ—ç”Ÿæˆã€‚è¯¥è„šæœ¬é‡‡ç”¨äº†æ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼Œé¦–å…ˆç¨³å®šåºåˆ—ç‰¹å¾è®­ç»ƒï¼Œç„¶åé€æ­¥å¼•å…¥ç»“æ„ç‰¹å¾ã€‚

## ğŸ—ï¸ æ¶æ„ç»„ä»¶

### 1. æ ¸å¿ƒæ¨¡å‹æ¶æ„

```mermaid
graph TB
    A[è¾“å…¥è‚½æ®µåºåˆ—] --> B[ESM-2 åºåˆ—ç¼–ç å™¨]
    B --> C[åºåˆ—åµŒå…¥ 768ç»´]
    D[ESMFold ç»“æ„é¢„æµ‹å™¨] --> E[ç»“æ„ç‰¹å¾æå–]
    E --> F[ç»“æ„åµŒå…¥ 256ç»´]
    C --> G[ç»“æ„æ„ŸçŸ¥å»å™ªå™¨]
    F --> G
    G --> H[é¢„æµ‹å™ªå£°]
    I[é«˜æ–¯æ‰©æ•£è¿‡ç¨‹] --> J[å™ªå£°æ·»åŠ ]
    J --> G
    H --> K[MSEæŸå¤±]
    L[æ¡ä»¶ä¿¡æ¯] --> G
```

#### 1.1 åºåˆ—ç¼–ç å™¨ (ESM-2)
- **æ¨¡å‹**: `facebook/esm2_t6_8M_UR50D` (800ä¸‡å‚æ•°)
- **åŠŸèƒ½**: å°†è‚½æ®µåºåˆ—ç¼–ç ä¸ºé«˜ç»´å‘é‡è¡¨ç¤º
- **è¾“å‡ºç»´åº¦**: 768ç»´éšè—çŠ¶æ€
- **ç‰¹ç‚¹**: é¢„è®­ç»ƒçš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼Œç†è§£æ°¨åŸºé…¸åºåˆ—è¯­ä¹‰

#### 1.2 ç»“æ„é¢„æµ‹å™¨ (ESMFold)
- **æ¨¡å‹**: ESMFold (65äº¿å‚æ•°)
- **åŠŸèƒ½**: ä»åºåˆ—é¢„æµ‹3Dç»“æ„
- **è¾“å‡º**: pLDDTåˆ†æ•°ã€è·ç¦»çŸ©é˜µã€æ¥è§¦å›¾ã€äºŒé¢è§’ã€äºŒçº§ç»“æ„
- **çŠ¶æ€**: å½“å‰ç‰ˆæœ¬ä¸­å¤„äºå¤‡ç”¨çŠ¶æ€ï¼Œæœªç›´æ¥ä½¿ç”¨

#### 1.3 ç»“æ„æ„ŸçŸ¥å»å™ªå™¨
- **æ¶æ„**: 12å±‚ Transformer æ¶æ„
- **éšè—ç»´åº¦**: 768ç»´
- **æ³¨æ„åŠ›å¤´**: 12ä¸ª
- **åŠŸèƒ½**: åœ¨ç»™å®šæ—¶é—´æ­¥å’Œç»“æ„ä¿¡æ¯ä¸‹é¢„æµ‹å™ªå£°

### 2. æ‰©æ•£è¿‡ç¨‹æ¶æ„

#### 2.1 é«˜æ–¯æ‰©æ•£è¿‡ç¨‹
```python
# æ‰©æ•£å‚æ•°
num_timesteps = 1000
noise_schedule = "sqrt"
beta_start = 0.0001
beta_end = 0.02
```

#### 2.2 å‰å‘è¿‡ç¨‹ (åŠ å™ª)
```python
# åœ¨æ—¶é—´æ­¥tæ·»åŠ å™ªå£°
noisy_embeddings = diffusion.q_sample(seq_embeddings, timesteps, noise)
```

#### 2.3 åå‘è¿‡ç¨‹ (å»å™ª)
```python
# é¢„æµ‹æ·»åŠ çš„å™ªå£°
predicted_noise, _ = model.denoiser(
    noisy_embeddings,
    timesteps,
    attention_mask,
    structure_features=structure_features,
    conditions=conditions
)
```

## ğŸ”§ è®­ç»ƒè¿‡ç¨‹è¯¦è§£

### 1. è®­ç»ƒé…ç½®

```python
# æ ¸å¿ƒè®­ç»ƒå‚æ•°
batch_size = 8                    # æ‰¹æ¬¡å¤§å°
gradient_accumulation_steps = 2   # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
effective_batch_size = 16         # æœ‰æ•ˆæ‰¹æ¬¡å¤§å°
num_epochs = 200                  # è®­ç»ƒè½®æ•°
learning_rate = 1e-4              # å­¦ä¹ ç‡
weight_decay = 1e-5               # æƒé‡è¡°å‡
```

### 2. ä¼˜åŒ–ç­–ç•¥

#### 2.1 ä¼˜åŒ–å™¨é…ç½®
```python
optimizer = optim.AdamW(
    model.parameters(), 
    lr=1e-4, 
    weight_decay=1e-5
)
```

#### 2.2 å­¦ä¹ ç‡è°ƒåº¦
```python
scheduler = CosineAnnealingLR(
    optimizer, 
    T_max=200,      # 200ä¸ªepochçš„ä½™å¼¦é€€ç«
    eta_min=1e-6    # æœ€å°å­¦ä¹ ç‡
)
```

#### 2.3 æ¢¯åº¦å¤„ç†
```python
# æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# æ¢¯åº¦ç´¯ç§¯æé«˜æœ‰æ•ˆæ‰¹æ¬¡å¤§å°
if (batch_idx + 1) % gradient_accumulation_steps == 0:
    optimizer.step()
    optimizer.zero_grad()
```

### 3. è®­ç»ƒæ­¥éª¤è¯¦è§£

#### 3.1 å•æ­¥è®­ç»ƒè¿‡ç¨‹
```python
def training_step(model, diffusion, batch, device, esmfold_wrapper):
    # 1. è·å–åºåˆ—åµŒå…¥
    seq_embeddings = model.sequence_encoder(
        batch['sequences'], 
        attention_mask=batch['attention_mask']
    ).last_hidden_state
    
    # 2. é‡‡æ ·æ—¶é—´æ­¥
    timesteps = torch.randint(0, diffusion.num_timesteps, (batch_size,))
    
    # 3. æ·»åŠ å™ªå£°
    noise = torch.randn_like(seq_embeddings)
    noisy_embeddings = diffusion.q_sample(seq_embeddings, timesteps, noise)
    
    # 4. åˆ›å»ºæ¡ä»¶
    conditions = {'peptide_type': batch['label']}
    
    # 5. å»å™ªé¢„æµ‹
    predicted_noise, _ = model.denoiser(
        noisy_embeddings,
        timesteps,
        batch['attention_mask'],
        structure_features=None,  # å½“å‰ç‰ˆæœ¬æš‚æœªä½¿ç”¨
        conditions=conditions
    )
    
    # 6. è®¡ç®—æŸå¤±
    loss = nn.functional.mse_loss(predicted_noise, noise)
    
    return loss
```

#### 3.2 éªŒè¯è¿‡ç¨‹
```python
def validation_step(model, diffusion, val_loader, device, esmfold_wrapper, logger):
    model.eval()
    val_losses = []
    
    with torch.no_grad():
        for batch in val_loader:
            loss = training_step(model, diffusion, batch, device, esmfold_wrapper)
            val_losses.append(loss.item())
    
    avg_val_loss = np.mean(val_losses)
    model.train()
    return avg_val_loss
```

### 4. å†…å­˜ä¼˜åŒ–ç­–ç•¥

#### 4.1 æ˜¾å­˜ç®¡ç†
```python
# ç¦ç”¨ CUDNN åŸºå‡†æµ‹è¯•ä»¥èŠ‚çœæ˜¾å­˜
torch.backends.cudnn.benchmark = False
torch.backends.cudnn.deterministic = True

# å®šæœŸæ¸…ç†æ˜¾å­˜
if batch_idx % 20 == 0:
    torch.cuda.empty_cache()

# åƒåœ¾å›æ”¶
gc.collect()
```

#### 4.2 æ‰¹å¤„ç†ä¼˜åŒ–
```python
# è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°
def custom_collate_fn(batch):
    result = {}
    for key in batch[0].keys():
        if key == 'sequence':
            result[key] = [item[key] for item in batch]
        elif key == 'structures':
            continue  # è·³è¿‡ç»“æ„ç‰¹å¾
        else:
            result[key] = torch.stack([item[key] for item in batch])
    return result
```

## ğŸ“Š è®­ç»ƒç›‘æ§

### 1. å…³é”®æŒ‡æ ‡

#### 1.1 æŸå¤±æŒ‡æ ‡
- **è®­ç»ƒæŸå¤±**: MSEæŸå¤±ï¼Œè¡¡é‡å™ªå£°é¢„æµ‹å‡†ç¡®æ€§
- **éªŒè¯æŸå¤±**: éªŒè¯é›†ä¸Šçš„MSEæŸå¤±
- **æœ€ä½³éªŒè¯æŸå¤±**: ç”¨äºæ¨¡å‹é€‰æ‹©

#### 1.2 ç³»ç»ŸæŒ‡æ ‡
- **GPUå†…å­˜ä½¿ç”¨**: å®æ—¶ç›‘æ§æ˜¾å­˜å ç”¨
- **å­¦ä¹ ç‡**: åŠ¨æ€è°ƒæ•´çš„å­¦ä¹ ç‡
- **æ¢¯åº¦èŒƒæ•°**: ç›‘æ§æ¢¯åº¦ç¨³å®šæ€§

### 2. æ—¥å¿—è®°å½•

#### 2.1 è®­ç»ƒæ—¥å¿—
```python
# æ¯50ä¸ªæ‰¹æ¬¡è®°å½•ä¸€æ¬¡
if batch_idx % 50 == 0:
    logger.info(f"Epoch {epoch+1}, Batch {batch_idx}, "
                f"Loss: {loss.item():.6f}, "
                f"GPU Memory: {allocated}GB")
```

#### 2.2 è¿›åº¦æ¡æ˜¾ç¤º
```python
progress_bar.set_postfix({
    'loss': f'{loss.item():.6f}',
    'avg_loss': f'{epoch_loss/max(num_batches, 1):.6f}',
    'lr': f'{current_lr:.2e}',
    'gpu_mem': f'{allocated}GB'
})
```

### 3. æ£€æŸ¥ç‚¹ä¿å­˜

#### 3.1 å®šæœŸä¿å­˜
```python
# æ¯20ä¸ªepochä¿å­˜æ£€æŸ¥ç‚¹
if (epoch + 1) % save_every == 0:
    checkpoint_path = f"{output_dir}/checkpoint_epoch_{epoch+1}.pt"
    save_checkpoint(model, optimizer, scheduler, epoch + 1, avg_train_loss, checkpoint_path)
```

#### 3.2 æœ€ä½³æ¨¡å‹ä¿å­˜
```python
# ä¿å­˜éªŒè¯æŸå¤±æœ€ä½çš„æ¨¡å‹
if val_loss < best_val_loss:
    best_val_loss = val_loss
    best_model_path = f"{output_dir}/best_model.pt"
    torch.save(model.state_dict(), best_model_path)
```

## ğŸš€ å¦‚ä½•ä½¿ç”¨

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# ç¡®ä¿åœ¨æ­£ç¡®çš„ç¯å¢ƒä¸­
cd /home/qlyu/sequence/StructDiff-7.0.0

# æ£€æŸ¥GPUå¯ç”¨æ€§
nvidia-smi

# è®¾ç½®CUDAè®¾å¤‡
export CUDA_VISIBLE_DEVICES=1
```

### 2. å¼€å§‹è®­ç»ƒ

```bash
# ç›´æ¥è¿è¡Œè®­ç»ƒè„šæœ¬
python full_train_200_epochs_with_esmfold_fixed.py

# æˆ–è€…åœ¨åå°è¿è¡Œ
nohup python full_train_200_epochs_with_esmfold_fixed.py > training.log 2>&1 &
```

### 3. ç›‘æ§è®­ç»ƒ

```bash
# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f /home/qlyu/sequence/StructDiff-7.0.0/outputs/full_training_200_esmfold_fixed/training.log

# æŸ¥çœ‹GPUä½¿ç”¨æƒ…å†µ
watch -n 1 nvidia-smi

# æŸ¥çœ‹è®­ç»ƒæŒ‡æ ‡
cat /home/qlyu/sequence/StructDiff-7.0.0/outputs/full_training_200_esmfold_fixed/training_metrics.json
```

### 4. æ¢å¤è®­ç»ƒ

```python
# ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
checkpoint = torch.load("checkpoint_epoch_100.pt")
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
start_epoch = checkpoint['epoch']
```

## ğŸ”® åç»­è®­ç»ƒç­–ç•¥

### 1. é˜¶æ®µæ€§è®­ç»ƒè®¡åˆ’

#### é˜¶æ®µ1: åºåˆ—ç‰¹å¾è®­ç»ƒ (å½“å‰é˜¶æ®µ)
- **ç›®æ ‡**: ç¨³å®šåºåˆ—åµŒå…¥å’Œå»å™ªå™¨è®­ç»ƒ
- **ç‰¹ç‚¹**: ä¸ä½¿ç”¨ç»“æ„ç‰¹å¾ï¼Œä¸“æ³¨äºåºåˆ—è¯­ä¹‰å­¦ä¹ 
- **é¢„æœŸç»“æœ**: è®­ç»ƒæŸå¤±æ”¶æ•›åˆ°0.15-0.20

#### é˜¶æ®µ2: ç»“æ„ç‰¹å¾é›†æˆ
```python
# å¯ç”¨ç»“æ„ç‰¹å¾çš„é…ç½®ä¿®æ”¹
config.data.use_predicted_structures = True
config.model.structure_encoder.use_esmfold = True

# è°ƒæ•´è®­ç»ƒå‚æ•°
batch_size = 2  # é™ä½æ‰¹æ¬¡å¤§å°ä»¥é€‚åº”ESMFold
gradient_accumulation_steps = 8  # å¢åŠ æ¢¯åº¦ç´¯ç§¯
```

#### é˜¶æ®µ3: ç«¯åˆ°ç«¯å¾®è°ƒ
```python
# è§£å†»åºåˆ—ç¼–ç å™¨
config.model.sequence_encoder.freeze_encoder = False

# é™ä½å­¦ä¹ ç‡
learning_rate = 5e-5
```

### 2. ç»“æ„ç‰¹å¾é›†æˆä»£ç 

```python
def enable_structure_features():
    """å¯ç”¨ç»“æ„ç‰¹å¾çš„è®­ç»ƒå‡½æ•°"""
    
    # ä¿®æ”¹é…ç½®
    config.data.use_predicted_structures = True
    config.model.structure_encoder.use_esmfold = True
    
    # è°ƒæ•´è®­ç»ƒå‚æ•°
    batch_size = 2
    gradient_accumulation_steps = 8
    
    # åœ¨è®­ç»ƒæ­¥éª¤ä¸­ä½¿ç”¨ç»“æ„ç‰¹å¾
    def training_step_with_structure(model, diffusion, batch, device, esmfold_wrapper):
        # è·å–åºåˆ—åµŒå…¥
        seq_embeddings = model.sequence_encoder(
            batch['sequences'], 
            attention_mask=batch['attention_mask']
        ).last_hidden_state
        
        # é¢„æµ‹ç»“æ„ç‰¹å¾
        structure_features = None
        if esmfold_wrapper and esmfold_wrapper.available:
            structures = []
            for seq in batch['sequence']:
                struct = esmfold_wrapper.predict_structure(seq)
                structures.append(struct)
            
            # æå–ç»“æ„å¼ é‡
            structure_features = [
                extract_structure_tensor(struct, device) 
                for struct in structures
            ]
            
            # å¡«å……åˆ°ç›¸åŒé•¿åº¦
            if structure_features and structure_features[0] is not None:
                structure_features = torch.stack(structure_features)
        
        # å…¶ä½™è®­ç»ƒæ­¥éª¤ç›¸åŒ...
        # ...
        
        return loss
```

### 3. é«˜çº§è®­ç»ƒæŠ€å·§

#### 3.1 åŠ¨æ€æ‰¹æ¬¡å¤§å°
```python
def adaptive_batch_size(epoch, base_batch_size=8):
    """æ ¹æ®è®­ç»ƒè¿›åº¦è°ƒæ•´æ‰¹æ¬¡å¤§å°"""
    if epoch < 50:
        return base_batch_size
    elif epoch < 100:
        return base_batch_size // 2
    else:
        return max(1, base_batch_size // 4)
```

#### 3.2 æŸå¤±æƒé‡è°ƒæ•´
```python
def adaptive_loss_weights(epoch):
    """åŠ¨æ€è°ƒæ•´æŸå¤±æƒé‡"""
    weights = {
        'diffusion_loss': 1.0,
        'structure_loss': min(0.1, epoch / 1000),  # é€æ¸å¢åŠ ç»“æ„æŸå¤±æƒé‡
        'consistency_loss': 0.01
    }
    return weights
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### 1. å½“å‰æ€§èƒ½æŒ‡æ ‡

- **GPUå†…å­˜ä½¿ç”¨**: ~4GB (ä¸å«ESMFold)
- **è®­ç»ƒé€Ÿåº¦**: ~2-3ç§’/æ‰¹æ¬¡
- **æ¨¡å‹å‚æ•°**: ~8M (åºåˆ—ç¼–ç å™¨) + ~50M (å»å™ªå™¨)
- **æ”¶æ•›é€Ÿåº¦**: 50-100 epochs

### 2. ä¼˜åŒ–å»ºè®®

#### 2.1 ç¡¬ä»¶ä¼˜åŒ–
```python
# ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    loss = training_step(model, diffusion, batch, device, esmfold_wrapper)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

#### 2.2 æ•°æ®åŠ è½½ä¼˜åŒ–
```python
# ä¼˜åŒ–æ•°æ®åŠ è½½å™¨
train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=4,        # å¢åŠ å·¥ä½œè¿›ç¨‹
    pin_memory=True,      # å¯ç”¨å†…å­˜å›ºå®š
    prefetch_factor=2,    # é¢„å–å› å­
    persistent_workers=True  # æŒä¹…åŒ–å·¥ä½œè¿›ç¨‹
)
```

## ğŸ¯ é¢„æœŸç»“æœ

### 1. è®­ç»ƒæ”¶æ•›æŒ‡æ ‡

- **è®­ç»ƒæŸå¤±**: ä»1.0é™è‡³0.15-0.20
- **éªŒè¯æŸå¤±**: ç¨³å®šåœ¨0.16å·¦å³
- **æ”¶æ•›æ—¶é—´**: 50-100 epochs
- **æœ€ä½³æ¨¡å‹**: éªŒè¯æŸå¤±æœ€ä½çš„æ£€æŸ¥ç‚¹

### 2. ç”Ÿæˆè´¨é‡æŒ‡æ ‡

- **åºåˆ—å¤šæ ·æ€§**: é«˜
- **ç”Ÿç‰©å­¦åˆç†æ€§**: é€šè¿‡ESM-2è¯­è¨€æ¨¡å‹ä¿è¯
- **æ¡ä»¶æ§åˆ¶**: æ”¯æŒè‚½æ®µç±»å‹æ¡ä»¶ç”Ÿæˆ
- **é•¿åº¦æ§åˆ¶**: æ”¯æŒæŒ‡å®šé•¿åº¦ç”Ÿæˆ

## ğŸ”§ æ•…éšœæ’é™¤

### 1. å¸¸è§é—®é¢˜

#### 1.1 CUDAå†…å­˜ä¸è¶³
```bash
# è§£å†³æ–¹æ¡ˆ
export CUDA_VISIBLE_DEVICES=1  # ä½¿ç”¨ç‰¹å®šGPU
# æˆ–é™ä½æ‰¹æ¬¡å¤§å°
batch_size = 4
```

#### 1.2 è®­ç»ƒä¸æ”¶æ•›
```python
# æ£€æŸ¥å­¦ä¹ ç‡
learning_rate = 5e-5  # é™ä½å­¦ä¹ ç‡

# æ£€æŸ¥æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
```

#### 1.3 æ˜¾å­˜æ³„æ¼
```python
# å®šæœŸæ¸…ç†
if batch_idx % 10 == 0:
    torch.cuda.empty_cache()
    gc.collect()
```

### 2. è°ƒè¯•å·¥å…·

```python
# æ£€æŸ¥æ¨¡å‹å‚æ•°
def check_model_params(model):
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total params: {total_params:,}")
    print(f"Trainable params: {trainable_params:,}")

# æ£€æŸ¥GPUå†…å­˜
def check_gpu_memory():
    allocated = torch.cuda.memory_allocated() // 1024**3
    cached = torch.cuda.memory_reserved() // 1024**3
    print(f"GPU Memory - Allocated: {allocated}GB, Cached: {cached}GB")
```

## ğŸ“ æ€»ç»“

`full_train_200_epochs_with_esmfold_fixed.py` æ˜¯ä¸€ä¸ªç»è¿‡ç²¾å¿ƒè®¾è®¡çš„è®­ç»ƒè„šæœ¬ï¼Œé‡‡ç”¨äº†æ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼š

1. **å½“å‰é˜¶æ®µ**: ä¸“æ³¨äºåºåˆ—ç‰¹å¾å­¦ä¹ ï¼Œç¡®ä¿è®­ç»ƒç¨³å®šæ€§
2. **ä¸‹ä¸€é˜¶æ®µ**: é€æ­¥å¼•å…¥ç»“æ„ç‰¹å¾ï¼Œæå‡ç”Ÿæˆè´¨é‡
3. **æœ€ç»ˆç›®æ ‡**: å®ç°åºåˆ—-ç»“æ„ååŒçš„é«˜è´¨é‡è‚½æ®µç”Ÿæˆ

è¯¥æ¶æ„å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§å’Œç¨³å®šæ€§ï¼Œä¸ºåç»­çš„ç»“æ„ç‰¹å¾é›†æˆå’Œæ¨¡å‹ä¼˜åŒ–å¥ å®šäº†åšå®åŸºç¡€ã€‚ 