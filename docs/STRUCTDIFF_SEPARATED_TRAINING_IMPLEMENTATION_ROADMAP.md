# StructDiffåˆ†ç¦»å¼è®­ç»ƒå®æ–½è§„åˆ’

## ç¡¬ä»¶ç¯å¢ƒé…ç½®

### ğŸ–¥ï¸ æœåŠ¡å™¨ç¡¬ä»¶è§„æ ¼
- **æ“ä½œç³»ç»Ÿ**: CentOS 7 (Linux 3.10.0-957.el7.x86_64)
- **GPUé…ç½®**: 6å— NVIDIA GeForce RTX 4090 (24GB VRAM each)
  - GPU 0: 4F:00.0 (å½“å‰ä½¿ç”¨ä¸­)
  - GPU 1: 50:00.0 (å½“å‰ä½¿ç”¨ä¸­)  
  - GPU 2: 53:00.0 (å½“å‰ä½¿ç”¨ä¸­)
  - **GPU 3: 9D:00.0 (å¯ç”¨)** âœ…
  - **GPU 4: A0:00.0 (å¯ç”¨)** âœ…
  - **GPU 5: A4:00.0 (å¯ç”¨)** âœ…
- **å¯ç”¨GPU**: CUDA 2,3,4,5 (GPU IDs)
- **æ€»å¯ç”¨æ˜¾å­˜**: 96GB (4Ã—24GB)
- **CUDAç‰ˆæœ¬**: 12.4 (Driver 550.120)

### ğŸ Pythonç¯å¢ƒé…ç½®
- **Condaç¯å¢ƒ**: cuda12.1 (å½“å‰æ¿€æ´»)
- **å·¥ä½œç›®å½•**: `/home/qlyu`
- **é¡¹ç›®è·¯å¾„**: `/home/qlyu/sequence/StructDiff-7.0.0`

### ğŸš€ è®­ç»ƒèµ„æºåˆ†é…è®¡åˆ’
```bash
# GPUä½¿ç”¨ç­–ç•¥
export CUDA_VISIBLE_DEVICES=2,3,4,5  # ä½¿ç”¨4å—å¯ç”¨GPU
# é˜¶æ®µ1(å»å™ªå™¨è®­ç»ƒ): ä½¿ç”¨GPU 2,3 (48GB VRAM)
# é˜¶æ®µ2(è§£ç å™¨è®­ç»ƒ): ä½¿ç”¨GPU 4,5 (48GB VRAM) 
# ESMFoldç¼“å­˜é¢„è®¡ç®—: ä½¿ç”¨å•GPUæ¨ç†
```

## é¡¹ç›®ç°çŠ¶æ·±åº¦åˆ†æ

### ğŸ” å½“å‰å®æ–½æƒ…å†µè¯„ä¼°

#### å·²å®Œæˆçš„å·¥ä½œ
1. **ç®€åŒ–æ‰©æ•£æ¨¡å‹è®­ç»ƒæˆåŠŸ** (2025å¹´7æœˆ)
   - âœ… ä½¿ç”¨ `train_structdiff_fixed.py` å®Œæˆ5è½®ç«¯åˆ°ç«¯è®­ç»ƒ
   - âœ… æ¨¡å‹å‚æ•°: 23,884,987ä¸ª (ESM2 + å»å™ªå™¨)
   - âœ… æ”¶æ•›è¡¨ç°: éªŒè¯æŸå¤±ä»0.884é™è‡³0.404
   - âœ… æ¨¡å‹ä¿å­˜: `outputs/structdiff_fixed/best_model.pt` (202MB)

#### å®é™…è®­ç»ƒæ¶æ„åˆ†æ
```python
# å½“å‰å®é™…æ¶æ„ (ç®€åŒ–ç‰ˆ)
- ESM2åºåˆ—ç¼–ç å™¨ (facebook/esm2_t6_8M_UR50D)
- æ‰©æ•£å»å™ªå™¨ (6å±‚, 8å¤´, 320ç»´)
- âŒ æ— ç»“æ„ç‰¹å¾ (use_esmfold: false)
- âŒ æ— äº¤å‰æ³¨æ„åŠ› (use_cross_attention: false)
- âŒ æ— åˆ†ç¦»å¼è®­ç»ƒç­–ç•¥
```

#### å…³é”®é—®é¢˜è¯†åˆ«
1. **æ¶æ„ä¸å®Œæ•´**: ç¼ºå°‘ç»“æ„æ„ŸçŸ¥èƒ½åŠ›
2. **è®­ç»ƒç­–ç•¥é”™è¯¯**: ä½¿ç”¨ç«¯åˆ°ç«¯è€Œéåˆ†ç¦»å¼è®­ç»ƒ
3. **åŠŸèƒ½ç¼ºå¤±**: 
   - æ— é•¿åº¦æ§åˆ¶
   - æ— åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼(CFG)
   - æ— ç»“æ„-åºåˆ—ååŒå»ºæ¨¡

### ğŸ“Š ç†æƒ³ä¸ç°å®çš„å·®è·

| ç»„ä»¶ | CPL-Diffç†æƒ³æ¶æ„ | å½“å‰å®ç°çŠ¶æ€ | å·®è·è¯„ä¼° |
|------|------------------|--------------|----------|
| åºåˆ—ç¼–ç å™¨ | ESM2 + LoRAå¾®è°ƒ | âœ… å·²å®ç° | å®Œå–„ |
| ç»“æ„ç¼–ç å™¨ | å¤šå°ºåº¦ç»“æ„ç‰¹å¾ | âŒ è¢«ç¦ç”¨ | **ä¸¥é‡ç¼ºå¤±** |
| å»å™ªå™¨ | ç»“æ„æ„ŸçŸ¥å»å™ª | âš ï¸ ä»…åºåˆ—å»å™ª | **åŠŸèƒ½å—é™** |
| åºåˆ—è§£ç å™¨ | ç‹¬ç«‹è§£ç å™¨ | âŒ æœªå®ç° | **å®Œå…¨ç¼ºå¤±** |
| è®­ç»ƒç­–ç•¥ | ä¸¤é˜¶æ®µåˆ†ç¦» | âŒ ç«¯åˆ°ç«¯ | **æ ¹æœ¬é”™è¯¯** |
| CFGå¼•å¯¼ | åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ | âŒ æœªå¯ç”¨ | **ç¼ºå¤±** |
| é•¿åº¦æ§åˆ¶ | åŠ¨æ€é•¿åº¦é‡‡æ · | âŒ å›ºå®šé•¿åº¦ | **ç¼ºå¤±** |

## ğŸ¯ åˆ†ç¦»å¼è®­ç»ƒæ ¸å¿ƒç†è®º

### CPL-Diffå¯å‘çš„è®¾è®¡åŸç†

#### ä¸¤é˜¶æ®µè®­ç»ƒå“²å­¦
```
ä¼ ç»Ÿç«¯åˆ°ç«¯è®­ç»ƒé—®é¢˜:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç¼–ç å™¨ â†’ å»å™ªå™¨ â†’ è§£ç å™¨             â”‚
â”‚    â†‘       â†‘       â†‘               â”‚
â”‚   åŒæ—¶ä¼˜åŒ–æ‰€æœ‰ç»„ä»¶                   â”‚
â”‚   æ¢¯åº¦å†²çªã€æ”¶æ•›å›°éš¾                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CPL-Diffåˆ†ç¦»å¼è®­ç»ƒ:
é˜¶æ®µ1: å›ºå®šç¼–ç å™¨ï¼Œä¼˜åŒ–å»å™ªå™¨
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [ç¼–ç å™¨] â†’ å»å™ªå™¨ â†’ [æ— è§£ç å™¨]        â”‚
â”‚   å›ºå®š      è®­ç»ƒ     æš‚ä¸ä½¿ç”¨         â”‚
â”‚   â†“                                â”‚
â”‚   ä¸“æ³¨å™ªå£°é¢„æµ‹ä»»åŠ¡                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

é˜¶æ®µ2: å›ºå®šå»å™ªå™¨ï¼Œä¼˜åŒ–è§£ç å™¨  
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [ç¼–ç å™¨] â†’ [å»å™ªå™¨] â†’ è§£ç å™¨         â”‚
â”‚   å›ºå®š      å›ºå®š       è®­ç»ƒ          â”‚
â”‚   â†“                                â”‚
â”‚   ä¸“æ³¨åºåˆ—é‡å»ºä»»åŠ¡                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ä¼˜åŠ¿åˆ†æ
1. **é™ä½ä¼˜åŒ–å¤æ‚åº¦**: æ¯é˜¶æ®µç›®æ ‡å•ä¸€æ˜ç¡®
2. **é¿å…æ¢¯åº¦å†²çª**: åˆ†ç¦»ä¸åŒå­¦ä¹ ç›®æ ‡
3. **æå‡æ”¶æ•›ç¨³å®šæ€§**: é˜¶æ®µæ€§ä¼˜åŒ–æ›´å¯æ§
4. **èµ„æºåˆ©ç”¨é«˜æ•ˆ**: å›ºå®šç»„ä»¶é¿å…é‡å¤è®¡ç®—

### ç»“æ„æ„ŸçŸ¥èƒ½åŠ›åˆ†æ

#### å½“å‰ç¼ºå¤±çš„ç»“æ„å»ºæ¨¡
```python
# ç†æƒ³çš„ç»“æ„æ„ŸçŸ¥StructDiff
class StructDiff(nn.Module):
    def __init__(self):
        # åºåˆ—ç¼–ç å™¨
        self.sequence_encoder = ESM2(...)
        
        # å¤šå°ºåº¦ç»“æ„ç¼–ç å™¨ (ç¼ºå¤±!)
        self.structure_encoder = MultiScaleStructureEncoder(
            local_features=True,    # å±€éƒ¨äºŒçº§ç»“æ„
            global_features=True,   # å…¨å±€æ‹“æ‰‘ç‰¹å¾
            esm_fold=True          # å®æ—¶ç»“æ„é¢„æµ‹
        )
        
        # ç»“æ„æ„ŸçŸ¥å»å™ªå™¨ (éƒ¨åˆ†å®ç°)
        self.denoiser = StructureAwareDenoiser(
            cross_attention=True,  # åºåˆ—-ç»“æ„äº¤å‰æ³¨æ„åŠ›
            structure_conditioning=True
        )
        
        # åºåˆ—è§£ç å™¨ (å®Œå…¨ç¼ºå¤±!)
        self.sequence_decoder = SequenceDecoder(...)
```

## ğŸ› ï¸ å®Œæ•´å®æ–½è·¯çº¿å›¾

### Phase 1: åŸºç¡€è®¾æ–½å®Œå–„ (é¢„è®¡1-2å¤©)

#### 1.1 ä¾èµ–é¡¹å’Œç¯å¢ƒéªŒè¯
```bash
# ç¯å¢ƒæ£€æŸ¥æ¸…å•
â–¡ PyTorch 2.6.0 + CUDA 12.4 âœ…
â–¡ ESMFoldç›¸å…³ä¾èµ–å®‰è£…çŠ¶æ€
â–¡ BioPythonåºåˆ—å¤„ç†åº“ âœ…  
â–¡ å†…å­˜éœ€æ±‚è¯„ä¼° (ESMFoldéœ€è¦~8GB GPUå†…å­˜)
â–¡ æ•°æ®å®Œæ•´æ€§éªŒè¯
```

#### 1.2 é…ç½®æ–‡ä»¶ç³»ç»Ÿé‡æ„
```yaml
# configs/separated_training_production.yaml
separated_training:
  stage1:
    epochs: 50              # å®é™…ç”Ÿäº§ç”¨epochs
    batch_size: 8           # åŸºäºGPUå†…å­˜ä¼˜åŒ–
    learning_rate: 1e-4     # CPL-Diffæ¨èå‚æ•°
    gradient_clip: 1.0
  
  stage2:
    epochs: 30
    batch_size: 16          # é˜¶æ®µ2å¯ç”¨æ›´å¤§batch
    learning_rate: 5e-5
    gradient_clip: 0.5

model:
  structure_encoder:
    use_esmfold: true       # å¯ç”¨ç»“æ„ç‰¹å¾!
    hidden_dim: 256
    multi_scale: true
    
  denoiser:
    use_cross_attention: true  # å¯ç”¨ç»“æ„äº¤å‰æ³¨æ„åŠ›!
    hidden_dim: 320
    
  sequence_decoder:          # æ–°å¢è§£ç å™¨é…ç½®!
    hidden_dim: 320
    num_layers: 4
    vocab_size: 33

classifier_free_guidance:
  enabled: true             # å¯ç”¨CFG!
  dropout_prob: 0.1
  guidance_scale: 2.0

length_control:
  enabled: true             # å¯ç”¨é•¿åº¦æ§åˆ¶!
  min_length: 5
  max_length: 50
```

### Phase 2: ç»“æ„ç¼–ç å™¨é‡æ–°æ¿€æ´» (é¢„è®¡2-3å¤©)

#### 2.1 ESMFoldé›†æˆé—®é¢˜è§£å†³
```python
# é—®é¢˜å›é¡¾: ESMFoldå¯¼è‡´çš„é—®é¢˜
é—®é¢˜1: CUDAå†…å­˜ä¸è¶³
- ESMFoldæ¨¡å‹è¾ƒå¤§ (~1.6GB)
- ä¸è®­ç»ƒæ¨¡å‹å…±å­˜æ—¶å†…å­˜è¶…é™

è§£å†³æ–¹æ¡ˆ:
- ä½¿ç”¨é¢„è®¡ç®—ç¼“å­˜ç­–ç•¥
- å®ç°å»¶è¿ŸåŠ è½½æœºåˆ¶
- ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°

# æ–°çš„ç»“æ„ç¼–ç å™¨ç­–ç•¥
class OptimizedStructureEncoder:
    def __init__(self):
        # å»¶è¿Ÿåˆå§‹åŒ–ESMFold
        self.esmfold = None
        self.cache_dir = "./structure_cache"
        
    def get_structure_features(self, sequences):
        # 1. é¦–å…ˆæ£€æŸ¥ç¼“å­˜
        cached_features = self.load_from_cache(sequences)
        if cached_features is not None:
            return cached_features
            
        # 2. å®æ—¶è®¡ç®—(å°æ‰¹é‡)
        if len(sequences) <= 4:  # åªå¯¹å°æ‰¹é‡å®æ—¶è®¡ç®—
            return self.compute_realtime(sequences)
            
        # 3. é™çº§åˆ°åºåˆ—ç‰¹å¾
        return self.fallback_to_sequence_features(sequences)
```

#### 2.2 å¤šå°ºåº¦ç»“æ„ç‰¹å¾å®ç°
```python
# ç»“æ„ç‰¹å¾å±‚æ¬¡
1. å±€éƒ¨ç‰¹å¾ (æ®‹åŸºçº§åˆ«):
   - Ï†/Ïˆè§’åº¦
   - ä¾§é“¾æ–¹å‘
   - å±€éƒ¨äºŒçº§ç»“æ„

2. ä¸­ç­‰å°ºåº¦ç‰¹å¾ (ç‰‡æ®µçº§åˆ«):
   - èºæ—‹/æŠ˜å ç‰‡åŒºåŸŸ
   - è½¬è§’å’Œç¯åŒº
   - å±€éƒ¨æ‹“æ‰‘

3. å…¨å±€ç‰¹å¾ (åˆ†å­çº§åˆ«):
   - æ•´ä½“æŠ˜å æ¨¡å¼
   - æ¥è§¦å›¾
   - ç©ºé—´è·ç¦»çŸ©é˜µ
```

### Phase 3: åˆ†ç¦»å¼è®­ç»ƒæ ¸å¿ƒå®ç° (é¢„è®¡3-4å¤©)

#### 3.1 é˜¶æ®µ1å®ç°ç»†èŠ‚
```python
def stage1_training_loop():
    """é˜¶æ®µ1: å»å™ªå™¨è®­ç»ƒ"""
    
    # 1. å†»ç»“åºåˆ—ç¼–ç å™¨
    for param in model.sequence_encoder.parameters():
        param.requires_grad = False
    
    # 2. è®­ç»ƒç›®æ ‡: å™ªå£°é¢„æµ‹
    def training_step(batch):
        # è·å–å›ºå®šåºåˆ—åµŒå…¥
        with torch.no_grad():
            seq_embeddings = model.sequence_encoder(
                sequences, attention_mask
            )
        
        # è·å–ç»“æ„ç‰¹å¾
        structure_features = model.structure_encoder(
            sequences, attention_mask
        )
        
        # æ‰©æ•£è¿‡ç¨‹
        timesteps = sample_timesteps(batch_size)
        noise = torch.randn_like(seq_embeddings)
        noisy_embeddings = diffusion.q_sample(
            seq_embeddings, timesteps, noise
        )
        
        # ç»“æ„æ„ŸçŸ¥å»å™ª
        predicted_noise = model.denoiser(
            noisy_embeddings=noisy_embeddings,
            timesteps=timesteps,
            attention_mask=attention_mask,
            structure_features=structure_features,  # å…³é”®!
            conditions=conditions
        )
        
        # æŸå¤±: é¢„æµ‹å™ªå£° vs çœŸå®å™ªå£°
        loss = F.mse_loss(predicted_noise, noise)
        return loss
```

#### 3.2 é˜¶æ®µ2å®ç°ç»†èŠ‚
```python
def stage2_training_loop():
    """é˜¶æ®µ2: è§£ç å™¨è®­ç»ƒ"""
    
    # 1. å†»ç»“ç¼–ç å™¨å’Œå»å™ªå™¨
    for name, param in model.named_parameters():
        if 'sequence_decoder' not in name:
            param.requires_grad = False
    
    # 2. è®­ç»ƒç›®æ ‡: åºåˆ—é‡å»º
    def training_step(batch):
        # è·å–å¹²å‡€åµŒå…¥(å›ºå®šç¼–ç å™¨)
        with torch.no_grad():
            seq_embeddings = model.sequence_encoder(
                sequences, attention_mask
            )
        
        # åºåˆ—è§£ç è®­ç»ƒ
        logits = model.sequence_decoder(
            seq_embeddings, attention_mask
        )
        
        # æŸå¤±: äº¤å‰ç†µ
        loss = F.cross_entropy(
            logits.view(-1, vocab_size),
            sequences.view(-1)
        )
        return loss
```

### Phase 4: é«˜çº§åŠŸèƒ½é›†æˆ (é¢„è®¡2-3å¤©)

#### 4.1 åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼(CFG)
```python
class CFGEnabledModel:
    def forward(self, x, conditions=None, cfg_scale=2.0):
        if self.training and random.random() < 0.1:
            # è®­ç»ƒæ—¶éšæœºä¸¢å¼ƒæ¡ä»¶
            conditions = None
            
        if cfg_scale > 1.0 and not self.training:
            # æ¨ç†æ—¶åº”ç”¨CFG
            # 1. æ— æ¡ä»¶é¢„æµ‹
            uncond_output = self.model(x, conditions=None)
            
            # 2. æœ‰æ¡ä»¶é¢„æµ‹  
            cond_output = self.model(x, conditions=conditions)
            
            # 3. CFGæ’å€¼
            output = uncond_output + cfg_scale * (
                cond_output - uncond_output
            )
            return output
        else:
            return self.model(x, conditions=conditions)
```

#### 4.2 é•¿åº¦æ§åˆ¶æœºåˆ¶
```python
class LengthControlledSampling:
    def __init__(self):
        # ä»è®­ç»ƒæ•°æ®å­¦ä¹ é•¿åº¦åˆ†å¸ƒ
        self.length_distributions = {
            'antimicrobial': Normal(20, 8),
            'antifungal': Normal(25, 10),
            'antiviral': Normal(30, 12)
        }
    
    def sample_target_length(self, peptide_type):
        dist = self.length_distributions[peptide_type]
        length = int(dist.sample())
        return torch.clamp(length, 5, 50)
    
    def apply_length_penalty(self, logits, target_length, current_length):
        # åŠ¨æ€è°ƒæ•´ç”Ÿæˆæ¦‚ç‡ä»¥åŒ¹é…ç›®æ ‡é•¿åº¦
        if current_length > target_length:
            # å¢åŠ EOS tokenæ¦‚ç‡
            logits[EOS_TOKEN_ID] += 2.0
        elif current_length < target_length * 0.8:
            # é™ä½EOS tokenæ¦‚ç‡
            logits[EOS_TOKEN_ID] -= 1.0
        return logits
```

### Phase 5: è¯„ä¼°å’ŒéªŒè¯ (é¢„è®¡2å¤©)

#### 5.1 CPL-Diffæ ‡å‡†è¯„ä¼°
```python
evaluation_metrics = {
    'pseudo_perplexity': ESM2BasedPerplexity(),
    'information_entropy': SequenceEntropy(),
    'novelty_ratio': NoveltyAssessment(),
    'structure_plausibility': StructurePlausibility(),
    'activity_prediction': ActivityClassifier()
}

def comprehensive_evaluation(generated_sequences):
    results = {}
    for metric_name, metric_fn in evaluation_metrics.items():
        results[metric_name] = metric_fn(generated_sequences)
    return results
```

#### 5.2 å¯¹æ¯”éªŒè¯è®¾è®¡
```python
# å¯¹æ¯”å®éªŒè®¾è®¡
experiments = {
    'baseline_simple': {
        'model': 'current_simplified_model',
        'training': 'end_to_end'
    },
    'structdiff_separated': {
        'model': 'full_structdiff',
        'training': 'two_stage_separated'
    },
    'structdiff_e2e': {
        'model': 'full_structdiff', 
        'training': 'end_to_end'
    }
}

# è¯„ä¼°ç»´åº¦
evaluation_dimensions = [
    'generation_quality',
    'training_stability', 
    'computational_efficiency',
    'biological_plausibility'
]
```

## ğŸš§ å®æ–½æŒ‘æˆ˜ä¸é£é™©åˆ†æ

### æŠ€æœ¯æŒ‘æˆ˜

#### 1. å†…å­˜å’Œè®¡ç®—èµ„æº
- **ESMFoldé›†æˆ**: éœ€è¦é¢å¤–8GB GPUå†…å­˜
- **è§£å†³æ–¹æ¡ˆ**: é¢„è®¡ç®—ç¼“å­˜ + åŠ¨æ€åŠ è½½
- **é£é™©è¯„çº§**: ä¸­ç­‰

#### 2. è®­ç»ƒç¨³å®šæ€§
- **é˜¶æ®µè½¬æ¢**: é˜¶æ®µ1â†’é˜¶æ®µ2çš„å¹³æ»‘è¿‡æ¸¡
- **è§£å†³æ–¹æ¡ˆ**: æ¸è¿›å¼å­¦ä¹ ç‡è°ƒæ•´
- **é£é™©è¯„çº§**: ä½

#### 3. æ¶æ„å¤æ‚æ€§
- **ç»„ä»¶é›†æˆ**: å¤šä¸ªå­æ¨¡å—çš„åè°ƒ
- **è§£å†³æ–¹æ¡ˆ**: æ¨¡å—åŒ–è®¾è®¡ + å•å…ƒæµ‹è¯•
- **é£é™©è¯„çº§**: ä¸­ç­‰

### é¢„æœŸæ”¶ç›Šè¯„ä¼°

#### æ¨¡å‹èƒ½åŠ›æå‡
```
é¢„æœŸæ”¹è¿›æŒ‡æ ‡:
- ç”Ÿæˆè´¨é‡ (BLEU/ç›¸ä¼¼æ€§): +15-25%
- ç»“æ„åˆç†æ€§ (pLDDT): +20-30%  
- åºåˆ—å¤šæ ·æ€§ (ç†µ): +10-20%
- è®­ç»ƒç¨³å®šæ€§ (æŸå¤±æ³¢åŠ¨): -30-50%
```

#### åŠŸèƒ½æ‰©å±•
- âœ¨ ç»“æ„æ„ŸçŸ¥ç”Ÿæˆ
- âœ¨ é•¿åº¦ç²¾ç¡®æ§åˆ¶  
- âœ¨ æ¡ä»¶å¼•å¯¼ç”Ÿæˆ
- âœ¨ æ›´å¿«æ”¶æ•›é€Ÿåº¦

## ğŸ“‹ å®æ–½æ£€æŸ¥æ¸…å•

### å¼€å‘å‰å‡†å¤‡
- [ ] ç¯å¢ƒä¾èµ–å®Œæ•´æ€§æ£€æŸ¥
- [ ] GPUå†…å­˜éœ€æ±‚è¯„ä¼° (å»ºè®®16GB+)
- [ ] æ•°æ®é¢„å¤„ç†éªŒè¯
- [ ] å¤‡ä»½å½“å‰å·¥ä½œæ¨¡å‹

### é˜¶æ®µ1å¼€å‘
- [ ] ESMFoldé›†æˆæ–¹æ¡ˆç¡®å®š
- [ ] ç»“æ„ç¼–ç å™¨é‡æ–°æ¿€æ´»
- [ ] åˆ†ç¦»å¼è®­ç»ƒè„šæœ¬é€‚é…
- [ ] é…ç½®æ–‡ä»¶ç³»ç»Ÿé‡æ„

### é˜¶æ®µ2å¼€å‘  
- [ ] åºåˆ—è§£ç å™¨å®ç°
- [ ] CFGæœºåˆ¶é›†æˆ
- [ ] é•¿åº¦æ§åˆ¶ç³»ç»Ÿ
- [ ] è®­ç»ƒpipelineå®Œæ•´æ€§æµ‹è¯•

### éªŒè¯å’Œä¼˜åŒ–
- [ ] å¯¹æ¯”å®éªŒè®¾è®¡
- [ ] CPL-Diffè¯„ä¼°å®æ–½
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] æ–‡æ¡£å’Œç¤ºä¾‹æ›´æ–°

## ğŸ¯ æˆåŠŸæ ‡å‡†å®šä¹‰

### æœ€å°å¯è¡Œç›®æ ‡ (MVP)
1. **åˆ†ç¦»å¼è®­ç»ƒæˆåŠŸè¿è¡Œ**: ä¸¤é˜¶æ®µè®­ç»ƒæ— æŠ¥é”™å®Œæˆ
2. **ç»“æ„ç‰¹å¾æ­£å¸¸å·¥ä½œ**: ç»“æ„ç¼–ç å™¨è¾“å‡ºåˆç†ç‰¹å¾
3. **ç”Ÿæˆè´¨é‡ä¸ä½äºç°æœ‰**: è‡³å°‘ä¿æŒå½“å‰ç®€åŒ–æ¨¡å‹æ°´å¹³

### ç†æƒ³ç›®æ ‡
1. **ç”Ÿæˆè´¨é‡æ˜¾è‘—æå‡**: å¤šé¡¹æŒ‡æ ‡æ”¹å–„20%+
2. **è®­ç»ƒæ•ˆç‡æå‡**: æ”¶æ•›é€Ÿåº¦å¿«30%+
3. **åŠŸèƒ½å®Œæ•´æ€§**: CFGã€é•¿åº¦æ§åˆ¶ç­‰é«˜çº§åŠŸèƒ½æ­£å¸¸å·¥ä½œ

### éªŒæ”¶æ ‡å‡†
```python
acceptance_criteria = {
    'training_completion': True,
    'generation_quality_improvement': '>= 15%',
    'training_stability': 'loss_variance < 0.1',
    'feature_completeness': {
        'cfg_guidance': True,
        'length_control': True,
        'structure_awareness': True
    }
}
```

## ğŸ”¬ åç»­ç ”ç©¶æ–¹å‘

### çŸ­æœŸä¼˜åŒ– (1-2ä¸ªæœˆ)
- æ›´é«˜æ•ˆçš„ç»“æ„ç‰¹å¾ç¼“å­˜ç­–ç•¥
- è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒåº¦ä¼˜åŒ–
- æ›´ç²¾ç»†çš„é•¿åº¦æ§åˆ¶ç®—æ³•

### ä¸­æœŸæ‰©å±• (3-6ä¸ªæœˆ)  
- å¤šæ¨¡æ€æ¡ä»¶ç”Ÿæˆ (åºåˆ—+ç»“æ„+åŠŸèƒ½)
- å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ç”Ÿæˆç­–ç•¥
- æ›´å¤§è§„æ¨¡æ¨¡å‹æ¶æ„æ¢ç´¢

### é•¿æœŸæ„¿æ™¯ (6ä¸ªæœˆ+)
- ä¸å®éªŒéªŒè¯çš„é—­ç¯ä¼˜åŒ–
- ç‰¹å®šåº”ç”¨é¢†åŸŸçš„ä¸“é—¨åŒ–æ¨¡å‹
- ç”Ÿç‰©åŠŸèƒ½å¯¼å‘çš„ç”Ÿæˆä¼˜åŒ–

---

## ğŸ“š å‚è€ƒèµ„æº

### æ ¸å¿ƒè®ºæ–‡
1. CPL-DiffåŸå§‹è®ºæ–‡åŠå®ç°
2. StructDiffæ¶æ„è®¾è®¡æ–‡æ¡£
3. Classifier-Free Guidanceç†è®ºåŸºç¡€

### ä»£ç èµ„æº
- `scripts/train_separated.py`: åˆ†ç¦»å¼è®­ç»ƒè„šæœ¬
- `structdiff/training/separated_training.py`: è®­ç»ƒç®¡ç†å™¨
- `configs/separated_training.yaml`: å‚è€ƒé…ç½®

### è¯„ä¼°åŸºå‡†
- CPL-Diffæ ‡å‡†è¯„ä¼°å¥—ä»¶
- ESM2-basedè´¨é‡æŒ‡æ ‡
- ç»“æ„åˆç†æ€§éªŒè¯å·¥å…·

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¥æœŸ**: 2025å¹´7æœˆ10æ—¥  
**æœ€åæ›´æ–°**: 2025å¹´7æœˆ10æ—¥  
**çŠ¶æ€**: è§„åˆ’é˜¶æ®µ â†’ å¾…å®æ–½ 