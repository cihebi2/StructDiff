# evaluate_generated.py
import torch
from omegaconf import OmegaConf, DictConfig
from structdiff.models.structdiff import StructDiff
from structdiff.metrics import compute_sequence_metrics
import pandas as pd
import numpy as np

# æ·»åŠ  DictConfig åˆ°å®‰å…¨å…¨å±€å˜é‡åˆ—è¡¨
torch.serialization.add_safe_globals([DictConfig])

def main():
    print("æ­£åœ¨è¯„ä¼°ç”Ÿæˆçš„åºåˆ—...")
    
    # åŠ è½½æ¨¡å‹é…ç½®
    config = OmegaConf.load("configs/small_model.yaml")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = StructDiff(config).to(device)
    
    # å°è¯•åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    try:
        checkpoint = torch.load("checkpoints/model_epoch_10.pth", map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        print("âœ… æˆåŠŸåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹")
    except FileNotFoundError:
        print("âš ï¸  æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
    except Exception as e:
        print(f"âš ï¸  åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        print("ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
    
    model.eval()
    
    # ç”Ÿæˆåºåˆ—ç”¨äºè¯„ä¼°
    print("æ­£åœ¨ç”Ÿæˆåºåˆ—...")
    with torch.no_grad():
        samples = model.sample(
            batch_size=20,  # ç”Ÿæˆæ›´å¤šåºåˆ—ç”¨äºè¯„ä¼°
            seq_length=20,
            sampling_method='ddpm',
            temperature=1.0,
            progress_bar=True
        )
    
    generated_sequences = samples['sequences']
    print(f"ç”Ÿæˆäº† {len(generated_sequences)} ä¸ªåºåˆ—")
    
    # åŠ è½½å‚è€ƒåºåˆ—
    try:
        ref_df = pd.read_csv("data/processed/train.csv")  # ä½¿ç”¨è®­ç»ƒæ•°æ®ä½œä¸ºå‚è€ƒ
        reference_sequences = ref_df['sequence'].tolist()[:100]  # å–å‰100ä¸ªä½œä¸ºå‚è€ƒ
        print(f"åŠ è½½äº† {len(reference_sequences)} ä¸ªå‚è€ƒåºåˆ—")
    except FileNotFoundError:
        print("âš ï¸  å‚è€ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å‚è€ƒåºåˆ—")
        # ä½¿ç”¨ä¸€äº›å…¸å‹çš„æŠ—èŒè‚½åºåˆ—ä½œä¸ºå‚è€ƒ
        reference_sequences = [
            "GLRKRLRKFRNKIKYLRPRRN",
            "KWKLFKKIEKVGQNIRDGIVGGAAYAAGKYA", 
            "GIGKFLHSAKKFGKAFVGEIMNS",
            "GIGDPVTCLKSGAICHPVFCG",
            "FLPIIAKLLGLL",
            "FMNDNEDFFVA",
            "QITDVQGWGEDAPDQYAYQPKFNAFING",
            "DHMVYVYKVMYQMQNHIEC",
            "KMRYTPVRHY",
            "RHICQEEVKSDN"
        ]
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    print("\næ­£åœ¨è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    metrics = compute_sequence_metrics(generated_sequences, reference_sequences)
    
    print("\nğŸ“Š è¯„ä¼°ç»“æœ:")
    print("=" * 50)
    for metric, value in metrics.items():
        print(f"{metric}: {value:.4f}")
    
    # æ˜¾ç¤ºä¸€äº›ç”Ÿæˆçš„åºåˆ—ç¤ºä¾‹
    print("\nğŸ“‹ ç”Ÿæˆåºåˆ—ç¤ºä¾‹:")
    print("=" * 50)
    for i, seq in enumerate(generated_sequences[:10]):
        quality_score = samples['scores'][i].item()
        print(f"{i+1:2d}: {seq:30s} (è´¨é‡åˆ†æ•°: {quality_score:.3f})")
    
    # ä¿å­˜ç»“æœ
    results = {
        'generated_sequences': generated_sequences,
        'metrics': {k: float(v) for k, v in metrics.items()},  # è½¬æ¢ä¸º Python float
        'quality_scores': [float(score) for score in samples['scores'].tolist()]  # è½¬æ¢ä¸º Python float
    }
    
    import json
    with open('evaluation_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ° evaluation_results.json")

if __name__ == "__main__":
    main()