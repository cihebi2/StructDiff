#!/usr/bin/env python3

import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
from pathlib import Path
import sys
import os
from tqdm import tqdm
import logging
import json
from torch.optim.lr_scheduler import CosineAnnealingLR

# Add project root to path
sys.path.append('/home/qlyu/sequence/StructDiff-7.0.0')

from structdiff.data.dataset import PeptideStructureDataset
from structdiff.utils.config import load_config
from structdiff.utils.logger import setup_logger, get_logger
from structdiff.models.structdiff import StructDiff
from structdiff.diffusion.gaussian_diffusion import GaussianDiffusion
from torch.utils.data import DataLoader

def custom_collate_fn(batch):
    """è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°ï¼Œå¤„ç†å¯èƒ½çš„ç»“æ„ç‰¹å¾ä¸åŒ¹é…"""
    # æ”¶é›†æ‰€æœ‰é”®
    keys = batch[0].keys()
    result = {}
    
    for key in keys:
        if key == 'structures':
            # è·³è¿‡ç»“æ„ç‰¹å¾ï¼Œé¿å…å½¢çŠ¶ä¸åŒ¹é…é—®é¢˜
            continue
        elif key == 'sequence':
            # å­—ç¬¦ä¸²åˆ—è¡¨
            result[key] = [item[key] for item in batch]
        else:
            # å¼ é‡å †å 
            result[key] = torch.stack([item[key] for item in batch])
    
    return result

def create_model_and_diffusion(config):
    """åˆ›å»ºæ¨¡å‹å’Œæ‰©æ•£è¿‡ç¨‹"""
    # åˆ›å»ºæ¨¡å‹
    model = StructDiff(config.model)
    
    # åˆ›å»ºæ‰©æ•£è¿‡ç¨‹
    diffusion = GaussianDiffusion(
        num_timesteps=config.diffusion.num_timesteps,
        noise_schedule=config.diffusion.noise_schedule,
        beta_start=config.diffusion.beta_start,
        beta_end=config.diffusion.beta_end
    )
    
    return model, diffusion

def training_step(model, diffusion, batch, device):
    """æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤"""
    # Move batch to device
    for key in batch:
        if isinstance(batch[key], torch.Tensor):
            batch[key] = batch[key].to(device)
    
    # Get sequence embeddings
    seq_embeddings = model.sequence_encoder(
        batch['sequences'], 
        attention_mask=batch['attention_mask']
    ).last_hidden_state
    
    # Sample timesteps
    batch_size = seq_embeddings.shape[0]
    timesteps = torch.randint(0, diffusion.num_timesteps, (batch_size,), device=device)
    
    # Add noise
    noise = torch.randn_like(seq_embeddings)
    noisy_embeddings = diffusion.q_sample(seq_embeddings, timesteps, noise)
    
    # Create conditions
    conditions = {'peptide_type': batch['label']}
    
    # Forward pass through denoiser
    predicted_noise, _ = model.denoiser(
        noisy_embeddings,
        timesteps,
        batch['attention_mask'],
        structure_features=None,  # æ˜ç¡®è®¾ç½®ä¸ºNone
        conditions=conditions
    )
    
    # Compute loss
    loss = nn.functional.mse_loss(predicted_noise, noise)
    
    return loss

def validation_step(model, diffusion, val_loader, device, logger):
    """æ‰§è¡ŒéªŒè¯æ­¥éª¤"""
    model.eval()
    val_losses = []
    
    with torch.no_grad():
        for batch in val_loader:
            loss = training_step(model, diffusion, batch, device)
            val_losses.append(loss.item())
    
    avg_val_loss = np.mean(val_losses)
    logger.info(f"éªŒè¯æŸå¤±: {avg_val_loss:.6f}")
    
    model.train()
    return avg_val_loss

def save_checkpoint(model, optimizer, scheduler, epoch, loss, checkpoint_path, logger):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'loss': loss,
    }, checkpoint_path)
    logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")

def full_train():
    """å®Œæ•´çš„200 epochè®­ç»ƒå‡½æ•°"""
    print("ğŸš€ å¼€å§‹å®Œæ•´çš„200 epochè®­ç»ƒ...")
    
    # Setup logging
    output_dir = "/home/qlyu/sequence/StructDiff-7.0.0/outputs/full_training_200"
    os.makedirs(output_dir, exist_ok=True)
    
    setup_logger(
        level=logging.INFO,
        log_file=f"{output_dir}/training.log"
    )
    logger = get_logger(__name__)
    
    try:
        # Load config
        config_path = "/home/qlyu/sequence/StructDiff-7.0.0/configs/separated_training.yaml"
        config = load_config(config_path)
        
        # å¼ºåˆ¶ç¦ç”¨ç»“æ„é¢„æµ‹
        config.data.use_predicted_structures = False
        
        logger.info("âœ… é…ç½®åŠ è½½æˆåŠŸï¼Œå·²ç¦ç”¨ç»“æ„é¢„æµ‹")
        
        # Create datasets
        train_dataset = PeptideStructureDataset(
            data_path="/home/qlyu/sequence/StructDiff-7.0.0/data/processed/train.csv",
            config=config,
            is_training=True
        )
        
        val_dataset = PeptideStructureDataset(
            data_path="/home/qlyu/sequence/StructDiff-7.0.0/data/processed/val.csv",
            config=config,
            is_training=False
        )
        
        logger.info(f"âœ… æ•°æ®é›†åˆ›å»ºæˆåŠŸï¼Œè®­ç»ƒé›†: {len(train_dataset)}, éªŒè¯é›†: {len(val_dataset)}")
        
        # Create dataloaders with custom collate function
        batch_size = 16  # é€‚ä¸­çš„æ‰¹æ¬¡å¤§å°
        train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size,
            shuffle=True,
            collate_fn=custom_collate_fn,
            num_workers=0  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
        )
        
        val_loader = DataLoader(
            val_dataset, 
            batch_size=batch_size,
            shuffle=False,
            collate_fn=custom_collate_fn,
            num_workers=0
        )
        
        logger.info("âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        
        # Create model and diffusion
        model, diffusion = create_model_and_diffusion(config)
        logger.info(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
        
        # Move to GPU (when CUDA_VISIBLE_DEVICES=1, the visible device index is 0)
        device = torch.device('cuda:0')
        model = model.to(device)
        logger.info("âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°GPU")
        
        # Create optimizer and scheduler
        optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)
        scheduler = CosineAnnealingLR(optimizer, T_max=200, eta_min=1e-6)
        
        # Training parameters
        num_epochs = 200
        save_every = 10  # æ¯10ä¸ªepochä¿å­˜ä¸€æ¬¡
        validate_every = 5  # æ¯5ä¸ªepochéªŒè¯ä¸€æ¬¡
        
        logger.info(f"ğŸ¯ å¼€å§‹è®­ç»ƒï¼Œå…± {num_epochs} ä¸ªepoch")
        logger.info(f"æ‰¹æ¬¡å¤§å°: {batch_size}, å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']}")
        
        # Training metrics
        train_losses = []
        val_losses = []
        best_val_loss = float('inf')
        
        for epoch in range(num_epochs):
            epoch_loss = 0.0
            num_batches = 0
            
            # Training loop
            model.train()
            progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")
            
            for batch_idx, batch in enumerate(progress_bar):
                optimizer.zero_grad()
                
                # Training step
                loss = training_step(model, diffusion, batch, device)
                
                # Backward pass
                loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                # Optimizer step
                optimizer.step()
                
                # Update metrics
                epoch_loss += loss.item()
                num_batches += 1
                
                # Update progress bar
                progress_bar.set_postfix({
                    'loss': f'{loss.item():.6f}',
                    'avg_loss': f'{epoch_loss/num_batches:.6f}',
                    'lr': f'{optimizer.param_groups[0]["lr"]:.2e}'
                })
                
                # Log every 50 batches
                if batch_idx % 50 == 0:
                    logger.info(f"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.6f}")
            
            # Update learning rate
            scheduler.step()
            
            avg_train_loss = epoch_loss / num_batches
            train_losses.append(avg_train_loss)
            
            logger.info(f"Epoch {epoch+1} å®Œæˆï¼Œå¹³å‡è®­ç»ƒæŸå¤±: {avg_train_loss:.6f}, å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")
            
            # Validation
            if (epoch + 1) % validate_every == 0:
                val_loss = validation_step(model, diffusion, val_loader, device, logger)
                val_losses.append(val_loss)
                
                # Save best model
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_model_path = f"{output_dir}/best_model.pt"
                    torch.save(model.state_dict(), best_model_path)
                    logger.info(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_model_path} (éªŒè¯æŸå¤±: {val_loss:.6f})")
            
            # Save checkpoint
            if (epoch + 1) % save_every == 0:
                checkpoint_path = f"{output_dir}/checkpoint_epoch_{epoch+1}.pt"
                save_checkpoint(model, optimizer, scheduler, epoch + 1, avg_train_loss, checkpoint_path, logger)
                
                # Save training metrics
                metrics = {
                    'epoch': epoch + 1,
                    'train_losses': train_losses,
                    'val_losses': val_losses,
                    'best_val_loss': best_val_loss
                }
                metrics_path = f"{output_dir}/training_metrics.json"
                with open(metrics_path, 'w') as f:
                    json.dump(metrics, f, indent=2)
        
        logger.info("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        
        # Save final model
        final_model_path = f"{output_dir}/final_model_epoch_{num_epochs}.pt"
        torch.save(model.state_dict(), final_model_path)
        logger.info(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
        
        # Save final metrics
        final_metrics = {
            'total_epochs': num_epochs,
            'final_train_loss': train_losses[-1],
            'best_val_loss': best_val_loss,
            'train_losses': train_losses,
            'val_losses': val_losses
        }
        
        final_metrics_path = f"{output_dir}/final_metrics.json"
        with open(final_metrics_path, 'w') as f:
            json.dump(final_metrics, f, indent=2)
        
        logger.info(f"è®­ç»ƒæ‘˜è¦:")
        logger.info(f"  æ€»epochæ•°: {num_epochs}")
        logger.info(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.6f}")
        logger.info(f"  æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        logger.info(f"  æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise

if __name__ == "__main__":
    full_train() 