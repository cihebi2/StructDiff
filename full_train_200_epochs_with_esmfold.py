#!/usr/bin/env python3

import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
from pathlib import Path
import sys
import os
from tqdm import tqdm
import logging
import json
from torch.optim.lr_scheduler import CosineAnnealingLR

# Add project root to path
sys.path.append('/home/qlyu/sequence/StructDiff-7.0.0')

from structdiff.data.dataset import PeptideStructureDataset
from structdiff.utils.config import load_config
from structdiff.utils.logger import setup_logger, get_logger
from structdiff.models.structdiff import StructDiff
from structdiff.diffusion.gaussian_diffusion import GaussianDiffusion
from structdiff.models.esmfold_wrapper import ESMFoldWrapper
from torch.utils.data import DataLoader

def custom_collate_fn(batch):
    """è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°ï¼Œå¤„ç†ç»“æ„ç‰¹å¾"""
    keys = batch[0].keys()
    result = {}
    
    for key in keys:
        if key == 'sequence':
            # å­—ç¬¦ä¸²åˆ—è¡¨
            result[key] = [item[key] for item in batch]
        elif key == 'structures':
            # å¤„ç†ç»“æ„ç‰¹å¾ - å¦‚æœå­˜åœ¨çš„è¯
            structures = []
            for item in batch:
                if key in item and item[key] is not None:
                    structures.append(item[key])
                else:
                    structures.append(None)
            result[key] = structures
        else:
            # å¼ é‡å †å 
            result[key] = torch.stack([item[key] for item in batch])
    
    return result

def create_model_and_diffusion(config):
    """åˆ›å»ºæ¨¡å‹å’Œæ‰©æ•£è¿‡ç¨‹"""
    # åˆ›å»ºæ¨¡å‹
    model = StructDiff(config.model)
    
    # åˆ›å»ºæ‰©æ•£è¿‡ç¨‹
    diffusion = GaussianDiffusion(
        num_timesteps=config.diffusion.num_timesteps,
        noise_schedule=config.diffusion.noise_schedule,
        beta_start=config.diffusion.beta_start,
        beta_end=config.diffusion.beta_end
    )
    
    return model, diffusion

def training_step(model, diffusion, batch, device, esmfold_wrapper):
    """æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤ï¼ŒåŒ…å«ç»“æ„ç‰¹å¾"""
    # Move batch to device
    for key in batch:
        if isinstance(batch[key], torch.Tensor):
            batch[key] = batch[key].to(device)
    
    # Get sequence embeddings
    seq_embeddings = model.sequence_encoder(
        batch['sequences'], 
        attention_mask=batch['attention_mask']
    ).last_hidden_state
    
    # Sample timesteps
    batch_size = seq_embeddings.shape[0]
    timesteps = torch.randint(0, diffusion.num_timesteps, (batch_size,), device=device)
    
    # Add noise
    noise = torch.randn_like(seq_embeddings)
    noisy_embeddings = diffusion.q_sample(seq_embeddings, timesteps, noise)
    
    # Create conditions
    conditions = {'peptide_type': batch['label']}
    
    # Prepare structure features
    structure_features = None
    if 'structures' in batch and batch['structures'] is not None:
        # å¤„ç†ç»“æ„ç‰¹å¾ - å¦‚æœæ‰¹æ¬¡ä¸­æœ‰ç»“æ„æ•°æ®
        try:
            # ç®€å•å¤„ç†ï¼šä¸ºæ¯ä¸ªåºåˆ—é¢„æµ‹ç»“æ„
            structure_features = []
            for i, seq in enumerate(batch['sequence']):
                if batch['structures'][i] is not None:
                    structure_features.append(batch['structures'][i])
                else:
                    # å®æ—¶é¢„æµ‹ç»“æ„
                    try:
                        struct_feat = esmfold_wrapper.predict_structure(seq)
                        structure_features.append(struct_feat)
                    except:
                        structure_features.append(None)
        except Exception as e:
            print(f"ç»“æ„ç‰¹å¾å¤„ç†é”™è¯¯: {e}")
            structure_features = None
    
    # Forward pass through denoiser
    predicted_noise, _ = model.denoiser(
        noisy_embeddings,
        timesteps,
        batch['attention_mask'],
        structure_features=structure_features,
        conditions=conditions
    )
    
    # Compute loss
    loss = nn.functional.mse_loss(predicted_noise, noise)
    
    return loss

def validation_step(model, diffusion, val_loader, device, esmfold_wrapper, logger):
    """æ‰§è¡ŒéªŒè¯æ­¥éª¤"""
    model.eval()
    val_losses = []
    
    with torch.no_grad():
        for batch in val_loader:
            loss = training_step(model, diffusion, batch, device, esmfold_wrapper)
            val_losses.append(loss.item())
    
    avg_val_loss = np.mean(val_losses)
    logger.info(f"éªŒè¯æŸå¤±: {avg_val_loss:.6f}")
    
    model.train()
    return avg_val_loss

def save_checkpoint(model, optimizer, scheduler, epoch, loss, checkpoint_path, logger):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'loss': loss,
    }, checkpoint_path)
    logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")

def full_train_with_esmfold():
    """å®Œæ•´çš„200 epochè®­ç»ƒå‡½æ•°ï¼Œå¯ç”¨ESMFold"""
    print("ğŸš€ å¼€å§‹å®Œæ•´çš„200 epochè®­ç»ƒï¼ˆå¯ç”¨ESMFoldï¼‰...")
    
    # Setup logging
    output_dir = "/home/qlyu/sequence/StructDiff-7.0.0/outputs/full_training_200_esmfold"
    os.makedirs(output_dir, exist_ok=True)
    
    setup_logger(
        level=logging.INFO,
        log_file=f"{output_dir}/training.log"
    )
    logger = get_logger(__name__)
    
    try:
        # Load config
        config_path = "/home/qlyu/sequence/StructDiff-7.0.0/configs/separated_training.yaml"
        config = load_config(config_path)
        
        # å¯ç”¨ç»“æ„é¢„æµ‹
        config.data.use_predicted_structures = True
        config.model.structure_encoder.use_esmfold = True
        
        logger.info("âœ… é…ç½®åŠ è½½æˆåŠŸï¼Œå·²å¯ç”¨ESMFoldç»“æ„é¢„æµ‹")
        
        # åˆå§‹åŒ–ESMFold
        device = torch.device('cuda:0')
        logger.info("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–ESMFold...")
        esmfold_wrapper = ESMFoldWrapper(device=device)
        
        if esmfold_wrapper.available:
            logger.info("âœ… ESMFoldåˆå§‹åŒ–æˆåŠŸ")
        else:
            logger.error("âŒ ESMFoldåˆå§‹åŒ–å¤±è´¥")
            raise RuntimeError("ESMFoldä¸å¯ç”¨")
        
        # Create datasets with ESMFold
        train_dataset = PeptideStructureDataset(
            data_path="/home/qlyu/sequence/StructDiff-7.0.0/data/processed/train.csv",
            config=config,
            is_training=True,
            shared_esmfold=esmfold_wrapper
        )
        
        val_dataset = PeptideStructureDataset(
            data_path="/home/qlyu/sequence/StructDiff-7.0.0/data/processed/val.csv",
            config=config,
            is_training=False,
            shared_esmfold=esmfold_wrapper
        )
        
        logger.info(f"âœ… æ•°æ®é›†åˆ›å»ºæˆåŠŸï¼Œè®­ç»ƒé›†: {len(train_dataset)}, éªŒè¯é›†: {len(val_dataset)}")
        
        # Create dataloaders with custom collate function
        batch_size = 8  # å‡å°æ‰¹æ¬¡å¤§å°ä»¥é€‚åº”ESMFoldçš„æ˜¾å­˜éœ€æ±‚
        train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size,
            shuffle=True,
            collate_fn=custom_collate_fn,
            num_workers=0  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
        )
        
        val_loader = DataLoader(
            val_dataset, 
            batch_size=batch_size,
            shuffle=False,
            collate_fn=custom_collate_fn,
            num_workers=0
        )
        
        logger.info("âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        
        # Create model and diffusion
        model, diffusion = create_model_and_diffusion(config)
        logger.info(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
        
        # Move to GPU
        model = model.to(device)
        logger.info("âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°GPU")
        
        # Create optimizer and scheduler
        optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-5)  # é™ä½å­¦ä¹ ç‡
        scheduler = CosineAnnealingLR(optimizer, T_max=200, eta_min=1e-6)
        
        # Training parameters
        num_epochs = 200
        save_every = 10  # æ¯10ä¸ªepochä¿å­˜ä¸€æ¬¡
        validate_every = 5  # æ¯5ä¸ªepochéªŒè¯ä¸€æ¬¡
        
        logger.info(f"ğŸ¯ å¼€å§‹è®­ç»ƒï¼Œå…± {num_epochs} ä¸ªepoch")
        logger.info(f"æ‰¹æ¬¡å¤§å°: {batch_size}, å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']}")
        logger.info(f"é¢„è®¡æ˜¾å­˜ä½¿ç”¨: 15-20GB (ESMFold + æ¨¡å‹)")
        
        # Training metrics
        train_losses = []
        val_losses = []
        best_val_loss = float('inf')
        
        for epoch in range(num_epochs):
            epoch_loss = 0.0
            num_batches = 0
            
            # Training loop
            model.train()
            progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")
            
            for batch_idx, batch in enumerate(progress_bar):
                optimizer.zero_grad()
                
                # Training step
                loss = training_step(model, diffusion, batch, device, esmfold_wrapper)
                
                # Backward pass
                loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                # Optimizer step
                optimizer.step()
                
                # Update metrics
                epoch_loss += loss.item()
                num_batches += 1
                
                # Update progress bar
                progress_bar.set_postfix({
                    'loss': f'{loss.item():.6f}',
                    'avg_loss': f'{epoch_loss/num_batches:.6f}',
                    'lr': f'{optimizer.param_groups[0]["lr"]:.2e}',
                    'gpu_mem': f'{torch.cuda.memory_allocated(device)//1024**3}GB'
                })
                
                # Log every 20 batches (æ›´é¢‘ç¹çš„æ—¥å¿—)
                if batch_idx % 20 == 0:
                    logger.info(f"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.6f}, GPU Memory: {torch.cuda.memory_allocated(device)//1024**3}GB")
                
                # æ¸…ç†æ˜¾å­˜
                if batch_idx % 10 == 0:
                    torch.cuda.empty_cache()
            
            # Update learning rate
            scheduler.step()
            
            avg_train_loss = epoch_loss / num_batches
            train_losses.append(avg_train_loss)
            
            logger.info(f"Epoch {epoch+1} å®Œæˆï¼Œå¹³å‡è®­ç»ƒæŸå¤±: {avg_train_loss:.6f}, å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")
            
            # Validation
            if (epoch + 1) % validate_every == 0:
                val_loss = validation_step(model, diffusion, val_loader, device, esmfold_wrapper, logger)
                val_losses.append(val_loss)
                
                # Save best model
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_model_path = f"{output_dir}/best_model.pt"
                    torch.save(model.state_dict(), best_model_path)
                    logger.info(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_model_path} (éªŒè¯æŸå¤±: {val_loss:.6f})")
            
            # Save checkpoint
            if (epoch + 1) % save_every == 0:
                checkpoint_path = f"{output_dir}/checkpoint_epoch_{epoch+1}.pt"
                save_checkpoint(model, optimizer, scheduler, epoch + 1, avg_train_loss, checkpoint_path, logger)
                
                # Save training metrics
                metrics = {
                    'epoch': epoch + 1,
                    'train_losses': train_losses,
                    'val_losses': val_losses,
                    'best_val_loss': best_val_loss
                }
                metrics_path = f"{output_dir}/training_metrics.json"
                with open(metrics_path, 'w') as f:
                    json.dump(metrics, f, indent=2)
        
        logger.info("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        
        # Save final model
        final_model_path = f"{output_dir}/final_model_epoch_{num_epochs}.pt"
        torch.save(model.state_dict(), final_model_path)
        logger.info(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
        
        # Save final metrics
        final_metrics = {
            'total_epochs': num_epochs,
            'final_train_loss': train_losses[-1],
            'best_val_loss': best_val_loss,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'esmfold_enabled': True
        }
        
        final_metrics_path = f"{output_dir}/final_metrics.json"
        with open(final_metrics_path, 'w') as f:
            json.dump(final_metrics, f, indent=2)
        
        logger.info(f"è®­ç»ƒæ‘˜è¦:")
        logger.info(f"  æ€»epochæ•°: {num_epochs}")
        logger.info(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.6f}")
        logger.info(f"  æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        logger.info(f"  æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        logger.info(f"  ESMFoldå·²å¯ç”¨: âœ…")
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise

if __name__ == "__main__":
    full_train_with_esmfold() 