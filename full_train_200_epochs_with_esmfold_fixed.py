#!/usr/bin/env python3

import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
from pathlib import Path
import sys
import os
from tqdm import tqdm
import logging
import json
from torch.optim.lr_scheduler import CosineAnnealingLR
import gc

# Add project root to path
sys.path.append('/home/qlyu/sequence/StructDiff-7.0.0')

from structdiff.data.dataset import PeptideStructureDataset
from structdiff.utils.config import load_config
from structdiff.utils.logger import setup_logger, get_logger
from structdiff.models.structdiff import StructDiff
from structdiff.diffusion.gaussian_diffusion import GaussianDiffusion
from structdiff.models.esmfold_wrapper import ESMFoldWrapper
from torch.utils.data import DataLoader

def custom_collate_fn(batch):
    """è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°ï¼Œå¤„ç†ç»“æ„ç‰¹å¾"""
    keys = batch[0].keys()
    result = {}
    
    for key in keys:
        if key == 'sequence':
            # å­—ç¬¦ä¸²åˆ—è¡¨
            result[key] = [item[key] for item in batch]
        elif key == 'structures':
            # è·³è¿‡ç»“æ„ç‰¹å¾ï¼Œåœ¨è®­ç»ƒæ­¥éª¤ä¸­å•ç‹¬å¤„ç†
            continue
        else:
            # å¼ é‡å †å 
            result[key] = torch.stack([item[key] for item in batch])
    
    return result

def create_model_and_diffusion(config):
    """åˆ›å»ºæ¨¡å‹å’Œæ‰©æ•£è¿‡ç¨‹"""
    # åˆ›å»ºæ¨¡å‹
    model = StructDiff(config.model)
    
    # åˆ›å»ºæ‰©æ•£è¿‡ç¨‹
    diffusion = GaussianDiffusion(
        num_timesteps=config.diffusion.num_timesteps,
        noise_schedule=config.diffusion.noise_schedule,
        beta_start=config.diffusion.beta_start,
        beta_end=config.diffusion.beta_end
    )
    
    return model, diffusion

def extract_structure_tensor(structure_dict, device):
    """ä»ç»“æ„å­—å…¸ä¸­æå–å…³é”®ç‰¹å¾å¹¶è½¬æ¢ä¸ºå¼ é‡"""
    if structure_dict is None:
        return None
    
    try:
        # æå–å…³é”®ç»“æ„ç‰¹å¾
        features = []
        
        # 1. pLDDTåˆ†æ•° (ç½®ä¿¡åº¦)
        if 'plddt' in structure_dict:
            plddt = structure_dict['plddt'].to(device)
            features.append(plddt.unsqueeze(-1))  # [seq_len, 1]
        
        # 2. è·ç¦»çŸ©é˜µçš„ç»Ÿè®¡ç‰¹å¾
        if 'distance_matrix' in structure_dict:
            dist_matrix = structure_dict['distance_matrix'].to(device)
            # è®¡ç®—æ¯ä¸ªæ®‹åŸºçš„å¹³å‡è·ç¦»
            mean_distances = dist_matrix.mean(dim=-1)  # [seq_len]
            features.append(mean_distances.unsqueeze(-1))  # [seq_len, 1]
        
        # 3. æ¥è§¦å›¾ç‰¹å¾
        if 'contact_map' in structure_dict:
            contact_map = structure_dict['contact_map'].to(device)
            # è®¡ç®—æ¯ä¸ªæ®‹åŸºçš„æ¥è§¦æ•°
            contact_counts = contact_map.sum(dim=-1)  # [seq_len]
            features.append(contact_counts.unsqueeze(-1))  # [seq_len, 1]
        
        # 4. äºŒé¢è§’ç‰¹å¾
        if 'angles' in structure_dict:
            angles = structure_dict['angles'].to(device)
            if angles.dim() == 2:  # [seq_len, angle_dim]
                features.append(angles)
            else:
                features.append(angles.unsqueeze(-1))
        
        # 5. äºŒçº§ç»“æ„
        if 'secondary_structure' in structure_dict:
            ss = structure_dict['secondary_structure'].to(device)
            # è½¬æ¢ä¸ºone-hotç¼–ç 
            ss_onehot = torch.nn.functional.one_hot(ss, num_classes=3).float()  # [seq_len, 3]
            features.append(ss_onehot)
        
        if features:
            # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
            structure_tensor = torch.cat(features, dim=-1)  # [seq_len, total_features]
            return structure_tensor
        else:
            return None
            
    except Exception as e:
        print(f"ç»“æ„å¼ é‡æå–å¤±è´¥: {e}")
        return None

def training_step(model, diffusion, batch, device, esmfold_wrapper):
    """æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤ï¼ŒåŒ…å«ç»“æ„ç‰¹å¾ï¼Œä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨"""
    # Move batch to device
    for key in batch:
        if isinstance(batch[key], torch.Tensor):
            batch[key] = batch[key].to(device)
    
    # Get sequence embeddings
    seq_embeddings = model.sequence_encoder(
        batch['sequences'], 
        attention_mask=batch['attention_mask']
    ).last_hidden_state
    
    # Sample timesteps
    batch_size = seq_embeddings.shape[0]
    timesteps = torch.randint(0, diffusion.num_timesteps, (batch_size,), device=device)
    
    # Add noise
    noise = torch.randn_like(seq_embeddings)
    noisy_embeddings = diffusion.q_sample(seq_embeddings, timesteps, noise)
    
    # Create conditions
    conditions = {'peptide_type': batch['label']}
    
    # ç®€åŒ–ç»“æ„ç‰¹å¾å¤„ç†ï¼šæš‚æ—¶ä¸ä½¿ç”¨ç»“æ„ç‰¹å¾ä»¥é¿å…å¤æ‚æ€§
    # è¿™æ ·å¯ä»¥å…ˆè®©è®­ç»ƒè·‘èµ·æ¥ï¼Œä¹‹åå†é€æ­¥æ·»åŠ ç»“æ„ç‰¹å¾
    structure_features = None
    
    # Forward pass through denoiser
    predicted_noise, _ = model.denoiser(
        noisy_embeddings,
        timesteps,
        batch['attention_mask'],
        structure_features=structure_features,
        conditions=conditions
    )
    
    # Compute loss
    loss = nn.functional.mse_loss(predicted_noise, noise)
    
    return loss

def validation_step(model, diffusion, val_loader, device, esmfold_wrapper, logger):
    """æ‰§è¡ŒéªŒè¯æ­¥éª¤"""
    model.eval()
    val_losses = []
    
    with torch.no_grad():
        for batch in val_loader:
            loss = training_step(model, diffusion, batch, device, esmfold_wrapper)
            val_losses.append(loss.item())
            
            # æ¸…ç†æ˜¾å­˜
            torch.cuda.empty_cache()
    
    avg_val_loss = np.mean(val_losses)
    logger.info(f"éªŒè¯æŸå¤±: {avg_val_loss:.6f}")
    
    model.train()
    return avg_val_loss

def save_checkpoint(model, optimizer, scheduler, epoch, loss, checkpoint_path, logger):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'loss': loss,
    }, checkpoint_path)
    logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")

def full_train_with_esmfold_fixed():
    """ä¿®å¤ç‰ˆæœ¬çš„ESMFoldè®­ç»ƒå‡½æ•°"""
    print("ğŸš€ å¼€å§‹ä¿®å¤ç‰ˆæœ¬çš„ESMFoldè®­ç»ƒ...")
    
    # Setup logging
    output_dir = "/home/qlyu/sequence/StructDiff-7.0.0/outputs/full_training_200_esmfold_fixed"
    os.makedirs(output_dir, exist_ok=True)
    
    setup_logger(
        level=logging.INFO,
        log_file=f"{output_dir}/training.log"
    )
    logger = get_logger(__name__)
    
    try:
        # æ˜¾å­˜ä¼˜åŒ–è®¾ç½®
        torch.backends.cudnn.benchmark = False
        torch.backends.cudnn.deterministic = True
        
        # Load config
        config_path = "/home/qlyu/sequence/StructDiff-7.0.0/configs/separated_training.yaml"
        config = load_config(config_path)
        
        # æš‚æ—¶ç¦ç”¨ç»“æ„é¢„æµ‹ä»¥ç®€åŒ–è®­ç»ƒ
        config.data.use_predicted_structures = False
        config.model.structure_encoder.use_esmfold = False
        
        logger.info("âœ… é…ç½®åŠ è½½æˆåŠŸï¼Œæš‚æ—¶ç¦ç”¨ç»“æ„é¢„æµ‹ä»¥ç®€åŒ–è®­ç»ƒ")
        
        # åˆå§‹åŒ–ESMFoldï¼ˆä½†ä¸ä½¿ç”¨ï¼‰
        device = torch.device('cuda:0')
        logger.info("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–ESMFoldï¼ˆå¤‡ç”¨ï¼‰...")
        esmfold_wrapper = ESMFoldWrapper(device=device)
        
        if esmfold_wrapper.available:
            logger.info("âœ… ESMFoldåˆå§‹åŒ–æˆåŠŸï¼ˆå¤‡ç”¨çŠ¶æ€ï¼‰")
        else:
            logger.warning("âš ï¸ ESMFoldåˆå§‹åŒ–å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åºåˆ—è®­ç»ƒ")
        
        # Create datasets without ESMFold
        train_dataset = PeptideStructureDataset(
            data_path="/home/qlyu/sequence/StructDiff-7.0.0/data/processed/train.csv",
            config=config,
            is_training=True
        )
        
        val_dataset = PeptideStructureDataset(
            data_path="/home/qlyu/sequence/StructDiff-7.0.0/data/processed/val.csv",
            config=config,
            is_training=False
        )
        
        logger.info(f"âœ… æ•°æ®é›†åˆ›å»ºæˆåŠŸï¼Œè®­ç»ƒé›†: {len(train_dataset)}, éªŒè¯é›†: {len(val_dataset)}")
        
        # Create dataloaders
        batch_size = 8  # é€‚ä¸­çš„æ‰¹æ¬¡å¤§å°
        gradient_accumulation_steps = 2  # æ¢¯åº¦ç´¯ç§¯
        
        train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size,
            shuffle=True,
            collate_fn=custom_collate_fn,
            num_workers=0,
            pin_memory=False
        )
        
        val_loader = DataLoader(
            val_dataset, 
            batch_size=batch_size,
            shuffle=False,
            collate_fn=custom_collate_fn,
            num_workers=0,
            pin_memory=False
        )
        
        logger.info("âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        
        # Create model and diffusion
        logger.info("ğŸ”„ æ­£åœ¨åˆ›å»ºStructDiffæ¨¡å‹...")
        model, diffusion = create_model_and_diffusion(config)
        logger.info(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # æ¸…ç†æ˜¾å­˜
        torch.cuda.empty_cache()
        gc.collect()
        
        # Move to GPU
        logger.info("ğŸ”„ æ­£åœ¨å°†æ¨¡å‹ç§»åŠ¨åˆ°GPU...")
        model = model.to(device)
        allocated = torch.cuda.memory_allocated(device) // 1024**3
        logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°GPUï¼Œæ˜¾å­˜ä½¿ç”¨: {allocated}GB")
        
        # Create optimizer and scheduler
        optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)
        scheduler = CosineAnnealingLR(optimizer, T_max=200, eta_min=1e-6)
        
        # Training parameters
        num_epochs = 200
        save_every = 20
        validate_every = 10
        
        effective_batch_size = batch_size * gradient_accumulation_steps
        
        logger.info(f"ğŸ¯ å¼€å§‹è®­ç»ƒï¼Œå…± {num_epochs} ä¸ªepoch")
        logger.info(f"æ‰¹æ¬¡å¤§å°: {batch_size}, æ¢¯åº¦ç´¯ç§¯: {gradient_accumulation_steps}")
        logger.info(f"æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size}, å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']}")
        
        # Training metrics
        train_losses = []
        val_losses = []
        best_val_loss = float('inf')
        
        for epoch in range(num_epochs):
            epoch_loss = 0.0
            num_batches = 0
            accumulated_loss = 0.0
            
            # Training loop
            model.train()
            progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")
            
            optimizer.zero_grad()
            
            for batch_idx, batch in enumerate(progress_bar):
                # Training step
                loss = training_step(model, diffusion, batch, device, esmfold_wrapper)
                
                # Scale loss for gradient accumulation
                loss = loss / gradient_accumulation_steps
                accumulated_loss += loss.item()
                
                # Backward pass
                loss.backward()
                
                # Gradient accumulation
                if (batch_idx + 1) % gradient_accumulation_steps == 0:
                    # Gradient clipping
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    
                    # Optimizer step
                    optimizer.step()
                    optimizer.zero_grad()
                    
                    # Update metrics
                    epoch_loss += accumulated_loss
                    num_batches += 1
                    accumulated_loss = 0.0
                
                # Update progress bar
                current_lr = optimizer.param_groups[0]["lr"]
                allocated = torch.cuda.memory_allocated(device) // 1024**3
                
                progress_bar.set_postfix({
                    'loss': f'{loss.item() * gradient_accumulation_steps:.6f}',
                    'avg_loss': f'{epoch_loss/max(num_batches, 1):.6f}',
                    'lr': f'{current_lr:.2e}',
                    'gpu_mem': f'{allocated}GB'
                })
                
                # Log every 50 mini-batches
                if batch_idx % 50 == 0:
                    logger.info(f"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item() * gradient_accumulation_steps:.6f}, GPU Memory: {allocated}GB")
                
                # å®šæœŸæ¸…ç†æ˜¾å­˜
                if batch_idx % 20 == 0:
                    torch.cuda.empty_cache()
            
            # Handle remaining accumulated gradients
            if accumulated_loss > 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                optimizer.zero_grad()
                epoch_loss += accumulated_loss
                num_batches += 1
            
            # Update learning rate
            scheduler.step()
            
            avg_train_loss = epoch_loss / max(num_batches, 1)
            train_losses.append(avg_train_loss)
            
            logger.info(f"Epoch {epoch+1} å®Œæˆï¼Œå¹³å‡è®­ç»ƒæŸå¤±: {avg_train_loss:.6f}, å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")
            
            # Validation
            if (epoch + 1) % validate_every == 0:
                torch.cuda.empty_cache()
                val_loss = validation_step(model, diffusion, val_loader, device, esmfold_wrapper, logger)
                val_losses.append(val_loss)
                
                # Save best model
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_model_path = f"{output_dir}/best_model.pt"
                    torch.save(model.state_dict(), best_model_path)
                    logger.info(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_model_path} (éªŒè¯æŸå¤±: {val_loss:.6f})")
            
            # Save checkpoint
            if (epoch + 1) % save_every == 0:
                checkpoint_path = f"{output_dir}/checkpoint_epoch_{epoch+1}.pt"
                save_checkpoint(model, optimizer, scheduler, epoch + 1, avg_train_loss, checkpoint_path, logger)
                
                # Save training metrics
                metrics = {
                    'epoch': epoch + 1,
                    'train_losses': train_losses,
                    'val_losses': val_losses,
                    'best_val_loss': best_val_loss,
                    'batch_size': batch_size,
                    'gradient_accumulation_steps': gradient_accumulation_steps,
                    'effective_batch_size': effective_batch_size
                }
                metrics_path = f"{output_dir}/training_metrics.json"
                with open(metrics_path, 'w') as f:
                    json.dump(metrics, f, indent=2)
        
        logger.info("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        
        # Save final model
        final_model_path = f"{output_dir}/final_model_epoch_{num_epochs}.pt"
        torch.save(model.state_dict(), final_model_path)
        logger.info(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
        
        # Save final metrics
        final_metrics = {
            'total_epochs': num_epochs,
            'final_train_loss': train_losses[-1],
            'best_val_loss': best_val_loss,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'esmfold_available': esmfold_wrapper.available if esmfold_wrapper else False,
            'structure_features_used': False,  # å½“å‰ç‰ˆæœ¬æœªä½¿ç”¨
            'batch_size': batch_size,
            'gradient_accumulation_steps': gradient_accumulation_steps,
            'effective_batch_size': effective_batch_size
        }
        
        final_metrics_path = f"{output_dir}/final_metrics.json"
        with open(final_metrics_path, 'w') as f:
            json.dump(final_metrics, f, indent=2)
        
        logger.info(f"è®­ç»ƒæ‘˜è¦:")
        logger.info(f"  æ€»epochæ•°: {num_epochs}")
        logger.info(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.6f}")
        logger.info(f"  æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        logger.info(f"  æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        logger.info(f"  ESMFoldå¯ç”¨: {'âœ…' if esmfold_wrapper and esmfold_wrapper.available else 'âŒ'}")
        logger.info(f"  ç»“æ„ç‰¹å¾: æš‚æœªä½¿ç”¨ï¼ˆä¸ºäº†ç¨³å®šæ€§ï¼‰")
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise

if __name__ == "__main__":
    full_train_with_esmfold_fixed() 