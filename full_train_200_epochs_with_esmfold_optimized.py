#!/usr/bin/env python3

import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
from pathlib import Path
import sys
import os
from tqdm import tqdm
import logging
import json
from torch.optim.lr_scheduler import CosineAnnealingLR
import gc

# Add project root to path
sys.path.append('/home/qlyu/sequence/StructDiff-7.0.0')

from structdiff.data.dataset import PeptideStructureDataset
from structdiff.utils.config import load_config
from structdiff.utils.logger import setup_logger, get_logger
from structdiff.models.structdiff import StructDiff
from structdiff.diffusion.gaussian_diffusion import GaussianDiffusion
from structdiff.models.esmfold_wrapper import ESMFoldWrapper
from torch.utils.data import DataLoader

def custom_collate_fn(batch):
    """è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°ï¼Œå¤„ç†ç»“æ„ç‰¹å¾"""
    keys = batch[0].keys()
    result = {}
    
    for key in keys:
        if key == 'sequence':
            # å­—ç¬¦ä¸²åˆ—è¡¨
            result[key] = [item[key] for item in batch]
        elif key == 'structures':
            # å¤„ç†ç»“æ„ç‰¹å¾ - å¦‚æœå­˜åœ¨çš„è¯
            structures = []
            for item in batch:
                if key in item and item[key] is not None:
                    structures.append(item[key])
                else:
                    structures.append(None)
            result[key] = structures
        else:
            # å¼ é‡å †å 
            result[key] = torch.stack([item[key] for item in batch])
    
    return result

def create_model_and_diffusion(config):
    """åˆ›å»ºæ¨¡å‹å’Œæ‰©æ•£è¿‡ç¨‹"""
    # åˆ›å»ºæ¨¡å‹
    model = StructDiff(config.model)
    
    # åˆ›å»ºæ‰©æ•£è¿‡ç¨‹
    diffusion = GaussianDiffusion(
        num_timesteps=config.diffusion.num_timesteps,
        noise_schedule=config.diffusion.noise_schedule,
        beta_start=config.diffusion.beta_start,
        beta_end=config.diffusion.beta_end
    )
    
    return model, diffusion

def training_step(model, diffusion, batch, device, esmfold_wrapper):
    """æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤ï¼ŒåŒ…å«ç»“æ„ç‰¹å¾ï¼Œä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨"""
    # Move batch to device
    for key in batch:
        if isinstance(batch[key], torch.Tensor):
            batch[key] = batch[key].to(device)
    
    # Get sequence embeddings with gradient checkpointing
    with torch.cuda.amp.autocast(enabled=False):  # æš‚æ—¶ç¦ç”¨æ··åˆç²¾åº¦é¿å…å…¼å®¹æ€§é—®é¢˜
        seq_embeddings = model.sequence_encoder(
            batch['sequences'], 
            attention_mask=batch['attention_mask']
        ).last_hidden_state
    
    # Sample timesteps
    batch_size = seq_embeddings.shape[0]
    timesteps = torch.randint(0, diffusion.num_timesteps, (batch_size,), device=device)
    
    # Add noise
    noise = torch.randn_like(seq_embeddings)
    noisy_embeddings = diffusion.q_sample(seq_embeddings, timesteps, noise)
    
    # Create conditions
    conditions = {'peptide_type': batch['label']}
    
    # Prepare structure features - æ‰¹é‡å¤„ç†ä»¥æé«˜æ•ˆç‡
    structure_features = None
    if 'structures' in batch and batch['structures'] is not None:
        try:
            # æ”¶é›†éœ€è¦é¢„æµ‹çš„åºåˆ—
            sequences_to_predict = []
            indices_to_predict = []
            
            for i, seq in enumerate(batch['sequence']):
                if batch['structures'][i] is None:
                    sequences_to_predict.append(seq)
                    indices_to_predict.append(i)
            
            # æ‰¹é‡é¢„æµ‹ç»“æ„ï¼ˆå¦‚æœæœ‰éœ€è¦é¢„æµ‹çš„ï¼‰
            if sequences_to_predict:
                with torch.no_grad():  # ç»“æ„é¢„æµ‹ä¸éœ€è¦æ¢¯åº¦
                    predicted_structures = []
                    for seq in sequences_to_predict:
                        try:
                            struct_feat = esmfold_wrapper.predict_structure(seq)
                            predicted_structures.append(struct_feat)
                        except Exception as e:
                            print(f"ç»“æ„é¢„æµ‹å¤±è´¥: {e}")
                            predicted_structures.append(None)
                
                # å¡«å……é¢„æµ‹ç»“æœ
                structure_features = []
                pred_idx = 0
                for i, seq in enumerate(batch['sequence']):
                    if batch['structures'][i] is not None:
                        structure_features.append(batch['structures'][i])
                    else:
                        if pred_idx < len(predicted_structures):
                            structure_features.append(predicted_structures[pred_idx])
                            pred_idx += 1
                        else:
                            structure_features.append(None)
            else:
                structure_features = batch['structures']
                
        except Exception as e:
            print(f"ç»“æ„ç‰¹å¾å¤„ç†é”™è¯¯: {e}")
            structure_features = None
    
    # Forward pass through denoiser
    predicted_noise, _ = model.denoiser(
        noisy_embeddings,
        timesteps,
        batch['attention_mask'],
        structure_features=structure_features,
        conditions=conditions
    )
    
    # Compute loss
    loss = nn.functional.mse_loss(predicted_noise, noise)
    
    # æ¸…ç†ä¸­é—´å˜é‡
    del seq_embeddings, noisy_embeddings, noise
    if structure_features is not None:
        del structure_features
    
    return loss

def validation_step(model, diffusion, val_loader, device, esmfold_wrapper, logger):
    """æ‰§è¡ŒéªŒè¯æ­¥éª¤"""
    model.eval()
    val_losses = []
    
    with torch.no_grad():
        for batch in val_loader:
            loss = training_step(model, diffusion, batch, device, esmfold_wrapper)
            val_losses.append(loss.item())
            
            # æ¸…ç†æ˜¾å­˜
            torch.cuda.empty_cache()
    
    avg_val_loss = np.mean(val_losses)
    logger.info(f"éªŒè¯æŸå¤±: {avg_val_loss:.6f}")
    
    model.train()
    return avg_val_loss

def save_checkpoint(model, optimizer, scheduler, epoch, loss, checkpoint_path, logger):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'loss': loss,
    }, checkpoint_path)
    logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")

def full_train_with_esmfold_optimized():
    """æ˜¾å­˜ä¼˜åŒ–çš„200 epochè®­ç»ƒå‡½æ•°ï¼Œå¯ç”¨ESMFold"""
    print("ğŸš€ å¼€å§‹æ˜¾å­˜ä¼˜åŒ–çš„200 epochè®­ç»ƒï¼ˆå¯ç”¨ESMFoldï¼‰...")
    
    # Setup logging
    output_dir = "/home/qlyu/sequence/StructDiff-7.0.0/outputs/full_training_200_esmfold_optimized"
    os.makedirs(output_dir, exist_ok=True)
    
    setup_logger(
        level=logging.INFO,
        log_file=f"{output_dir}/training.log"
    )
    logger = get_logger(__name__)
    
    try:
        # æ˜¾å­˜ä¼˜åŒ–è®¾ç½®
        torch.backends.cudnn.benchmark = False  # å‡å°‘å†…å­˜ç¢ç‰‡
        torch.backends.cudnn.deterministic = True
        
        # Load config
        config_path = "/home/qlyu/sequence/StructDiff-7.0.0/configs/separated_training.yaml"
        config = load_config(config_path)
        
        # å¯ç”¨ç»“æ„é¢„æµ‹
        config.data.use_predicted_structures = True
        config.model.structure_encoder.use_esmfold = True
        
        logger.info("âœ… é…ç½®åŠ è½½æˆåŠŸï¼Œå·²å¯ç”¨ESMFoldç»“æ„é¢„æµ‹")
        
        # åˆå§‹åŒ–ESMFold
        device = torch.device('cuda:0')
        logger.info("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–ESMFold...")
        esmfold_wrapper = ESMFoldWrapper(device=device)
        
        if esmfold_wrapper.available:
            logger.info("âœ… ESMFoldåˆå§‹åŒ–æˆåŠŸ")
            # æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨
            allocated = torch.cuda.memory_allocated(device) // 1024**3
            logger.info(f"ESMFoldæ˜¾å­˜ä½¿ç”¨: {allocated}GB")
        else:
            logger.error("âŒ ESMFoldåˆå§‹åŒ–å¤±è´¥")
            raise RuntimeError("ESMFoldä¸å¯ç”¨")
        
        # Create datasets with ESMFold
        train_dataset = PeptideStructureDataset(
            data_path="/home/qlyu/sequence/StructDiff-7.0.0/data/processed/train.csv",
            config=config,
            is_training=True,
            shared_esmfold=esmfold_wrapper
        )
        
        val_dataset = PeptideStructureDataset(
            data_path="/home/qlyu/sequence/StructDiff-7.0.0/data/processed/val.csv",
            config=config,
            is_training=False,
            shared_esmfold=esmfold_wrapper
        )
        
        logger.info(f"âœ… æ•°æ®é›†åˆ›å»ºæˆåŠŸï¼Œè®­ç»ƒé›†: {len(train_dataset)}, éªŒè¯é›†: {len(val_dataset)}")
        
        # Create dataloaders with very small batch size for memory optimization
        batch_size = 2  # æå°çš„æ‰¹æ¬¡å¤§å°ä»¥é€‚åº”ESMFoldçš„æ˜¾å­˜éœ€æ±‚
        gradient_accumulation_steps = 8  # ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ¥æ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹æ¬¡
        
        train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size,
            shuffle=True,
            collate_fn=custom_collate_fn,
            num_workers=0,
            pin_memory=False  # ç¦ç”¨pin_memoryä»¥èŠ‚çœæ˜¾å­˜
        )
        
        val_loader = DataLoader(
            val_dataset, 
            batch_size=batch_size,
            shuffle=False,
            collate_fn=custom_collate_fn,
            num_workers=0,
            pin_memory=False
        )
        
        logger.info("âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        
        # Create model and diffusion
        logger.info("ğŸ”„ æ­£åœ¨åˆ›å»ºStructDiffæ¨¡å‹...")
        model, diffusion = create_model_and_diffusion(config)
        logger.info(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
        
        # æ¸…ç†æ˜¾å­˜
        torch.cuda.empty_cache()
        gc.collect()
        
        # Move to GPU with memory monitoring
        logger.info("ğŸ”„ æ­£åœ¨å°†æ¨¡å‹ç§»åŠ¨åˆ°GPU...")
        try:
            model = model.to(device)
            allocated = torch.cuda.memory_allocated(device) // 1024**3
            logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°GPUï¼Œæ€»æ˜¾å­˜ä½¿ç”¨: {allocated}GB")
        except RuntimeError as e:
            if "out of memory" in str(e):
                logger.error(f"âŒ æ˜¾å­˜ä¸è¶³: {e}")
                logger.info("ğŸ’¡ å°è¯•ä½¿ç”¨CPUè®­ç»ƒæˆ–å‡å°‘æ¨¡å‹å¤§å°")
                raise
            else:
                raise
        
        # Create optimizer and scheduler with lower learning rate
        optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-5)  # æ›´ä½çš„å­¦ä¹ ç‡
        scheduler = CosineAnnealingLR(optimizer, T_max=200, eta_min=1e-7)
        
        # Training parameters
        num_epochs = 200
        save_every = 20  # å‡å°‘ä¿å­˜é¢‘ç‡
        validate_every = 10  # å‡å°‘éªŒè¯é¢‘ç‡
        
        effective_batch_size = batch_size * gradient_accumulation_steps
        
        logger.info(f"ğŸ¯ å¼€å§‹è®­ç»ƒï¼Œå…± {num_epochs} ä¸ªepoch")
        logger.info(f"å®é™…æ‰¹æ¬¡å¤§å°: {batch_size}, æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
        logger.info(f"æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size}, å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']}")
        logger.info(f"é¢„è®¡æ˜¾å­˜ä½¿ç”¨: 18-22GB (ESMFold + æ¨¡å‹)")
        
        # Training metrics
        train_losses = []
        val_losses = []
        best_val_loss = float('inf')
        
        for epoch in range(num_epochs):
            epoch_loss = 0.0
            num_batches = 0
            accumulated_loss = 0.0
            
            # Training loop
            model.train()
            progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")
            
            optimizer.zero_grad()
            
            for batch_idx, batch in enumerate(progress_bar):
                # Training step
                loss = training_step(model, diffusion, batch, device, esmfold_wrapper)
                
                # Scale loss for gradient accumulation
                loss = loss / gradient_accumulation_steps
                accumulated_loss += loss.item()
                
                # Backward pass
                loss.backward()
                
                # Gradient accumulation
                if (batch_idx + 1) % gradient_accumulation_steps == 0:
                    # Gradient clipping
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    
                    # Optimizer step
                    optimizer.step()
                    optimizer.zero_grad()
                    
                    # Update metrics
                    epoch_loss += accumulated_loss
                    num_batches += 1
                    accumulated_loss = 0.0
                
                # Update progress bar
                current_lr = optimizer.param_groups[0]["lr"]
                allocated = torch.cuda.memory_allocated(device) // 1024**3
                
                progress_bar.set_postfix({
                    'loss': f'{loss.item() * gradient_accumulation_steps:.6f}',
                    'avg_loss': f'{epoch_loss/max(num_batches, 1):.6f}',
                    'lr': f'{current_lr:.2e}',
                    'gpu_mem': f'{allocated}GB'
                })
                
                # Log every 50 mini-batches
                if batch_idx % 50 == 0:
                    logger.info(f"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item() * gradient_accumulation_steps:.6f}, GPU Memory: {allocated}GB")
                
                # å®šæœŸæ¸…ç†æ˜¾å­˜
                if batch_idx % 20 == 0:
                    torch.cuda.empty_cache()
                    gc.collect()
            
            # Handle remaining accumulated gradients
            if accumulated_loss > 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                optimizer.zero_grad()
                epoch_loss += accumulated_loss
                num_batches += 1
            
            # Update learning rate
            scheduler.step()
            
            avg_train_loss = epoch_loss / max(num_batches, 1)
            train_losses.append(avg_train_loss)
            
            logger.info(f"Epoch {epoch+1} å®Œæˆï¼Œå¹³å‡è®­ç»ƒæŸå¤±: {avg_train_loss:.6f}, å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")
            
            # Validation
            if (epoch + 1) % validate_every == 0:
                torch.cuda.empty_cache()  # éªŒè¯å‰æ¸…ç†æ˜¾å­˜
                val_loss = validation_step(model, diffusion, val_loader, device, esmfold_wrapper, logger)
                val_losses.append(val_loss)
                
                # Save best model
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_model_path = f"{output_dir}/best_model.pt"
                    torch.save(model.state_dict(), best_model_path)
                    logger.info(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_model_path} (éªŒè¯æŸå¤±: {val_loss:.6f})")
            
            # Save checkpoint
            if (epoch + 1) % save_every == 0:
                checkpoint_path = f"{output_dir}/checkpoint_epoch_{epoch+1}.pt"
                save_checkpoint(model, optimizer, scheduler, epoch + 1, avg_train_loss, checkpoint_path, logger)
                
                # Save training metrics
                metrics = {
                    'epoch': epoch + 1,
                    'train_losses': train_losses,
                    'val_losses': val_losses,
                    'best_val_loss': best_val_loss,
                    'batch_size': batch_size,
                    'gradient_accumulation_steps': gradient_accumulation_steps,
                    'effective_batch_size': effective_batch_size
                }
                metrics_path = f"{output_dir}/training_metrics.json"
                with open(metrics_path, 'w') as f:
                    json.dump(metrics, f, indent=2)
        
        logger.info("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        
        # Save final model
        final_model_path = f"{output_dir}/final_model_epoch_{num_epochs}.pt"
        torch.save(model.state_dict(), final_model_path)
        logger.info(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
        
        # Save final metrics
        final_metrics = {
            'total_epochs': num_epochs,
            'final_train_loss': train_losses[-1],
            'best_val_loss': best_val_loss,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'esmfold_enabled': True,
            'batch_size': batch_size,
            'gradient_accumulation_steps': gradient_accumulation_steps,
            'effective_batch_size': effective_batch_size
        }
        
        final_metrics_path = f"{output_dir}/final_metrics.json"
        with open(final_metrics_path, 'w') as f:
            json.dump(final_metrics, f, indent=2)
        
        logger.info(f"è®­ç»ƒæ‘˜è¦:")
        logger.info(f"  æ€»epochæ•°: {num_epochs}")
        logger.info(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.6f}")
        logger.info(f"  æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        logger.info(f"  æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        logger.info(f"  ESMFoldå·²å¯ç”¨: âœ…")
        logger.info(f"  æ˜¾å­˜ä¼˜åŒ–: æ‰¹æ¬¡å¤§å°={batch_size}, æ¢¯åº¦ç´¯ç§¯={gradient_accumulation_steps}")
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise

if __name__ == "__main__":
    full_train_with_esmfold_optimized() 