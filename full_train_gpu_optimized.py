#!/usr/bin/env python3
"""
GPUåˆ©ç”¨ç‡ä¼˜åŒ–ç‰ˆæœ¬è®­ç»ƒè„šæœ¬
ç›®æ ‡ï¼šå°†GPUåˆ©ç”¨ç‡ä»20%æå‡åˆ°70%ä»¥ä¸Šï¼Œè®­ç»ƒé€Ÿåº¦æå‡3-5å€
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler
import logging
import time
import gc
from datetime import datetime
import json

# è®¾ç½®ç¯å¢ƒå˜é‡è¿›è¡Œå†…å­˜ä¼˜åŒ–
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:128'

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/home/qlyu/sequence/StructDiff-7.0.0')

from structdiff.data.dataset import PeptideStructureDataset
from structdiff.data.collator import PeptideStructureCollator
from structdiff.utils.config import load_config
from structdiff.utils.logger import setup_logger, get_logger
from structdiff.models.structdiff import StructDiff
from structdiff.diffusion.gaussian_diffusion import GaussianDiffusion
from structdiff.models.esmfold_wrapper import ESMFoldWrapper

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('outputs/gpu_optimized_training/training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def setup_optimized_environment():
    """è®¾ç½®ä¼˜åŒ–ç¯å¢ƒ"""
    # å¯ç”¨PyTorchä¼˜åŒ–
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    
    # è®¾ç½®å¤šçº¿ç¨‹
    torch.set_num_threads(4)
    
    logger.info("âœ… ä¼˜åŒ–ç¯å¢ƒè®¾ç½®å®Œæˆ")

def check_gpu_availability():
    """æ£€æŸ¥GPUå¯ç”¨æ€§å’Œå†…å­˜"""
    if not torch.cuda.is_available():
        raise RuntimeError("âŒ CUDAä¸å¯ç”¨")
    
    device_count = torch.cuda.device_count()
    logger.info(f"ğŸ“Š æ£€æµ‹åˆ° {device_count} ä¸ªGPU")
    
    # é€‰æ‹©æœ€é€‚åˆçš„GPU
    best_gpu = 1  # é»˜è®¤ä½¿ç”¨GPU 1
    max_free_memory = 0
    
    for i in range(device_count):
        props = torch.cuda.get_device_properties(i)
        total_memory = props.total_memory / 1e9
        
        # æ£€æŸ¥å½“å‰å†…å­˜ä½¿ç”¨
        torch.cuda.set_device(i)
        allocated = torch.cuda.memory_allocated() / 1e9
        free_memory = total_memory - allocated
        
        logger.info(f"GPU {i}: {props.name}, æ€»å†…å­˜: {total_memory:.1f}GB, å¯ç”¨: {free_memory:.1f}GB")
        
        if free_memory > max_free_memory and free_memory > 15:  # è‡³å°‘éœ€è¦15GB
            best_gpu = i
            max_free_memory = free_memory
    
    if max_free_memory < 15:
        raise RuntimeError(f"âŒ æ²¡æœ‰è¶³å¤Ÿçš„GPUå†…å­˜ (éœ€è¦è‡³å°‘15GB)")
    
    logger.info(f"ğŸ¯ é€‰æ‹©GPU {best_gpu}è¿›è¡Œè®­ç»ƒ")
    return best_gpu

def create_optimized_data_loaders(config, esmfold_wrapper):
    """åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨"""
    logger.info("ğŸ”„ åˆ›å»ºä¼˜åŒ–æ•°æ®åŠ è½½å™¨...")
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = PeptideDataset(
        data_path=config.data.train_data_path,
        max_length=config.data.max_length,
        structure_prediction_enabled=config.model.structure_prediction_enabled,
        esmfold_wrapper=esmfold_wrapper
    )
    
    val_dataset = PeptideDataset(
        data_path=config.data.val_data_path,
        max_length=config.data.max_length,
        structure_prediction_enabled=config.model.structure_prediction_enabled,
        esmfold_wrapper=esmfold_wrapper
    )
    
    # åˆ›å»ºä¼˜åŒ–çš„collator
    collator = PeptideCollator(
        max_length=config.data.max_length,
        structure_prediction_enabled=config.model.structure_prediction_enabled
    )
    
    # ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨é…ç½®
    optimized_batch_size = 8  # ä»2å¢åŠ åˆ°8
    num_workers = 4          # ä»0å¢åŠ åˆ°4
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=optimized_batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,           # å¯ç”¨å†…å­˜å›ºå®š
        prefetch_factor=2,         # é¢„å–å› å­
        persistent_workers=True,   # æŒä¹…åŒ–å·¥ä½œè¿›ç¨‹
        collate_fn=collator,
        drop_last=True            # ä¸¢å¼ƒæœ€åä¸å®Œæ•´çš„æ‰¹æ¬¡
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=optimized_batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        prefetch_factor=2,
        persistent_workers=True,
        collate_fn=collator,
        drop_last=False
    )
    
    logger.info(f"âœ… ä¼˜åŒ–æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
    logger.info(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {optimized_batch_size} (4å€æå‡)")
    logger.info(f"ğŸ“Š å·¥ä½œè¿›ç¨‹: {num_workers}")
    logger.info(f"ğŸ“Š è®­ç»ƒæ•°æ®é›†: {len(train_dataset)} æ ·æœ¬")
    logger.info(f"ğŸ“Š éªŒè¯æ•°æ®é›†: {len(val_dataset)} æ ·æœ¬")
    
    return train_loader, val_loader

def create_optimized_model(config, device, esmfold_wrapper):
    """åˆ›å»ºä¼˜åŒ–çš„æ¨¡å‹"""
    logger.info("ğŸ”„ åˆ›å»ºä¼˜åŒ–æ¨¡å‹...")
    
    # ä¸´æ—¶ç¦ç”¨æ¨¡å‹å†…éƒ¨ESMFoldåŠ è½½
    original_enabled = config.model.structure_prediction_enabled
    config.model.structure_prediction_enabled = False
    
    # åˆ›å»ºæ¨¡å‹
    model = StructDiff(config.model)
    model = model.to(device)
    
    # æ¢å¤è®¾ç½®å¹¶å¼ºåˆ¶è®¾ç½®ESMFoldå®ä¾‹
    config.model.structure_prediction_enabled = original_enabled
    model.esmfold_wrapper = esmfold_wrapper
    
    # å¯ç”¨è®­ç»ƒæ¨¡å¼
    model.train()
    
    # æ¨¡å‹ç¼–è¯‘ä¼˜åŒ– (PyTorch 2.0+)
    try:
        model = torch.compile(model, mode="reduce-overhead")
        logger.info("âœ… æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–å¯ç”¨")
    except Exception as e:
        logger.warning(f"âš ï¸ æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–å¤±è´¥: {e}")
    
    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    logger.info(f"ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    logger.info(f"  æ€»å‚æ•°: {total_params:,}")
    logger.info(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    return model

def create_optimized_optimizer_and_scheduler(model, config, train_loader):
    """åˆ›å»ºä¼˜åŒ–çš„ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨"""
    # ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config.training.learning_rate,
        weight_decay=0.01,
        betas=(0.9, 0.999),
        eps=1e-8
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer,
        T_0=len(train_loader) * 5,  # 5ä¸ªepochä¸ºä¸€ä¸ªå‘¨æœŸ
        T_mult=2,
        eta_min=1e-6
    )
    
    logger.info("âœ… ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨åˆ›å»ºå®Œæˆ")
    return optimizer, scheduler

def optimized_training_step(model, batch, optimizer, scaler, device, gradient_accumulation_steps=2):
    """ä¼˜åŒ–çš„è®­ç»ƒæ­¥éª¤"""
    model.train()
    
    # ç§»åŠ¨æ•°æ®åˆ°GPU
    for key in batch:
        if torch.is_tensor(batch[key]):
            batch[key] = batch[key].to(device, non_blocking=True)
    
    # åˆ›å»ºéšæœºæ—¶é—´æ­¥
    batch_size = batch['sequences'].size(0)
    timesteps = torch.randint(0, 1000, (batch_size,), device=device)
    
    # å‰å‘ä¼ æ’­ (ä½¿ç”¨æ··åˆç²¾åº¦)
    with autocast():
        outputs = model(
            sequences=batch['sequences'],
            attention_mask=batch['attention_mask'],
            timesteps=timesteps,
            structures=batch.get('structures'),
            conditions=batch.get('conditions'),
            return_loss=True
        )
        loss = outputs['total_loss'] / gradient_accumulation_steps
    
    # åå‘ä¼ æ’­
    scaler.scale(loss).backward()
    
    return outputs, loss.item() * gradient_accumulation_steps

def optimized_training_loop(model, train_loader, val_loader, optimizer, scheduler, scaler, device, config):
    """ä¼˜åŒ–çš„è®­ç»ƒå¾ªç¯"""
    logger.info("ğŸš€ å¼€å§‹GPUä¼˜åŒ–è®­ç»ƒ...")
    
    # è®­ç»ƒé…ç½®
    num_epochs = config.training.num_epochs
    gradient_accumulation_steps = 2  # ä»8å‡å°‘åˆ°2
    log_interval = 10  # æ›´é¢‘ç¹çš„æ—¥å¿—
    
    logger.info(f"ğŸ“Š è®­ç»ƒé…ç½®:")
    logger.info(f"  æ€»epoch: {num_epochs}")
    logger.info(f"  æ‰¹æ¬¡å¤§å°: {train_loader.batch_size}")
    logger.info(f"  æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
    logger.info(f"  æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {train_loader.batch_size * gradient_accumulation_steps}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs("outputs/gpu_optimized_training", exist_ok=True)
    
    # æ€§èƒ½ç›‘æ§
    training_stats = {
        'epoch_times': [],
        'batch_times': [],
        'gpu_utilization': [],
        'memory_usage': []
    }
    
    best_val_loss = float('inf')
    
    for epoch in range(num_epochs):
        epoch_start_time = time.time()
        model.train()
        
        total_loss = 0
        num_batches = 0
        
        for batch_idx, batch in enumerate(train_loader):
            batch_start_time = time.time()
            
            # æ‰§è¡Œä¼˜åŒ–çš„è®­ç»ƒæ­¥éª¤
            outputs, loss = optimized_training_step(
                model, batch, optimizer, scaler, device, gradient_accumulation_steps
            )
            
            total_loss += loss
            num_batches += 1
            
            # æ¢¯åº¦æ›´æ–°
            if (batch_idx + 1) % gradient_accumulation_steps == 0:
                # æ¢¯åº¦è£å‰ª
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                # ä¼˜åŒ–å™¨æ­¥éª¤
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
                
                # å­¦ä¹ ç‡è°ƒåº¦
                scheduler.step()
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            batch_time = time.time() - batch_start_time
            training_stats['batch_times'].append(batch_time)
            
            # è®°å½•GPUå†…å­˜ä½¿ç”¨
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated(device) / 1e9
                training_stats['memory_usage'].append(memory_used)
            
            # æ—¥å¿—è¾“å‡º
            if batch_idx % log_interval == 0:
                current_lr = scheduler.get_last_lr()[0]
                logger.info(
                    f"Epoch {epoch+1}/{num_epochs}, "
                    f"Batch {batch_idx}/{len(train_loader)}, "
                    f"Loss: {loss:.6f}, "
                    f"LR: {current_lr:.2e}, "
                    f"Time: {batch_time:.2f}s, "
                    f"GPU Memory: {memory_used:.1f}GB"
                )
            
            # å®šæœŸæ¸…ç†å†…å­˜
            if batch_idx % 50 == 0:
                torch.cuda.empty_cache()
                gc.collect()
        
        # Epochç»“æŸç»Ÿè®¡
        epoch_time = time.time() - epoch_start_time
        avg_loss = total_loss / num_batches
        avg_batch_time = sum(training_stats['batch_times'][-num_batches:]) / num_batches
        
        training_stats['epoch_times'].append(epoch_time)
        
        logger.info(f"ğŸ“Š Epoch {epoch+1} å®Œæˆ:")
        logger.info(f"  å¹³å‡æŸå¤±: {avg_loss:.6f}")
        logger.info(f"  Epochæ—¶é—´: {epoch_time:.2f}s")
        logger.info(f"  å¹³å‡æ‰¹æ¬¡æ—¶é—´: {avg_batch_time:.2f}s")
        logger.info(f"  é¢„è®¡å‰©ä½™æ—¶é—´: {epoch_time * (num_epochs - epoch - 1) / 3600:.1f}å°æ—¶")
        
        # éªŒè¯
        if epoch % 5 == 0:  # æ¯5ä¸ªepochéªŒè¯ä¸€æ¬¡
            val_loss = validate_model(model, val_loader, device, scaler)
            logger.info(f"ğŸ“Š éªŒè¯æŸå¤±: {val_loss:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'scaler_state_dict': scaler.state_dict(),
                    'best_val_loss': best_val_loss,
                    'training_stats': training_stats
                }, 'outputs/gpu_optimized_training/best_model.pth')
                
                logger.info(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (éªŒè¯æŸå¤±: {best_val_loss:.6f})")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % 10 == 0:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'scaler_state_dict': scaler.state_dict(),
                'training_stats': training_stats
            }, f'outputs/gpu_optimized_training/checkpoint_epoch_{epoch}.pth')
    
    # ä¿å­˜æœ€ç»ˆç»Ÿè®¡
    with open('outputs/gpu_optimized_training/training_stats.json', 'w') as f:
        json.dump(training_stats, f, indent=2)
    
    logger.info("ğŸ‰ GPUä¼˜åŒ–è®­ç»ƒå®Œæˆ!")
    
    # æ€§èƒ½åˆ†æ
    analyze_training_performance(training_stats)

def validate_model(model, val_loader, device, scaler):
    """éªŒè¯æ¨¡å‹"""
    model.eval()
    total_loss = 0
    num_batches = 0
    
    with torch.no_grad():
        for batch in val_loader:
            # ç§»åŠ¨æ•°æ®åˆ°GPU
            for key in batch:
                if torch.is_tensor(batch[key]):
                    batch[key] = batch[key].to(device, non_blocking=True)
            
            batch_size = batch['sequences'].size(0)
            timesteps = torch.randint(0, 1000, (batch_size,), device=device)
            
            with autocast():
                outputs = model(
                    sequences=batch['sequences'],
                    attention_mask=batch['attention_mask'],
                    timesteps=timesteps,
                    structures=batch.get('structures'),
                    conditions=batch.get('conditions'),
                    return_loss=True
                )
                loss = outputs['total_loss']
            
            total_loss += loss.item()
            num_batches += 1
    
    return total_loss / num_batches

def analyze_training_performance(training_stats):
    """åˆ†æè®­ç»ƒæ€§èƒ½"""
    logger.info("ğŸ“Š è®­ç»ƒæ€§èƒ½åˆ†æ:")
    
    if training_stats['batch_times']:
        avg_batch_time = sum(training_stats['batch_times']) / len(training_stats['batch_times'])
        logger.info(f"  å¹³å‡æ‰¹æ¬¡æ—¶é—´: {avg_batch_time:.2f}s")
        
        # ä¸åŸå§‹é…ç½®æ¯”è¾ƒ (åŸå§‹: batch_size=2, çº¦2s/batch)
        original_batch_time = 2.0
        speedup = original_batch_time / avg_batch_time
        logger.info(f"  è®­ç»ƒé€Ÿåº¦æå‡: {speedup:.2f}x")
    
    if training_stats['memory_usage']:
        avg_memory = sum(training_stats['memory_usage']) / len(training_stats['memory_usage'])
        max_memory = max(training_stats['memory_usage'])
        logger.info(f"  å¹³å‡GPUå†…å­˜ä½¿ç”¨: {avg_memory:.1f}GB")
        logger.info(f"  å³°å€¼GPUå†…å­˜ä½¿ç”¨: {max_memory:.1f}GB")

def main():
    """ä¸»å‡½æ•°"""
    try:
        logger.info("ğŸš€ å¯åŠ¨GPUä¼˜åŒ–è®­ç»ƒ...")
        
        # è®¾ç½®ä¼˜åŒ–ç¯å¢ƒ
        setup_optimized_environment()
        
        # æ£€æŸ¥GPU
        device_id = check_gpu_availability()
        device = torch.device(f'cuda:{device_id}')
        
        # åŠ è½½é…ç½®
        config = Config()
        config.model.structure_prediction_enabled = True
        config.training.batch_size = 8  # ä¼˜åŒ–æ‰¹æ¬¡å¤§å°
        config.training.num_epochs = 100
        config.training.learning_rate = 5e-5
        
        logger.info("âœ… é…ç½®åŠ è½½æˆåŠŸï¼Œå·²å¯ç”¨ç»“æ„ç‰¹å¾å’ŒGPUä¼˜åŒ–")
        
        # åˆ›å»ºå…±äº«ESMFoldå®ä¾‹
        logger.info("ğŸ”„ åˆ›å»ºå…±äº«ESMFoldå®ä¾‹...")
        torch.cuda.empty_cache()
        
        esmfold_wrapper = ESMFoldWrapper(device=device)
        logger.info("âœ… å…±äº«ESMFoldå®ä¾‹åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨
        train_loader, val_loader = create_optimized_data_loaders(config, esmfold_wrapper)
        
        # åˆ›å»ºä¼˜åŒ–çš„æ¨¡å‹
        model = create_optimized_model(config, device, esmfold_wrapper)
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        logger.info("ğŸ”„ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
        checkpoint_path = "outputs/sequence_feature_training/best_model.pth"
        if os.path.exists(checkpoint_path):
            checkpoint = torch.load(checkpoint_path, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'], strict=False)
            logger.info("âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
        else:
            logger.warning("âš ï¸ é¢„è®­ç»ƒæ¨¡å‹æœªæ‰¾åˆ°ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
        
        # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        optimizer, scheduler = create_optimized_optimizer_and_scheduler(model, config, train_loader)
        
        # åˆ›å»ºæ¢¯åº¦ç¼©æ”¾å™¨
        scaler = GradScaler()
        
        # å¼€å§‹ä¼˜åŒ–è®­ç»ƒ
        optimized_training_loop(model, train_loader, val_loader, optimizer, scheduler, scaler, device, config)
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise

if __name__ == "__main__":
    main() 