#!/usr/bin/env python3
"""
GPUåˆ©ç”¨ç‡ä¼˜åŒ–ç‰ˆæœ¬è®­ç»ƒè„šæœ¬ - ä¿®å¤ç‰ˆæœ¬
ç›®æ ‡ï¼šå°†GPUåˆ©ç”¨ç‡ä»20%æå‡åˆ°70%ä»¥ä¸Šï¼Œè®­ç»ƒé€Ÿåº¦æå‡3-5å€
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler
import logging
import time
import gc
from datetime import datetime
import json
import numpy as np
from tqdm import tqdm
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡è¿›è¡Œå†…å­˜ä¼˜åŒ–
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:128'

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/home/qlyu/sequence/StructDiff-7.0.0')

from structdiff.data.dataset import PeptideStructureDataset
from structdiff.data.collator import PeptideStructureCollator
from structdiff.utils.config import load_config
from structdiff.utils.logger import setup_logger, get_logger
from structdiff.models.structdiff import StructDiff
from structdiff.diffusion.gaussian_diffusion import GaussianDiffusion
from structdiff.models.esmfold_wrapper import ESMFoldWrapper

def clear_memory():
    """æ¸…ç†GPUå†…å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()

def setup_optimized_environment():
    """è®¾ç½®ä¼˜åŒ–ç¯å¢ƒ"""
    # å¯ç”¨PyTorchä¼˜åŒ–
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    
    # è®¾ç½®å¤šçº¿ç¨‹
    torch.set_num_threads(4)
    
    print("âœ… ä¼˜åŒ–ç¯å¢ƒè®¾ç½®å®Œæˆ")

def check_gpu_availability():
    """æ£€æŸ¥GPUå¯ç”¨æ€§å’Œå†…å­˜"""
    if not torch.cuda.is_available():
        raise RuntimeError("âŒ CUDAä¸å¯ç”¨")
    
    device_count = torch.cuda.device_count()
    print(f"ğŸ“Š æ£€æµ‹åˆ° {device_count} ä¸ªGPU")
    
    # é€‰æ‹©æœ€é€‚åˆçš„GPU
    best_gpu = 1  # é»˜è®¤ä½¿ç”¨GPU 1
    max_free_memory = 0
    
    for i in range(device_count):
        props = torch.cuda.get_device_properties(i)
        total_memory = props.total_memory / 1e9
        
        # æ£€æŸ¥å½“å‰å†…å­˜ä½¿ç”¨
        torch.cuda.set_device(i)
        allocated = torch.cuda.memory_allocated() / 1e9
        free_memory = total_memory - allocated
        
        print(f"GPU {i}: {props.name}, æ€»å†…å­˜: {total_memory:.1f}GB, å¯ç”¨: {free_memory:.1f}GB")
        
        if free_memory > max_free_memory and free_memory > 15:  # è‡³å°‘éœ€è¦15GB
            best_gpu = i
            max_free_memory = free_memory
    
    if max_free_memory < 15:
        raise RuntimeError(f"âŒ æ²¡æœ‰è¶³å¤Ÿçš„GPUå†…å­˜ (éœ€è¦è‡³å°‘15GB)")
    
    print(f"ğŸ¯ é€‰æ‹©GPU {best_gpu}è¿›è¡Œè®­ç»ƒ")
    return best_gpu

def setup_shared_esmfold(device):
    """åˆ›å»ºå…±äº«çš„ESMFoldå®ä¾‹"""
    print("ğŸ”„ åˆ›å»ºå…±äº«ESMFoldå®ä¾‹...")
    
    # æ¸…ç†å†…å­˜
    clear_memory()
    
    try:
        shared_esmfold = ESMFoldWrapper(device=device)
        
        if shared_esmfold.available:
            print("âœ… å…±äº«ESMFoldå®ä¾‹åˆ›å»ºæˆåŠŸ")
            print(f"ESMFoldå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1e9:.1f}GB")
        else:
            print("âŒ ESMFoldå®ä¾‹åˆ›å»ºå¤±è´¥")
            shared_esmfold = None
            
    except Exception as e:
        print(f"âŒ ESMFoldåˆå§‹åŒ–å¤±è´¥: {e}")
        shared_esmfold = None
    
    return shared_esmfold

def create_optimized_data_loaders(config, shared_esmfold):
    """åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨"""
    print("ğŸ”„ åˆ›å»ºä¼˜åŒ–æ•°æ®åŠ è½½å™¨...")
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = PeptideStructureDataset(
        data_path="/home/qlyu/sequence/StructDiff-7.0.0/data/processed/train.csv",
        config=config,
        is_training=True,
        cache_dir="./cache/train",
        shared_esmfold=shared_esmfold
    )
    
    val_dataset = PeptideStructureDataset(
        data_path="/home/qlyu/sequence/StructDiff-7.0.0/data/processed/val.csv",
        config=config,
        is_training=False,
        cache_dir="./cache/val",
        shared_esmfold=shared_esmfold
    )
    
    # åˆ›å»ºæ•°æ®æ•´ç†å™¨
    collator = PeptideStructureCollator(config)
    
    # ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨é…ç½®
    optimized_batch_size = 8  # ä»2å¢åŠ åˆ°8
    num_workers = 4          # ä»0å¢åŠ åˆ°4
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=optimized_batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,           # å¯ç”¨å†…å­˜å›ºå®š
        prefetch_factor=2,         # é¢„å–å› å­
        persistent_workers=True,   # æŒä¹…åŒ–å·¥ä½œè¿›ç¨‹
        collate_fn=collator,
        drop_last=True            # ä¸¢å¼ƒæœ€åä¸å®Œæ•´çš„æ‰¹æ¬¡
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=optimized_batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        prefetch_factor=2,
        persistent_workers=True,
        collate_fn=collator,
        drop_last=False
    )
    
    print(f"âœ… ä¼˜åŒ–æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
    print(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {optimized_batch_size} (4å€æå‡)")
    print(f"ğŸ“Š å·¥ä½œè¿›ç¨‹: {num_workers}")
    print(f"ğŸ“Š è®­ç»ƒæ•°æ®é›†: {len(train_dataset)} æ ·æœ¬")
    print(f"ğŸ“Š éªŒè¯æ•°æ®é›†: {len(val_dataset)} æ ·æœ¬")
    
    return train_loader, val_loader

def create_optimized_model(config, device, shared_esmfold):
    """åˆ›å»ºä¼˜åŒ–çš„æ¨¡å‹"""
    print("ğŸ”„ åˆ›å»ºä¼˜åŒ–æ¨¡å‹...")
    
    # ä¸´æ—¶ç¦ç”¨æ¨¡å‹å†…éƒ¨ESMFoldåŠ è½½
    original_enabled = config.model.structure_encoder.get('use_esmfold', False)
    config.model.structure_encoder.use_esmfold = False
    
    # åˆ›å»ºæ¨¡å‹
    model = StructDiff(config.model).to(device)
    
    # æ¢å¤è®¾ç½®å¹¶å¼ºåˆ¶è®¾ç½®ESMFoldå®ä¾‹
    config.model.structure_encoder.use_esmfold = original_enabled
    
    # å¦‚æœæœ‰å…±äº«çš„ESMFoldå®ä¾‹ï¼Œæ‰‹åŠ¨è®¾ç½®åˆ°æ¨¡å‹ä¸­
    if shared_esmfold and shared_esmfold.available:
        print("ğŸ”— è®¾ç½®å…±äº«ESMFoldå®ä¾‹åˆ°æ¨¡å‹...")
        # è®¾ç½®ESMFoldå®ä¾‹
        if hasattr(model.structure_encoder, 'esmfold'):
            model.structure_encoder.esmfold = shared_esmfold
        if hasattr(model.structure_encoder, '_esmfold'):
            model.structure_encoder._esmfold = shared_esmfold
        
        # ç¡®ä¿ESMFoldè¢«æ ‡è®°ä¸ºå¯ç”¨
        model.structure_encoder.use_esmfold = True
        print("âœ… å…±äº«ESMFoldå®ä¾‹è®¾ç½®å®Œæˆ")
    
    # å¯ç”¨è®­ç»ƒæ¨¡å¼
    model.train()
    
    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    print(f"  æ€»å‚æ•°: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    return model

def move_to_device(obj, device):
    """é€’å½’åœ°å°†å¯¹è±¡ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
    if isinstance(obj, torch.Tensor):
        return obj.to(device, non_blocking=True)
    elif isinstance(obj, dict):
        return {k: move_to_device(v, device) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [move_to_device(item, device) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(move_to_device(item, device) for item in obj)
    else:
        return obj

def optimized_training_step(model, diffusion, batch, device, scaler, gradient_accumulation_steps=2):
    """ä¼˜åŒ–çš„è®­ç»ƒæ­¥éª¤"""
    try:
        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
        batch = move_to_device(batch, device)
        
        # æ£€æŸ¥åŸºæœ¬å­—æ®µ
        if 'sequences' not in batch or 'attention_mask' not in batch:
            return None, float('inf')
        
        # é‡‡æ ·æ—¶é—´æ­¥
        batch_size = batch['sequences'].shape[0]
        timesteps = torch.randint(0, diffusion.num_timesteps, (batch_size,), device=device)
        
        # å‰å‘ä¼ æ’­ (ä½¿ç”¨æ··åˆç²¾åº¦)
        with autocast():
            outputs = model(
                sequences=batch['sequences'],
                attention_mask=batch['attention_mask'],
                timesteps=timesteps,
                structures=batch.get('structures'),
                conditions=batch.get('conditions'),
                return_loss=True
            )
            
            # è·å–æŸå¤±
            if 'total_loss' in outputs:
                loss = outputs['total_loss']
            elif 'diffusion_loss' in outputs:
                loss = outputs['diffusion_loss']
            elif 'loss' in outputs:
                loss = outputs['loss']
            else:
                return None, float('inf')
            
            loss = loss / gradient_accumulation_steps
        
        # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºNaN
        if torch.isnan(loss) or torch.isinf(loss):
            return None, float('inf')
        
        # åå‘ä¼ æ’­
        scaler.scale(loss).backward()
        
        return outputs, loss.item() * gradient_accumulation_steps
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒæ­¥éª¤å¤±è´¥: {e}")
        return None, float('inf')

def optimized_training_loop(model, diffusion, train_loader, val_loader, optimizer, scheduler, scaler, device, num_epochs=100):
    """ä¼˜åŒ–çš„è®­ç»ƒå¾ªç¯"""
    print("ğŸš€ å¼€å§‹GPUä¼˜åŒ–è®­ç»ƒ...")
    
    # è®­ç»ƒé…ç½®
    gradient_accumulation_steps = 2  # ä»8å‡å°‘åˆ°2
    log_interval = 10  # æ›´é¢‘ç¹çš„æ—¥å¿—
    
    print(f"ğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"  æ€»epoch: {num_epochs}")
    print(f"  æ‰¹æ¬¡å¤§å°: {train_loader.batch_size}")
    print(f"  æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
    print(f"  æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {train_loader.batch_size * gradient_accumulation_steps}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "outputs/gpu_optimized_training"
    os.makedirs(output_dir, exist_ok=True)
    
    # æ€§èƒ½ç›‘æ§
    training_stats = {
        'epoch_times': [],
        'batch_times': [],
        'memory_usage': []
    }
    
    best_val_loss = float('inf')
    
    for epoch in range(num_epochs):
        epoch_start_time = time.time()
        model.train()
        
        total_loss = 0
        num_batches = 0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")
        
        for batch_idx, batch in enumerate(progress_bar):
            batch_start_time = time.time()
            
            # æ‰§è¡Œä¼˜åŒ–çš„è®­ç»ƒæ­¥éª¤
            outputs, loss = optimized_training_step(
                model, diffusion, batch, device, scaler, gradient_accumulation_steps
            )
            
            if outputs is not None:
                total_loss += loss
                num_batches += 1
            
            # æ¢¯åº¦æ›´æ–°
            if (batch_idx + 1) % gradient_accumulation_steps == 0:
                # æ¢¯åº¦è£å‰ª
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                # ä¼˜åŒ–å™¨æ­¥éª¤
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
                
                # å­¦ä¹ ç‡è°ƒåº¦
                if scheduler:
                    scheduler.step()
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            batch_time = time.time() - batch_start_time
            training_stats['batch_times'].append(batch_time)
            
            # è®°å½•GPUå†…å­˜ä½¿ç”¨
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated(device) / 1e9
                training_stats['memory_usage'].append(memory_used)
            
            # æ›´æ–°è¿›åº¦æ¡
            if outputs is not None:
                progress_bar.set_postfix({
                    'Loss': f'{loss:.4f}',
                    'GPU_Mem': f'{memory_used:.1f}GB',
                    'Time': f'{batch_time:.2f}s'
                })
            
            # å®šæœŸæ¸…ç†å†…å­˜
            if batch_idx % 50 == 0:
                clear_memory()
        
        # Epochç»“æŸç»Ÿè®¡
        epoch_time = time.time() - epoch_start_time
        avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')
        avg_batch_time = sum(training_stats['batch_times'][-num_batches:]) / num_batches if num_batches > 0 else 0
        
        training_stats['epoch_times'].append(epoch_time)
        
        print(f"ğŸ“Š Epoch {epoch+1} å®Œæˆ:")
        print(f"  å¹³å‡æŸå¤±: {avg_loss:.6f}")
        print(f"  Epochæ—¶é—´: {epoch_time:.2f}s")
        print(f"  å¹³å‡æ‰¹æ¬¡æ—¶é—´: {avg_batch_time:.2f}s")
        print(f"  é¢„è®¡å‰©ä½™æ—¶é—´: {epoch_time * (num_epochs - epoch - 1) / 3600:.1f}å°æ—¶")
        
        # éªŒè¯
        if epoch % 5 == 0:  # æ¯5ä¸ªepochéªŒè¯ä¸€æ¬¡
            val_loss = validate_model(model, diffusion, val_loader, device, scaler)
            print(f"ğŸ“Š éªŒè¯æŸå¤±: {val_loss:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
                    'scaler_state_dict': scaler.state_dict(),
                    'best_val_loss': best_val_loss,
                    'training_stats': training_stats
                }, f'{output_dir}/best_model.pth')
                
                print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (éªŒè¯æŸå¤±: {best_val_loss:.6f})")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % 10 == 0:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
                'scaler_state_dict': scaler.state_dict(),
                'training_stats': training_stats
            }, f'{output_dir}/checkpoint_epoch_{epoch}.pth')
    
    # ä¿å­˜æœ€ç»ˆç»Ÿè®¡
    with open(f'{output_dir}/training_stats.json', 'w') as f:
        json.dump(training_stats, f, indent=2)
    
    print("ğŸ‰ GPUä¼˜åŒ–è®­ç»ƒå®Œæˆ!")
    
    # æ€§èƒ½åˆ†æ
    analyze_training_performance(training_stats)

def validate_model(model, diffusion, val_loader, device, scaler):
    """éªŒè¯æ¨¡å‹"""
    model.eval()
    total_loss = 0
    num_batches = 0
    
    with torch.no_grad():
        for batch in val_loader:
            outputs, loss = optimized_training_step(model, diffusion, batch, device, scaler, 1)
            if outputs is not None:
                total_loss += loss
                num_batches += 1
            
            # æ¸…ç†å†…å­˜
            clear_memory()
    
    avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')
    model.train()
    return avg_loss

def analyze_training_performance(training_stats):
    """åˆ†æè®­ç»ƒæ€§èƒ½"""
    print("ğŸ“Š è®­ç»ƒæ€§èƒ½åˆ†æ:")
    
    if training_stats['batch_times']:
        avg_batch_time = sum(training_stats['batch_times']) / len(training_stats['batch_times'])
        print(f"  å¹³å‡æ‰¹æ¬¡æ—¶é—´: {avg_batch_time:.2f}s")
        
        # ä¸åŸå§‹é…ç½®æ¯”è¾ƒ (åŸå§‹: batch_size=2, çº¦2s/batch)
        original_batch_time = 2.0
        speedup = original_batch_time / avg_batch_time
        print(f"  è®­ç»ƒé€Ÿåº¦æå‡: {speedup:.2f}x")
    
    if training_stats['memory_usage']:
        avg_memory = sum(training_stats['memory_usage']) / len(training_stats['memory_usage'])
        max_memory = max(training_stats['memory_usage'])
        print(f"  å¹³å‡GPUå†…å­˜ä½¿ç”¨: {avg_memory:.1f}GB")
        print(f"  å³°å€¼GPUå†…å­˜ä½¿ç”¨: {max_memory:.1f}GB")

def main():
    """ä¸»å‡½æ•°"""
    try:
        print("ğŸš€ å¯åŠ¨GPUä¼˜åŒ–è®­ç»ƒ...")
        
        # è®¾ç½®ä¼˜åŒ–ç¯å¢ƒ
        setup_optimized_environment()
        
        # æ£€æŸ¥GPU
        device_id = check_gpu_availability()
        device = torch.device(f'cuda:{device_id}')
        
        # åŠ è½½é…ç½®
        config_path = "/home/qlyu/sequence/StructDiff-7.0.0/configs/separated_training.yaml"
        config = load_config(config_path)
        
        # å¯ç”¨ç»“æ„ç‰¹å¾å’ŒGPUä¼˜åŒ–
        config.data.use_predicted_structures = True
        config.model.structure_encoder.use_esmfold = True
        
        print("âœ… é…ç½®åŠ è½½æˆåŠŸï¼Œå·²å¯ç”¨ç»“æ„ç‰¹å¾å’ŒGPUä¼˜åŒ–")
        
        # åˆ›å»ºå…±äº«ESMFoldå®ä¾‹
        shared_esmfold = setup_shared_esmfold(device)
        
        if not shared_esmfold or not shared_esmfold.available:
            print("âŒ å…±äº«ESMFoldå®ä¾‹åˆ›å»ºå¤±è´¥")
            return
        
        # åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨
        train_loader, val_loader = create_optimized_data_loaders(config, shared_esmfold)
        
        # åˆ›å»ºä¼˜åŒ–çš„æ¨¡å‹
        model = create_optimized_model(config, device, shared_esmfold)
        
        # åˆ›å»ºæ‰©æ•£è¿‡ç¨‹
        diffusion = GaussianDiffusion(
            num_timesteps=config.diffusion.num_timesteps,
            noise_schedule=config.diffusion.noise_schedule,
            beta_start=config.diffusion.beta_start,
            beta_end=config.diffusion.beta_end
        )
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        print("ğŸ”„ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
        checkpoint_path = "/home/qlyu/sequence/StructDiff-7.0.0/outputs/full_training_200_esmfold_fixed/best_model.pt"
        if os.path.exists(checkpoint_path):
            checkpoint = torch.load(checkpoint_path, map_location=device)
            model.load_state_dict(checkpoint, strict=False)
            print("âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
        else:
            print("âš ï¸ é¢„è®­ç»ƒæ¨¡å‹æœªæ‰¾åˆ°ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
        
        # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        # åªä¼˜åŒ–éESMFoldå‚æ•°
        trainable_params = []
        for name, param in model.named_parameters():
            if not ('esmfold' in name.lower() and 'structure_encoder' in name):
                trainable_params.append(param)
            else:
                param.requires_grad = False
        
        optimizer = optim.AdamW(
            trainable_params,
            lr=5e-5,
            weight_decay=0.01,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        from torch.optim.lr_scheduler import CosineAnnealingLR
        scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)
        
        # åˆ›å»ºæ¢¯åº¦ç¼©æ”¾å™¨
        scaler = GradScaler()
        
        # å¼€å§‹ä¼˜åŒ–è®­ç»ƒ
        optimized_training_loop(
            model, diffusion, train_loader, val_loader, 
            optimizer, scheduler, scaler, device, num_epochs=100
        )
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise

if __name__ == "__main__":
    main() 