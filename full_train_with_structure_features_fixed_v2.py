#!/usr/bin/env python3

import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
from pathlib import Path
import sys
import os
from tqdm import tqdm
import logging
import json
from torch.optim.lr_scheduler import CosineAnnealingLR
import gc

# Add project root to path
sys.path.append('/home/qlyu/sequence/StructDiff-7.0.0')

from structdiff.data.dataset import PeptideStructureDataset
from structdiff.data.collator import PeptideStructureCollator
from structdiff.utils.config import load_config
from structdiff.utils.logger import setup_logger, get_logger
from structdiff.models.structdiff import StructDiff
from structdiff.diffusion.gaussian_diffusion import GaussianDiffusion
from structdiff.models.esmfold_wrapper import ESMFoldWrapper
from torch.utils.data import DataLoader

def clear_memory():
    """æ¸…ç†GPUå†…å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()

def setup_shared_esmfold(device: torch.device):
    """åˆ›å»ºå…±äº«çš„ESMFoldå®ä¾‹ - åŸºäºæˆåŠŸè„šæœ¬çš„ç­–ç•¥"""
    logger = get_logger(__name__)
    shared_esmfold = None
    
    logger.info("ğŸ”„ æ­£åœ¨åˆ›å»ºå…±äº«ESMFoldå®ä¾‹...")
    
    # æ›´æ¿€è¿›çš„å†…å­˜æ¸…ç†ï¼Œä¸ºESMFoldè…¾å‡ºç©ºé—´
    if torch.cuda.is_available():
        # å¼ºåˆ¶æ¸…ç†æ‰€æœ‰ç¼“å­˜
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥ - å…³é”®ä¼˜åŒ–
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:128'
        
        # å†æ¬¡æ¸…ç†
        torch.cuda.empty_cache()
        
        current_mem = torch.cuda.memory_allocated() / 1024**3
        logger.info(f"ğŸ§¹ ESMFoldåˆå§‹åŒ–å‰GPUå†…å­˜æ¸…ç†å®Œæˆ: {current_mem:.2f}GB")
    
    try:
        # é¦–å…ˆå°è¯•GPU
        shared_esmfold = ESMFoldWrapper(device=device)
        
        if shared_esmfold.available:
            logger.info("âœ… å…±äº«ESMFold GPUå®ä¾‹åˆ›å»ºæˆåŠŸ")
            logger.info(f"ESMFoldå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1e9:.1f}GB")
        else:
            logger.error("âŒ ESMFold GPUå®ä¾‹åˆ›å»ºå¤±è´¥")
            shared_esmfold = None
            
    except Exception as gpu_error:
        logger.warning(f"âš ï¸ ESMFold GPUåˆå§‹åŒ–å¤±è´¥: {gpu_error}")
        
        try:
            # å°è¯•CPU fallback
            logger.info("ğŸ”„ å°è¯•ä½¿ç”¨CPUåˆ›å»ºESMFold...")
            shared_esmfold = ESMFoldWrapper(device='cpu')
            if shared_esmfold.available:
                logger.info("âœ… ESMFold CPUå®ä¾‹åˆ›å»ºæˆåŠŸ")
            else:
                shared_esmfold = None
        except Exception as cpu_error:
            logger.error(f"âŒ ESMFold CPUåˆå§‹åŒ–ä¹Ÿå¤±è´¥: {cpu_error}")
            shared_esmfold = None
    
    return shared_esmfold

def create_model_and_diffusion(config):
    """åˆ›å»ºæ¨¡å‹å’Œæ‰©æ•£è¿‡ç¨‹"""
    # åˆ›å»ºæ¨¡å‹
    model = StructDiff(config.model)
    
    # åˆ›å»ºæ‰©æ•£è¿‡ç¨‹
    diffusion = GaussianDiffusion(
        num_timesteps=config.diffusion.num_timesteps,
        noise_schedule=config.diffusion.noise_schedule,
        beta_start=config.diffusion.beta_start,
        beta_end=config.diffusion.beta_end
    )
    
    return model, diffusion

def setup_model_and_training(config, device, shared_esmfold):
    """è®¾ç½®æ¨¡å‹å’Œè®­ç»ƒç»„ä»¶ - åŸºäºæˆåŠŸè„šæœ¬çš„ç­–ç•¥"""
    logger = get_logger(__name__)
    logger.info("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
    
    try:
        # å¤‡ä»½åŸå§‹é…ç½®
        original_use_esmfold = config.model.structure_encoder.get('use_esmfold', False)
        
        # å¦‚æœå·²æœ‰å…±äº«å®ä¾‹ï¼Œä¸´æ—¶ç¦ç”¨æ¨¡å‹å†…éƒ¨çš„ESMFoldåŠ è½½
        if shared_esmfold and shared_esmfold.available:
            logger.info("ğŸ’¡ ä¸´æ—¶ç¦ç”¨æ¨¡å‹å†…éƒ¨ESMFoldåŠ è½½ä»¥é¿å…å†…å­˜ä¸è¶³...")
            config.model.structure_encoder.use_esmfold = False
        
        # åˆ›å»ºæ¨¡å‹
        model = StructDiff(config.model).to(device)
        
        # æ¢å¤é…ç½®å¹¶è®¾ç½®å…±äº«å®ä¾‹
        config.model.structure_encoder.use_esmfold = original_use_esmfold
        
        # å¦‚æœæœ‰å…±äº«çš„ESMFoldå®ä¾‹ï¼Œæ‰‹åŠ¨è®¾ç½®åˆ°æ¨¡å‹ä¸­
        if shared_esmfold and shared_esmfold.available:
            logger.info("ğŸ”— æ­£åœ¨å°†å…±äº« ESMFold å®ä¾‹è®¾ç½®åˆ°æ¨¡å‹ä¸­...")
            
            # å°è¯•å¤šç§æ–¹å¼è®¾ç½®ESMFoldå®ä¾‹
            if hasattr(model.structure_encoder, 'esmfold') or hasattr(model.structure_encoder, '_esmfold'):
                # è®¾ç½®ESMFoldå®ä¾‹
                model.structure_encoder.esmfold = shared_esmfold
                model.structure_encoder._esmfold = shared_esmfold
                # ç¡®ä¿ESMFoldè¢«æ ‡è®°ä¸ºå¯ç”¨
                model.structure_encoder.use_esmfold = True
                logger.info("âœ… å…±äº« ESMFold å®ä¾‹å·²è®¾ç½®åˆ°æ¨¡å‹ä¸­")
            else:
                # å¦‚æœæ¨¡å‹ç»“æ„ä¸åŒï¼Œå°è¯•ç›´æ¥è®¾ç½®å±æ€§
                setattr(model.structure_encoder, 'esmfold', shared_esmfold)
                setattr(model.structure_encoder, 'use_esmfold', True)
                logger.info("âœ… å…±äº« ESMFold å®ä¾‹å·²å¼ºåˆ¶è®¾ç½®åˆ°æ¨¡å‹ä¸­")
            
            clear_memory()
        
        logger.info(f"âœ… æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # åˆ›å»ºä¼˜åŒ–å™¨ - åªæ’é™¤ESMFoldå‚æ•°ï¼Œä¿ç•™å…¶ä»–æ‰€æœ‰å‚æ•°
        trainable_params = []
        esmfold_params = 0
        total_params = 0
        sequence_encoder_params = 0
        other_params = 0
        
        for name, param in model.named_parameters():
            total_params += param.numel()
            
            # åªæ’é™¤çœŸæ­£çš„ESMFoldæ¨¡å‹å‚æ•°ï¼Œä¿ç•™å…¶ä»–æ‰€æœ‰å‚æ•°
            if ('structure_encoder.esmfold.' in name or 
                'structure_encoder._esmfold.' in name or
                name.startswith('esmfold.')):
                esmfold_params += param.numel()
                param.requires_grad = False  # å†»ç»“ESMFoldå‚æ•°
                logger.debug(f"å†»ç»“ESMFoldå‚æ•°: {name}")
            else:
                trainable_params.append(param)
                if 'sequence_encoder' in name:
                    sequence_encoder_params += param.numel()
                else:
                    other_params += param.numel()
        
        trainable_param_count = sum(p.numel() for p in trainable_params)
        
        logger.info(f"ğŸ“Š å‚æ•°ç»Ÿè®¡:")
        logger.info(f"  æ€»å‚æ•°: {total_params:,}")
        logger.info(f"  ESMFoldå‚æ•°(å†»ç»“): {esmfold_params:,}")
        logger.info(f"  åºåˆ—ç¼–ç å™¨å‚æ•°: {sequence_encoder_params:,}")
        logger.info(f"  å…¶ä»–å¯è®­ç»ƒå‚æ•°: {other_params:,}")
        logger.info(f"  å¯è®­ç»ƒå‚æ•°æ€»è®¡: {trainable_param_count:,}")
        
        if trainable_param_count < 1000000:  # å°‘äº100ä¸‡å‚æ•°
            logger.warning(f"âš ï¸ å¯è®­ç»ƒå‚æ•°è¿‡å°‘ ({trainable_param_count:,})ï¼Œå¯èƒ½å½±å“è®­ç»ƒæ•ˆæœ")
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = optim.AdamW(
            trainable_params,  # åªä¼˜åŒ–éESMFoldå‚æ•°
            lr=5e-5,  # é™ä½å­¦ä¹ ç‡
            weight_decay=1e-5
        )
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)
        
        logger.info(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ŒGPUå†…å­˜: {torch.cuda.memory_allocated() / 1e9:.1f}GB")
        
        return model, optimizer, scheduler
        
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•å®Œå…¨ç¦ç”¨ESMFold
        logger.info("ğŸ”„ å°è¯•ç¦ç”¨ESMFoldé‡æ–°åˆå§‹åŒ–æ¨¡å‹...")
        try:
            config.model.structure_encoder.use_esmfold = False
            config.data.use_predicted_structures = False
            model = StructDiff(config.model).to(device)
            logger.info("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼ˆæœªä½¿ç”¨ESMFoldï¼‰")
            
            # åˆ›å»ºåŸºç¡€ç»„ä»¶
            optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-5)
            scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)
            
            return model, optimizer, scheduler
            
        except Exception as e2:
            logger.error(f"âŒ ç¦ç”¨ESMFoldåä»ç„¶å¤±è´¥: {e2}")
            raise

def create_data_loaders(config, shared_esmfold):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨ - åŸºäºæˆåŠŸè„šæœ¬çš„ç­–ç•¥"""
    logger = get_logger(__name__)
    logger.info("ğŸ”„ æ­£åœ¨åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    
    try:
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = PeptideStructureDataset(
            data_path="/home/qlyu/sequence/StructDiff-7.0.0/data/processed/train.csv",
            config=config,
            is_training=True,
            cache_dir="./cache/train",
            shared_esmfold=shared_esmfold  # ä¼ é€’å…±äº«å®ä¾‹
        )
        
        val_dataset = PeptideStructureDataset(
            data_path="/home/qlyu/sequence/StructDiff-7.0.0/data/processed/val.csv",
            config=config,
            is_training=False,
            cache_dir="./cache/val",
            shared_esmfold=shared_esmfold  # ä¼ é€’å…±äº«å®ä¾‹
        )
        
        logger.info(f"âœ… è®­ç»ƒæ•°æ®é›†: {len(train_dataset)} æ ·æœ¬")
        logger.info(f"âœ… éªŒè¯æ•°æ®é›†: {len(val_dataset)} æ ·æœ¬")
        
        # åˆ›å»ºæ•°æ®æ•´ç†å™¨
        collator = PeptideStructureCollator(config)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - ä½¿ç”¨æ›´ä¿å®ˆçš„è®¾ç½®
        train_loader = DataLoader(
            train_dataset,
            batch_size=2,  # å°æ‰¹æ¬¡å¤§å°é€‚åº”ESMFold
            shuffle=True,
            num_workers=0,  # å…³é”®ï¼šä½¿ç”¨0é¿å…å¤šè¿›ç¨‹ç¼“å­˜ç«äº‰é—®é¢˜
            pin_memory=False,  # ç¦ç”¨pin_memoryèŠ‚çœå†…å­˜
            collate_fn=collator,
            drop_last=True
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=2,
            shuffle=False,  
            num_workers=0,  # å…³é”®ï¼šä½¿ç”¨0é¿å…å¤šè¿›ç¨‹ç¼“å­˜ç«äº‰é—®é¢˜
            pin_memory=False,  # ç¦ç”¨pin_memoryèŠ‚çœå†…å­˜
            collate_fn=collator,
            drop_last=False
        )
        
        logger.info("âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        return train_loader, val_loader
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥: {e}")
        raise

def move_to_device(obj, device):
    """é€’å½’åœ°å°†å¯¹è±¡ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
    if isinstance(obj, torch.Tensor):
        return obj.to(device)
    elif isinstance(obj, dict):
        return {k: move_to_device(v, device) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [move_to_device(item, device) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(move_to_device(item, device) for item in obj)
    else:
        return obj

def train_step(model, diffusion, batch, device, gradient_accumulation_steps):
    """æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤ - åŸºäºæˆåŠŸè„šæœ¬çš„ç­–ç•¥"""
    try:
        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
        batch = move_to_device(batch, device)
        
        # æ£€æŸ¥batchçš„åŸºæœ¬å­—æ®µ
        if 'sequences' not in batch or 'attention_mask' not in batch:
            return None, float('inf')
        
        # æ£€æŸ¥å¼ é‡å½¢çŠ¶ä¸€è‡´æ€§
        seq_shape = batch['sequences'].shape
        mask_shape = batch['attention_mask'].shape
        
        if seq_shape != mask_shape:
            # ä¿®æ­£attention_maskçš„å½¢çŠ¶
            if mask_shape[1] != seq_shape[1]:
                min_len = min(mask_shape[1], seq_shape[1])
                batch['sequences'] = batch['sequences'][:, :min_len]
                batch['attention_mask'] = batch['attention_mask'][:, :min_len]
        
        # æ£€æŸ¥ç»“æ„æ•°æ®çš„å½¢çŠ¶ä¸€è‡´æ€§
        if 'structures' in batch and batch['structures'] is not None:
            expected_struct_len = batch['sequences'].shape[1] - 2  # é™¤å»CLS/SEP
            
            for key, value in batch['structures'].items():
                if value is None:
                    continue
                    
                # å¯¹äºç»“æ„ç‰¹å¾ï¼Œç¬¬äºŒä¸ªç»´åº¦åº”è¯¥ä¸åºåˆ—é•¿åº¦-2åŒ¹é…
                if len(value.shape) >= 2:
                    actual_len = value.shape[1]
                    if actual_len != expected_struct_len:
                        # æˆªæ–­æˆ–å¡«å……ç»“æ„ç‰¹å¾
                        if actual_len > expected_struct_len:
                            if len(value.shape) == 2:
                                batch['structures'][key] = value[:, :expected_struct_len]
                            elif len(value.shape) == 3:
                                if 'matrix' in key or 'map' in key:
                                    batch['structures'][key] = value[:, :expected_struct_len, :expected_struct_len]
                                else:
                                    batch['structures'][key] = value[:, :expected_struct_len, :]
                        elif actual_len < expected_struct_len:
                            pad_size = expected_struct_len - actual_len
                            if len(value.shape) == 2:
                                batch['structures'][key] = torch.nn.functional.pad(value, (0, 0, 0, pad_size), value=0)
                            elif len(value.shape) == 3:
                                if 'matrix' in key or 'map' in key:
                                    batch['structures'][key] = torch.nn.functional.pad(value, (0, pad_size, 0, pad_size), value=0)
                                else:
                                    batch['structures'][key] = torch.nn.functional.pad(value, (0, 0, 0, pad_size), value=0)
        
        # é‡‡æ ·æ—¶é—´æ­¥
        batch_size = batch['sequences'].shape[0]
        timesteps = torch.randint(0, diffusion.num_timesteps, (batch_size,), device=device)
        
        # å‰å‘ä¼ æ’­
        outputs = model(
            sequences=batch['sequences'],
            attention_mask=batch['attention_mask'],
            timesteps=timesteps,
            structures=batch.get('structures'),
            conditions=batch.get('conditions'),
            return_loss=True
        )
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æŸå¤±
        if 'total_loss' not in outputs:
            if 'diffusion_loss' in outputs:
                outputs['total_loss'] = outputs['diffusion_loss']
            elif 'loss' in outputs:
                outputs['total_loss'] = outputs['loss']
            else:
                return None, float('inf')
        
        loss = outputs['total_loss'] / gradient_accumulation_steps
        
        # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºNaN
        if torch.isnan(loss) or torch.isinf(loss):
            return None, float('inf')
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        return outputs, loss.item() * gradient_accumulation_steps
        
    except Exception as e:
        logger = get_logger(__name__)
        logger.error(f"âŒ è®­ç»ƒæ­¥éª¤å¤±è´¥: {e}")
        return None, float('inf')

def validation_step(model, diffusion, val_loader, device, logger):
    """æ‰§è¡ŒéªŒè¯æ­¥éª¤"""
    model.eval()
    val_losses = []
    
    with torch.no_grad():
        for batch in val_loader:
            outputs, loss = train_step(model, diffusion, batch, device, 1)
            if outputs is not None:
                val_losses.append(loss)
            
            # æ¸…ç†æ˜¾å­˜
            clear_memory()
    
    avg_val_loss = np.mean(val_losses) if val_losses else float('inf')
    logger.info(f"âœ… éªŒè¯æŸå¤±: {avg_val_loss:.6f}")
    
    model.train()
    return avg_val_loss

def save_checkpoint(model, optimizer, scheduler, epoch, loss, checkpoint_path, logger):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'loss': loss,
    }, checkpoint_path)
    logger.info(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")

def full_train_with_structure_features_fixed():
    """ä¿®å¤ç‰ˆæœ¬çš„ç»“æ„ç‰¹å¾è®­ç»ƒå‡½æ•°"""
    print("ğŸš€ å¼€å§‹ä¿®å¤ç‰ˆæœ¬çš„ç»“æ„ç‰¹å¾è®­ç»ƒ...")
    
    # Setup logging
    output_dir = "/home/qlyu/sequence/StructDiff-7.0.0/outputs/structure_feature_training_fixed"
    os.makedirs(output_dir, exist_ok=True)
    
    setup_logger(
        level=logging.INFO,
        log_file=f"{output_dir}/training.log"
    )
    logger = get_logger(__name__)
    
    try:
        # è®¾ç½®è®¾å¤‡
        device = torch.device('cuda:0')
        logger.info(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {device}")
        
        # æ˜¾å­˜ä¼˜åŒ–è®¾ç½®
        torch.backends.cudnn.benchmark = False
        torch.backends.cudnn.deterministic = True
        
        # Load config
        config_path = "/home/qlyu/sequence/StructDiff-7.0.0/configs/separated_training.yaml"
        config = load_config(config_path)
        
        # å¯ç”¨ç»“æ„ç‰¹å¾
        config.data.use_predicted_structures = True
        config.model.structure_encoder.use_esmfold = True
        
        logger.info("âœ… é…ç½®åŠ è½½æˆåŠŸï¼Œå·²å¯ç”¨ç»“æ„ç‰¹å¾")
        
        # å…³é”®æ­¥éª¤ï¼šå…ˆåˆ›å»ºå…±äº«ESMFoldå®ä¾‹
        shared_esmfold = setup_shared_esmfold(device)
        
        if not shared_esmfold or not shared_esmfold.available:
            logger.error("âŒ å…±äº«ESMFoldå®ä¾‹åˆ›å»ºå¤±è´¥ï¼Œæ— æ³•è¿›è¡Œç»“æ„ç‰¹å¾è®­ç»ƒ")
            return
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader = create_data_loaders(config, shared_esmfold)
        
        # åˆ›å»ºæ¨¡å‹å’Œè®­ç»ƒç»„ä»¶
        model, optimizer, scheduler = setup_model_and_training(config, device, shared_esmfold)
        
        # åˆ›å»ºæ‰©æ•£è¿‡ç¨‹
        diffusion = GaussianDiffusion(
            num_timesteps=config.diffusion.num_timesteps,
            noise_schedule=config.diffusion.noise_schedule,
            beta_start=config.diffusion.beta_start,
            beta_end=config.diffusion.beta_end
        )
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        checkpoint_path = "/home/qlyu/sequence/StructDiff-7.0.0/outputs/full_training_200_esmfold_fixed/best_model.pt"
        if os.path.exists(checkpoint_path):
            logger.info("ğŸ”„ æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            model.load_state_dict(checkpoint, strict=False)
            logger.info("âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
        else:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
        
        # è®­ç»ƒå‚æ•°
        num_epochs = 100
        gradient_accumulation_steps = 8  # å¢åŠ æ¢¯åº¦ç´¯ç§¯
        save_every = 10
        validate_every = 5
        
        logger.info(f"ğŸ¯ å¼€å§‹ç»“æ„ç‰¹å¾è®­ç»ƒï¼Œå…± {num_epochs} ä¸ªepoch")
        logger.info(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: 2, æ¢¯åº¦ç´¯ç§¯: {gradient_accumulation_steps}")
        logger.info(f"ğŸ“Š æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {2 * gradient_accumulation_steps}, å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']}")
        
        # è®­ç»ƒæŒ‡æ ‡
        train_losses = []
        val_losses = []
        best_val_loss = float('inf')
        
        for epoch in range(num_epochs):
            epoch_loss = 0.0
            num_batches = 0
            accumulated_loss = 0.0
            
            # è®­ç»ƒå¾ªç¯
            model.train()
            progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")
            
            optimizer.zero_grad()
            
            for batch_idx, batch in enumerate(progress_bar):
                # è®­ç»ƒæ­¥éª¤
                outputs, loss = train_step(model, diffusion, batch, device, gradient_accumulation_steps)
                
                if outputs is None:
                    continue
                
                accumulated_loss += loss / gradient_accumulation_steps
                
                # æ¢¯åº¦ç´¯ç§¯
                if (batch_idx + 1) % gradient_accumulation_steps == 0:
                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    
                    # ä¼˜åŒ–å™¨æ­¥éª¤
                    optimizer.step()
                    optimizer.zero_grad()
                    
                    # æ›´æ–°æŒ‡æ ‡
                    epoch_loss += accumulated_loss
                    num_batches += 1
                    accumulated_loss = 0.0
                
                # æ›´æ–°è¿›åº¦æ¡
                current_lr = optimizer.param_groups[0]["lr"]
                allocated = torch.cuda.memory_allocated(device) // 1024**3
                
                progress_bar.set_postfix({
                    'loss': f'{loss:.6f}',
                    'avg_loss': f'{epoch_loss/max(num_batches, 1):.6f}',
                    'lr': f'{current_lr:.2e}',
                    'gpu_mem': f'{allocated}GB'
                })
                
                # å®šæœŸæ—¥å¿—å’Œæ¸…ç†
                if batch_idx % 25 == 0:
                    logger.info(f"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss:.6f}, GPU Memory: {allocated}GB")
                
                if batch_idx % 10 == 0:
                    clear_memory()
            
            # å¤„ç†å‰©ä½™çš„ç´¯ç§¯æ¢¯åº¦
            if accumulated_loss > 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                optimizer.zero_grad()
                epoch_loss += accumulated_loss
                num_batches += 1
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()
            
            avg_train_loss = epoch_loss / max(num_batches, 1)
            train_losses.append(avg_train_loss)
            
            logger.info(f"âœ… Epoch {epoch+1} å®Œæˆï¼Œå¹³å‡è®­ç»ƒæŸå¤±: {avg_train_loss:.6f}, å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")
            
            # éªŒè¯
            if (epoch + 1) % validate_every == 0:
                clear_memory()
                val_loss = validation_step(model, diffusion, val_loader, device, logger)
                val_losses.append(val_loss)
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_model_path = f"{output_dir}/best_model_with_structure.pt"
                    torch.save(model.state_dict(), best_model_path)
                    logger.info(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_model_path} (éªŒè¯æŸå¤±: {val_loss:.6f})")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % save_every == 0:
                checkpoint_path = f"{output_dir}/checkpoint_epoch_{epoch+1}.pt"
                save_checkpoint(model, optimizer, scheduler, epoch + 1, avg_train_loss, checkpoint_path, logger)
                
                # ä¿å­˜è®­ç»ƒæŒ‡æ ‡
                metrics = {
                    'epoch': epoch + 1,
                    'train_losses': train_losses,
                    'val_losses': val_losses,
                    'best_val_loss': best_val_loss,
                    'structure_features_enabled': True,
                    'esmfold_available': shared_esmfold.available if shared_esmfold else False
                }
                metrics_path = f"{output_dir}/training_metrics.json"
                with open(metrics_path, 'w') as f:
                    json.dump(metrics, f, indent=2)
        
        logger.info("ğŸ‰ ç»“æ„ç‰¹å¾è®­ç»ƒå®Œæˆï¼")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = f"{output_dir}/final_model_with_structure_epoch_{num_epochs}.pt"
        torch.save(model.state_dict(), final_model_path)
        logger.info(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
        
        # ä¿å­˜æœ€ç»ˆæŒ‡æ ‡
        final_metrics = {
            'total_epochs': num_epochs,
            'final_train_loss': train_losses[-1] if train_losses else 0,
            'best_val_loss': best_val_loss,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'esmfold_available': shared_esmfold.available if shared_esmfold else False,
            'structure_features_used': True,
            'training_successful': True
        }
        
        final_metrics_path = f"{output_dir}/final_metrics.json"
        with open(final_metrics_path, 'w') as f:
            json.dump(final_metrics, f, indent=2)
        
        logger.info(f"ğŸ“Š ç»“æ„ç‰¹å¾è®­ç»ƒæ‘˜è¦:")
        logger.info(f"  æ€»epochæ•°: {num_epochs}")
        logger.info(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.6f}" if train_losses else "  æœ€ç»ˆè®­ç»ƒæŸå¤±: N/A")
        logger.info(f"  æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        logger.info(f"  æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        logger.info(f"  ESMFoldå¯ç”¨: âœ…")
        logger.info(f"  ç»“æ„ç‰¹å¾: âœ… çœŸæ­£å¯ç”¨")
        
    except Exception as e:
        logger.error(f"âŒ ç»“æ„ç‰¹å¾è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise

if __name__ == "__main__":
    full_train_with_structure_features_fixed() 