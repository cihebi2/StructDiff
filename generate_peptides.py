#!/usr/bin/env python3
"""
è‚½æ®µç”Ÿæˆè„šæœ¬ - ä½¿ç”¨å½“å‰è®­ç»ƒå¥½çš„ç®€åŒ–æ¨¡å‹
"""

import torch
import numpy as np
from pathlib import Path
import sys
import json
from collections import Counter
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from structdiff.models.structdiff import StructDiff
from omegaconf import OmegaConf

class SimplePeptideGenerator:
    """ç®€åŒ–çš„è‚½æ®µç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.config = None
        
        # è‚½æ®µç±»å‹æ˜ å°„
        self.peptide_type_map = {
            'antimicrobial': 0,
            'antifungal': 1,
            'antiviral': 2
        }
        
        self.type_names = {v: k for k, v in self.peptide_type_map.items()}
        
    def load_model(self, checkpoint_path="./outputs/structdiff_fixed/best_model.pt"):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {checkpoint_path}")
        
        # åˆ›å»ºé…ç½®
        self.config = OmegaConf.create({
            'sequence_encoder': {
                'pretrained_model': 'facebook/esm2_t6_8M_UR50D',
                'freeze_encoder': False,
                'use_lora': True,
                'lora_rank': 16,
                'lora_alpha': 32,
                'lora_dropout': 0.1
            },
            'structure_encoder': {
                'hidden_dim': 256,
                'num_layers': 3,
                'use_esmfold': False,
                'use_coordinates': False,
                'use_distances': False,
                'use_angles': False,
                'use_secondary_structure': True
            },
            'denoiser': {
                'hidden_dim': 320,
                'num_layers': 6,
                'num_heads': 8,
                'dropout': 0.1,
                'use_cross_attention': False,
                'use_cfg': True,
                'cfg_dropout': 0.1
            },
            'data': {'max_length': 512}
        })
        
        # åˆ›å»ºæ¨¡å‹
        self.model = StructDiff(self.config)
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
        if 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
        else:
            state_dict = checkpoint
            
        self.model.load_state_dict(state_dict, strict=False)
        self.model = self.model.to(self.device)
        self.model.eval()
        
        print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå‚æ•°æ•°é‡: {self.model.count_parameters():,}")
        
    def generate_with_diffusion(self, peptide_type_id, seq_len=30, num_steps=20):
        """ä½¿ç”¨æ‰©æ•£è¿‡ç¨‹ç”Ÿæˆåºåˆ—"""
        batch_size = 1
        total_len = seq_len + 2  # åŒ…æ‹¬CLSå’ŒSEP
        
        # ä»å™ªå£°å¼€å§‹
        x = torch.randn(batch_size, total_len, self.model.seq_hidden_dim, device=self.device)
        attention_mask = torch.ones(batch_size, total_len, device=self.device)
        conditions = {'peptide_type': torch.tensor([peptide_type_id], device=self.device, dtype=torch.long)}
        
        # ç®€åŒ–çš„å»å™ªè¿‡ç¨‹
        for step in range(num_steps):
            t = torch.tensor([int(1000 * (1 - step / num_steps))], device=self.device, dtype=torch.long)
            
            with torch.no_grad():
                # æ¨¡å‹å‰å‘ä¼ æ’­
                outputs = self.model(
                    sequences=torch.zeros(batch_size, total_len, device=self.device, dtype=torch.long),
                    attention_mask=attention_mask,
                    timesteps=t,
                    conditions=conditions,
                    return_loss=False
                )
                
                denoised = outputs['denoised_embeddings']
                
                # ç®€å•çš„æ›´æ–°è§„åˆ™
                alpha = 0.9
                x = alpha * denoised + (1 - alpha) * x
        
        # è§£ç ä¸ºåºåˆ—
        with torch.no_grad():
            # è°ƒæ•´æ³¨æ„åŠ›æ©ç å¤§å°ä»¥åŒ¹é…denoised
            if denoised.shape[1] != attention_mask.shape[1]:
                attention_mask_adjusted = attention_mask[:, :denoised.shape[1]]
            else:
                attention_mask_adjusted = attention_mask
                
            sequences = self.model._decode_embeddings(denoised, attention_mask_adjusted)
        
        return sequences[0] if sequences else None
    
    def generate_multiple(self, peptide_type, num_samples=10, target_length=25):
        """ç”Ÿæˆå¤šä¸ªæ ·æœ¬"""
        peptide_type_id = self.peptide_type_map.get(peptide_type, 0)
        sequences = []
        
        print(f"ğŸ§¬ ç”Ÿæˆ {num_samples} ä¸ª {peptide_type} è‚½æ®µ...")
        
        for i in range(num_samples):
            try:
                # éšæœºå˜åŒ–é•¿åº¦
                length = target_length + np.random.randint(-5, 6)
                length = max(10, min(40, length))  # é™åˆ¶åœ¨10-40ä¹‹é—´
                
                sequence = self.generate_with_diffusion(peptide_type_id, seq_len=length)
                
                if sequence and len(sequence) > 5:  # åŸºæœ¬è´¨é‡æ£€æŸ¥
                    sequences.append(sequence)
                    if (i + 1) % 5 == 0:
                        print(f"  ç”Ÿæˆè¿›åº¦: {i + 1}/{num_samples}")
                
            except Exception as e:
                print(f"  ç”Ÿæˆç¬¬ {i+1} ä¸ªåºåˆ—æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"âœ“ æˆåŠŸç”Ÿæˆ {len(sequences)} ä¸ªåºåˆ—")
        return sequences
    
    def analyze_sequences(self, sequences, peptide_type):
        """åˆ†æç”Ÿæˆçš„åºåˆ—"""
        if not sequences:
            return {}
        
        # åŸºæœ¬ç»Ÿè®¡
        lengths = [len(seq) for seq in sequences]
        unique_sequences = list(set(sequences))
        
        # æ°¨åŸºé…¸é¢‘ç‡
        all_chars = ''.join(sequences)
        char_counts = Counter(all_chars)
        total_chars = len(all_chars)
        
        # è®¡ç®—ä¿¡æ¯ç†µ
        entropy = 0
        if total_chars > 0:
            for count in char_counts.values():
                prob = count / total_chars
                if prob > 0:
                    entropy -= prob * np.log2(prob)
        
        analysis = {
            'peptide_type': peptide_type,
            'total_sequences': len(sequences),
            'unique_sequences': len(unique_sequences),
            'uniqueness_ratio': len(unique_sequences) / len(sequences),
            'average_length': np.mean(lengths),
            'length_std': np.std(lengths),
            'min_length': min(lengths),
            'max_length': max(lengths),
            'sequence_entropy': entropy,
            'aa_composition': {aa: char_counts.get(aa, 0) / total_chars 
                             for aa in 'ACDEFGHIKLMNPQRSTVWY'}
        }
        
        return analysis
    
    def save_sequences(self, sequences, output_file, peptide_type):
        """ä¿å­˜åºåˆ—åˆ°FASTAæ–‡ä»¶"""
        if not sequences:
            print("âš ï¸ æ²¡æœ‰åºåˆ—å¯ä¿å­˜")
            return
        
        records = []
        for i, seq in enumerate(sequences):
            record = SeqRecord(
                Seq(seq),
                id=f"generated_{peptide_type}_{i+1:03d}",
                description=f"Generated {peptide_type} peptide using simplified StructDiff (length: {len(seq)})"
            )
            records.append(record)
        
        SeqIO.write(records, output_file, "fasta")
        print(f"ğŸ’¾ åºåˆ—å·²ä¿å­˜åˆ°: {output_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ§¬ è‚½æ®µç”Ÿæˆå™¨ - ç®€åŒ–StructDiffæ¨¡å‹")
    print("=" * 60)
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = SimplePeptideGenerator()
    
    # åŠ è½½æ¨¡å‹
    try:
        generator.load_model()
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("./generated_peptides")
    output_dir.mkdir(exist_ok=True)
    
    # ç”Ÿæˆå‚æ•°
    generation_params = {
        'num_samples': 15,
        'target_length': 25
    }
    
    # å¯¹æ¯ç§è‚½æ®µç±»å‹è¿›è¡Œç”Ÿæˆ
    peptide_types = ['antimicrobial', 'antifungal', 'antiviral']
    all_results = {}
    
    for peptide_type in peptide_types:
        print(f"\nğŸ¯ å¼€å§‹ç”Ÿæˆ {peptide_type} è‚½æ®µ")
        print("-" * 40)
        
        try:
            # ç”Ÿæˆåºåˆ—
            sequences = generator.generate_multiple(
                peptide_type=peptide_type,
                **generation_params
            )
            
            if sequences:
                # åˆ†æåºåˆ—
                analysis = generator.analyze_sequences(sequences, peptide_type)
                all_results[peptide_type] = analysis
                
                # ä¿å­˜åºåˆ—
                fasta_file = output_dir / f"{peptide_type}_peptides.fasta"
                generator.save_sequences(sequences, fasta_file, peptide_type)
                
                # æ˜¾ç¤ºç¤ºä¾‹
                print(f"\nğŸ“‹ {peptide_type.upper()} è‚½æ®µç¤ºä¾‹:")
                for i, seq in enumerate(sequences[:5]):
                    print(f"  {i+1}. {seq} (é•¿åº¦: {len(seq)})")
                
                if len(sequences) > 5:
                    print(f"  ... è¿˜æœ‰ {len(sequences)-5} ä¸ªåºåˆ—")
                
                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
                print(f"  æ€»æ•°é‡: {analysis['total_sequences']}")
                print(f"  å”¯ä¸€åºåˆ—: {analysis['unique_sequences']}")
                print(f"  å¹³å‡é•¿åº¦: {analysis['average_length']:.1f} Â± {analysis['length_std']:.1f}")
                print(f"  é•¿åº¦èŒƒå›´: {analysis['min_length']}-{analysis['max_length']}")
                print(f"  å”¯ä¸€æ€§æ¯”ç‡: {analysis['uniqueness_ratio']:.3f}")
                print(f"  åºåˆ—ç†µ: {analysis['sequence_entropy']:.3f}")
                
            else:
                print(f"âš ï¸ æ²¡æœ‰æˆåŠŸç”Ÿæˆ {peptide_type} åºåˆ—")
                
        except Exception as e:
            print(f"âŒ {peptide_type} ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # ä¿å­˜åˆ†æç»“æœ
    if all_results:
        results_file = output_dir / "generation_analysis.json"
        with open(results_file, 'w') as f:
            json.dump(all_results, f, indent=2)
        
        # æ€»ç»“
        print("\n" + "=" * 60)
        print("ğŸ‰ ç”Ÿæˆå®Œæˆï¼")
        print("=" * 60)
        
        total_generated = sum(result['total_sequences'] for result in all_results.values())
        total_unique = sum(result['unique_sequences'] for result in all_results.values())
        
        print(f"ğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
        print(f"  æ€»ç”Ÿæˆæ•°é‡: {total_generated}")
        print(f"  æ€»å”¯ä¸€åºåˆ—: {total_unique}")
        print(f"  æ•´ä½“å”¯ä¸€æ€§: {total_unique/total_generated:.3f}")
        
        print(f"\nğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
        print(f"ğŸ“Š è¯¦ç»†åˆ†æ: {results_file}")
    
    print("\nğŸ¯ ç»“è®º: ç®€åŒ–StructDiffæ¨¡å‹å¯ä»¥æˆåŠŸç”Ÿæˆå¤šæ ·åŒ–çš„è‚½æ®µåºåˆ—")

if __name__ == "__main__":
    main() 