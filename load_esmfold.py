# load_esmfold.py
"""
ä¸“é—¨ä»Ž Huggingface åŠ è½½ ESMFold æ¨¡åž‹çš„è„šæœ¬
ä½¿ç”¨fix_esmfold_patch.pyä¸­çš„è¡¥ä¸ä¿®å¤CUDAé”™è¯¯
"""

import os
import sys
import subprocess
import torch
import warnings
from typing import Optional, Tuple

# å¯¼å…¥è¡¥ä¸
from fix_esmfold_patch import apply_esmfold_patch, monkey_patch_esmfold

def install_dependencies():
    """å®‰è£…å¿…éœ€çš„ä¾èµ–åŒ…"""
    print("æ­£åœ¨æ£€æŸ¥å’Œå®‰è£…ä¾èµ–åŒ…...")
    
    # æ£€æŸ¥å’Œå®‰è£… accelerate
    try:
        import accelerate
        print(f"âœ“ accelerate å·²å®‰è£…ï¼Œç‰ˆæœ¬: {accelerate.__version__}")
    except ImportError:
        print("âŒ accelerate æœªå®‰è£…ï¼Œæ­£åœ¨å®‰è£…...")
        subprocess.check_call([
            sys.executable, "-m", "pip", "install", 
            "accelerate>=0.26.0", "-q"
        ])
        print("âœ“ accelerate å®‰è£…å®Œæˆ")
    
    # æ£€æŸ¥å…¶ä»–ä¾èµ–
    dependencies = {
        'transformers': '4.30.0',
        'torch': '2.0.0',
    }
    
    for package, min_version in dependencies.items():
        try:
            module = __import__(package)
            version = getattr(module, '__version__', 'unknown')
            print(f"âœ“ {package} å·²å®‰è£…ï¼Œç‰ˆæœ¬: {version}")
        except ImportError:
            print(f"âš ï¸ {package} æœªå®‰è£…ï¼Œè¯·æ‰‹åŠ¨å®‰è£…")

def load_esmfold_huggingface(
    model_name: str = "facebook/esmfold_v1",
    device: Optional[str] = None,
    use_low_cpu_mem: bool = True,
    cache_dir: Optional[str] = None
) -> Tuple[Optional[object], Optional[object], bool]:
    """
    ä¸“é—¨ä»Ž Huggingface åŠ è½½ ESMFold æ¨¡åž‹
    
    Args:
        model_name: æ¨¡åž‹åç§°ï¼Œé»˜è®¤ä¸º facebook/esmfold_v1
        device: è®¾å¤‡ï¼Œé»˜è®¤è‡ªåŠ¨é€‰æ‹©
        use_low_cpu_mem: æ˜¯å¦ä½¿ç”¨ä½ŽCPUå†…å­˜æ¨¡å¼
        cache_dir: ç¼“å­˜ç›®å½•
        
    Returns:
        (model, tokenizer, success): æ¨¡åž‹å¯¹è±¡ã€åˆ†è¯å™¨å’Œæ˜¯å¦æˆåŠŸåŠ è½½çš„æ ‡å¿—
    """
    
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    print(f"æ­£åœ¨ä»Ž Huggingface åŠ è½½ ESMFold æ¨¡åž‹: {model_name}")
    print(f"ç›®æ ‡è®¾å¤‡: {device}")
    
    try:
        # é¦–å…ˆåº”ç”¨è¡¥ä¸ä¿®å¤å·²çŸ¥é—®é¢˜
        print("åº”ç”¨ESMFoldè¡¥ä¸...")
        apply_esmfold_patch()
        monkey_patch_esmfold()
        
        from transformers import EsmForProteinFolding, AutoTokenizer
        
        print("æ­£åœ¨åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        print("æ­£åœ¨åŠ è½½æ¨¡åž‹...")
        # è®¾ç½®åŠ è½½å‚æ•°
        load_kwargs = {
            "torch_dtype": torch.float16 if device == "cuda" else torch.float32,
            "trust_remote_code": True
        }
        
        if use_low_cpu_mem:
            load_kwargs["low_cpu_mem_usage"] = True
            # å¦‚æžœåœ¨CUDAè®¾å¤‡ä¸Šï¼Œä½¿ç”¨device_map
            if device == "cuda":
                load_kwargs["device_map"] = "auto"
        
        if cache_dir:
            load_kwargs["cache_dir"] = cache_dir
        
        # åŠ è½½æ¨¡åž‹
        model = EsmForProteinFolding.from_pretrained(
            model_name,
            **load_kwargs
        )
        
        # å¦‚æžœæ²¡æœ‰ä½¿ç”¨device_mapï¼Œæ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡
        if "device_map" not in load_kwargs:
            model = model.to(device)
        
        # è®¾ç½®æ¨¡åž‹ä¸ºè¯„ä¼°æ¨¡å¼
        model.eval()
        
        # ä¼˜åŒ–è®¾ç½®
        if hasattr(model, 'trunk') and hasattr(model.trunk, 'set_chunk_size'):
            model.trunk.set_chunk_size(64)  # å‡å°‘å†…å­˜ä½¿ç”¨
        
        print("âœ“ ESMFold æ¨¡åž‹åŠ è½½æˆåŠŸ (Huggingface)")
        print(f"æ¨¡åž‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        return model, tokenizer, True
        
    except Exception as e:
        print(f"âŒ Huggingfaceæ–¹æ³•å¤±è´¥: {e}")
        print(f"é”™è¯¯ç±»åž‹: {type(e).__name__}")
        import traceback
        traceback.print_exc()
        return None, None, False

def test_esmfold_inference_safe(model, tokenizer, sequence: str = None, device: str = "cuda"):
    """å®‰å…¨åœ°æµ‹è¯•ESMFoldæŽ¨ç†åŠŸèƒ½"""
    if sequence is None:
        # ä½¿ç”¨è¾ƒçŸ­çš„æµ‹è¯•åºåˆ—é¿å…ç´¢å¼•é”™è¯¯
        sequence = "MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG"
    
    print(f"\næ­£åœ¨æµ‹è¯• ESMFold æŽ¨ç†...")
    print(f"æµ‹è¯•åºåˆ—: {sequence}")
    print(f"åºåˆ—é•¿åº¦: {len(sequence)}")
    
    try:
        with torch.no_grad():
            # å¤„ç†è¾“å…¥åºåˆ—
            print("æ­£åœ¨å¤„ç†è¾“å…¥åºåˆ—...")
            
            # ä½¿ç”¨add_special_tokens=Falseé¿å…ç´¢å¼•é—®é¢˜
            tokenized = tokenizer(
                sequence, 
                return_tensors="pt",
                add_special_tokens=False,
                padding=False,
                truncation=False
            )
            
            print(f"Tokenized keys: {list(tokenized.keys())}")
            print(f"Input shape: {tokenized['input_ids'].shape}")
            
            # ç§»åŠ¨åˆ°è®¾å¤‡å¹¶ç¡®ä¿æ•°æ®ç±»åž‹æ­£ç¡®
            if device == "cuda":
                for key in tokenized:
                    tokenized[key] = tokenized[key].to(device)
                    # ç¡®ä¿æ•´æ•°å¼ é‡æ˜¯LongTensor
                    if tokenized[key].dtype in [torch.int32, torch.int16, torch.int8]:
                        tokenized[key] = tokenized[key].long()
            
            print("æ­£åœ¨æ‰§è¡Œæ¨¡åž‹æŽ¨ç†...")
            
            # å°è¯•æŽ¨ç†
            outputs = model(tokenized['input_ids'])
            
            print("âœ“ æŽ¨ç†æµ‹è¯•æˆåŠŸ")
            if hasattr(outputs, 'positions'):
                print(f"ç»“æž„åæ ‡å½¢çŠ¶: {outputs.positions.shape}")
            if hasattr(outputs, 'plddt'):
                print(f"pLDDTåˆ†æ•°å½¢çŠ¶: {outputs.plddt.shape}")
                avg_plddt = outputs.plddt.mean().item()
                print(f"å¹³å‡pLDDTåˆ†æ•°: {avg_plddt:.3f}")
            
            return outputs
            
    except Exception as e:
        print(f"âŒ æŽ¨ç†æµ‹è¯•å¤±è´¥: {e}")
        print(f"é”™è¯¯ç±»åž‹: {type(e).__name__}")
        
        # å°è¯•æ›´çŸ­çš„åºåˆ—
        if len(sequence) > 50:
            print("å°è¯•ä½¿ç”¨æ›´çŸ­çš„åºåˆ—...")
            short_sequence = sequence[:30]  # æˆªçŸ­åˆ°30ä¸ªæ°¨åŸºé…¸
            return test_esmfold_inference_safe(model, tokenizer, short_sequence, device)
        
        return None

def main():
    """ä¸»å‡½æ•°"""
    print("=== ESMFold Huggingface åŠ è½½è„šæœ¬ ===\n")
    
    # 1. å®‰è£…ä¾èµ–
    install_dependencies()
    print()
    
    # 2. æ£€æŸ¥è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    if device == "cuda":
        print(f"GPU å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        print(f"å½“å‰ GPU å†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1e9:.1f}GB")
    print()
    
    # 3. åŠ è½½æ¨¡åž‹
    model, tokenizer, success = load_esmfold_huggingface(device=device)
    
    if success and model is not None:
        print(f"\nðŸŽ‰ ESMFold æ¨¡åž‹åŠ è½½æˆåŠŸï¼")
        
        # 4. æµ‹è¯•æŽ¨ç†
        test_result = test_esmfold_inference_safe(model, tokenizer, device=device)
        
        if test_result is not None:
            print("âœ“ æ¨¡åž‹å¯ä»¥æ­£å¸¸è¿›è¡Œç»“æž„é¢„æµ‹")
        else:
            print("âš ï¸ æ¨¡åž‹åŠ è½½æˆåŠŸä½†æŽ¨ç†æµ‹è¯•å¤±è´¥")
            
        # 5. æ˜¾ç¤ºå†…å­˜ä½¿ç”¨
        if device == "cuda":
            print(f"\næ¨¡åž‹åŠ è½½åŽ GPU å†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1e9:.1f}GB")
            
        return model, tokenizer
            
    else:
        print("\nâŒ ESMFold æ¨¡åž‹åŠ è½½å¤±è´¥")
        print("è¯·æ£€æŸ¥ä»¥ä¸‹é—®é¢˜ï¼š")
        print("1. æ˜¯å¦å®‰è£…äº†æ‰€æœ‰å¿…éœ€çš„ä¾èµ–åŒ…")
        print("2. ç½‘ç»œè¿žæŽ¥æ˜¯å¦æ­£å¸¸")
        print("3. æ˜¯å¦æœ‰è¶³å¤Ÿçš„å†…å­˜/æ˜¾å­˜")
        print("\nå»ºè®®æ‰‹åŠ¨å®‰è£…ä¾èµ–ï¼š")
        print("pip install accelerate>=0.26.0 transformers torch")
        
        return None, None

if __name__ == "__main__":
    main() 