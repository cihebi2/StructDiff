#!/usr/bin/env python3
"""
GPUå†…å­˜ç›‘æ§å’Œæ¸…ç†è„šæœ¬
"""

import os
import gc

def monitor_memory():
    """ç›‘æ§å†…å­˜ä½¿ç”¨"""
    print("ğŸ” å†…å­˜ç›‘æ§æŠ¥å‘Š")
    print("=" * 40)
    
    # æ£€æŸ¥torchæ˜¯å¦å¯ç”¨
    try:
        import torch
        
        # GPUå†…å­˜
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                allocated = torch.cuda.memory_allocated(i) / 1024**3
                reserved = torch.cuda.memory_reserved(i) / 1024**3
                total = torch.cuda.get_device_properties(i).total_memory / 1024**3
                
                print(f"GPU {i}:")
                print(f"  å·²åˆ†é…: {allocated:.2f}GB")
                print(f"  å·²é¢„ç•™: {reserved:.2f}GB") 
                print(f"  æ€»å®¹é‡: {total:.2f}GB")
                print(f"  ä½¿ç”¨ç‡: {allocated/total*100:.1f}%")
        else:
            print("CUDA ä¸å¯ç”¨")
    except ImportError:
        print("PyTorch æœªå®‰è£…")
    
    # ç³»ç»Ÿå†…å­˜
    try:
        import psutil
        memory = psutil.virtual_memory()
        print(f"\nç³»ç»Ÿå†…å­˜:")
        print(f"  å·²ä½¿ç”¨: {memory.used/1024**3:.2f}GB")
        print(f"  æ€»å®¹é‡: {memory.total/1024**3:.2f}GB")
        print(f"  ä½¿ç”¨ç‡: {memory.percent:.1f}%")
    except ImportError:
        print("\npsutil æœªå®‰è£…ï¼Œæ— æ³•æ£€æŸ¥ç³»ç»Ÿå†…å­˜")

def cleanup_memory():
    """æ¸…ç†å†…å­˜"""
    print("\nğŸ§¹ å¼€å§‹å†…å­˜æ¸…ç†...")
    
    # Pythonåƒåœ¾å›æ”¶
    collected = gc.collect()
    print(f"  åƒåœ¾å›æ”¶: {collected} ä¸ªå¯¹è±¡")
    
    # GPUå†…å­˜æ¸…ç†
    try:
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            print("  GPUç¼“å­˜å·²æ¸…ç†")
    except ImportError:
        print("  PyTorch æœªå®‰è£…ï¼Œè·³è¿‡GPUæ¸…ç†")
    
    print("âœ… å†…å­˜æ¸…ç†å®Œæˆ")

def optimize_pytorch_memory():
    """ä¼˜åŒ–PyTorchå†…å­˜è®¾ç½®"""
    print("\nâš™ï¸ ä¼˜åŒ–PyTorchå†…å­˜è®¾ç½®...")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:128'
    os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
    
    # è®¾ç½®å†…å­˜åˆ†æ•°é™åˆ¶
    try:
        import torch
        if torch.cuda.is_available():
            torch.cuda.set_per_process_memory_fraction(0.9)  # ä½¿ç”¨90%çš„GPUå†…å­˜
            print("  è®¾ç½®GPUå†…å­˜ä½¿ç”¨é™åˆ¶ä¸º90%")
    except ImportError:
        print("  PyTorch æœªå®‰è£…ï¼Œè·³è¿‡GPUå†…å­˜ä¼˜åŒ–")
    
    print("âœ… PyTorchå†…å­˜ä¼˜åŒ–å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    monitor_memory()
    cleanup_memory()
    optimize_pytorch_memory()
    
    print("\nğŸ“‹ å»ºè®®:")
    print("  1. åœ¨è®­ç»ƒå‰è¿è¡Œæ­¤è„šæœ¬")
    print("  2. è®¾ç½® PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True")
    print("  3. å‡å°æ‰¹æ¬¡å¤§å°æˆ–åºåˆ—é•¿åº¦")
    print("  4. è€ƒè™‘ä½¿ç”¨CPUè¿›è¡ŒESMFoldè¯„ä¼°")

if __name__ == "__main__":
    main()