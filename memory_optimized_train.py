# memory_optimized_train.py
"""
å†…å­˜ä¼˜åŒ–çš„è®­ç»ƒè„šæœ¬ï¼Œä¸“é—¨å¤„ç†ESMFoldçš„å†…å­˜ç®¡ç†
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from omegaconf import OmegaConf
from tqdm import tqdm
import os
import sys
import gc

# å¯¼å…¥ ESMFold è¡¥ä¸ä»¥ä¿®å¤å…¼å®¹æ€§é—®é¢˜
from fix_esmfold_patch import apply_esmfold_patch

from structdiff.models.structdiff import StructDiff
from structdiff.data.dataset import PeptideStructureDataset
from structdiff.data.collator import PeptideStructureCollator

def setup_memory_optimization():
    """è®¾ç½®å†…å­˜ä¼˜åŒ–"""
    if torch.cuda.is_available():
        # å¯ç”¨å†…å­˜ç‰‡æ®µç®¡ç†
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
        # å¯ç”¨è°ƒè¯•æ¨¡å¼
        os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
        torch.cuda.empty_cache()
    print("âœ“ å†…å­˜ä¼˜åŒ–è®¾ç½®å®Œæˆ")

def aggressive_memory_cleanup():
    """æ¿€è¿›çš„å†…å­˜æ¸…ç†"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

def check_memory_usage(stage=""):
    """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9
        reserved = torch.cuda.memory_reserved() / 1e9
        max_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"[{stage}] GPUå†…å­˜ - å·²åˆ†é…: {allocated:.1f}GB, å·²ä¿ç•™: {reserved:.1f}GB, æ€»è®¡: {max_memory:.1f}GB")
        return allocated, reserved, max_memory
    return 0, 0, 0

def create_lightweight_model(config, device, use_esmfold=False):
    """åˆ›å»ºè½»é‡çº§æ¨¡å‹"""
    print("æ­£åœ¨åˆ›å»ºè½»é‡çº§æ¨¡å‹...")
    
    # å¤‡ä»½åŸå§‹é…ç½®
    original_config = config.copy()
    
    # ç¦ç”¨ESMFold
    config.model.structure_encoder.use_esmfold = use_esmfold
    config.data.use_predicted_structures = use_esmfold
    
    try:
        model = StructDiff(config)
        
        # æ£€æŸ¥å†…å­˜ä½¿ç”¨
        check_memory_usage("æ¨¡å‹åˆ›å»ºå")
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        model = model.to(device)
        check_memory_usage("ç§»åŠ¨åˆ°GPUå")
        
        print(f"âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {model.count_parameters():,}")
        return model, original_config
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        # æ¢å¤åŸå§‹é…ç½®
        config.update(original_config)
        return None, original_config

def load_esmfold_separately(device):
    """å•ç‹¬åŠ è½½ESMFold"""
    print("æ­£åœ¨å•ç‹¬åŠ è½½ ESMFold...")
    
    try:
        from structdiff.models.esmfold_wrapper import ESMFoldWrapper
        
        # æ¸…ç†å†…å­˜
        aggressive_memory_cleanup()
        check_memory_usage("ESMFoldåŠ è½½å‰")
        
        esmfold = ESMFoldWrapper(device=device)
        
        if esmfold.available:
            check_memory_usage("ESMFoldåŠ è½½å")
            print("âœ“ ESMFold åŠ è½½æˆåŠŸ")
            return esmfold
        else:
            print("âŒ ESMFold ä¸å¯ç”¨")
            return None
            
    except Exception as e:
        print(f"âŒ ESMFold åŠ è½½å¤±è´¥: {e}")
        return None

def smart_attach_esmfold(model, esmfold):
    """æ™ºèƒ½åœ°å°†ESMFoldé™„åŠ åˆ°æ¨¡å‹"""
    if not esmfold or not esmfold.available:
        return False
    
    try:
        print("æ­£åœ¨å°† ESMFold é™„åŠ åˆ°æ¨¡å‹...")
        
        # å°è¯•å¤šç§æ–¹å¼è®¾ç½®ESMFold
        if hasattr(model, 'structure_encoder'):
            model.structure_encoder.esmfold = esmfold
            model.structure_encoder.use_esmfold = True
            
            # ç¡®ä¿æ¨¡å‹çŸ¥é“ESMFoldå¯ç”¨
            if hasattr(model.structure_encoder, '_esmfold'):
                model.structure_encoder._esmfold = esmfold
                
        print("âœ“ ESMFold æˆåŠŸé™„åŠ åˆ°æ¨¡å‹")
        return True
        
    except Exception as e:
        print(f"âŒ ESMFold é™„åŠ å¤±è´¥: {e}")
        return False

def create_minimal_dataset(config, shared_esmfold=None, max_samples=10):
    """åˆ›å»ºæœ€å°æ•°æ®é›†"""
    print(f"æ­£åœ¨åˆ›å»ºæœ€å°æ•°æ®é›†ï¼ˆæœ€å¤š{max_samples}ä¸ªæ ·æœ¬ï¼‰...")
    
    try:
        # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
        train_dataset = PeptideStructureDataset(
            config.data.train_path, 
            config,
            is_training=True,
            shared_esmfold=shared_esmfold
        )
        
        # é™åˆ¶æ•°æ®é›†å¤§å°
        if len(train_dataset) > max_samples:
            train_dataset.data = train_dataset.data.head(max_samples)
        
        print(f"âœ“ è®­ç»ƒæ•°æ®é›†å¤§å°: {len(train_dataset)}")
        
        # åˆ›å»ºéªŒè¯æ•°æ®é›†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        val_dataset = None
        if os.path.exists(config.data.get('val_path', '')):
            val_dataset = PeptideStructureDataset(
                config.data.val_path,
                config,
                is_training=False,
                shared_esmfold=shared_esmfold
            )
            
            max_val_samples = min(5, len(val_dataset))
            val_dataset.data = val_dataset.data.head(max_val_samples)
            print(f"âœ“ éªŒè¯æ•°æ®é›†å¤§å°: {len(val_dataset)}")
        
        return train_dataset, val_dataset
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥: {e}")
        return None, None

def safe_train_step(model, batch, optimizer, device, config):
    """å®‰å…¨çš„è®­ç»ƒæ­¥éª¤"""
    try:
        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
        if isinstance(batch, dict):
            batch = {k: v.to(device) if torch.is_tensor(v) else v 
                    for k, v in batch.items()}
        
        # æ£€æŸ¥å¿…è¦å­—æ®µ
        if 'sequences' not in batch or 'attention_mask' not in batch:
            return None, "ç¼ºå°‘å¿…è¦å­—æ®µ"
        
        # ç”Ÿæˆæ—¶é—´æ­¥
        batch_size = batch['sequences'].shape[0]
        timesteps = torch.randint(
            0, config.diffusion.num_timesteps, 
            (batch_size,), device=device
        )
        
        # å‰å‘ä¼ æ’­
        outputs = model(
            sequences=batch['sequences'],
            attention_mask=batch['attention_mask'],
            timesteps=timesteps,
            structures=batch.get('structures', None),
            return_loss=True
        )
        
        loss = outputs['total_loss']
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        return loss.item(), None
        
    except Exception as e:
        return None, str(e)

def main():
    """ä¸»å‡½æ•°"""
    print("=== å†…å­˜ä¼˜åŒ–è®­ç»ƒè„šæœ¬ ===\n")
    
    # 1. è®¾ç½®å†…å­˜ä¼˜åŒ–
    setup_memory_optimization()
    apply_esmfold_patch()
    
    # 2. åŠ è½½é…ç½®
    config = OmegaConf.load("configs/minimal_test.yaml")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    initial_allocated, initial_reserved, total_memory = check_memory_usage("åˆå§‹çŠ¶æ€")
    
    # 3. ç­–ç•¥é€‰æ‹©ï¼šæ ¹æ®å¯ç”¨å†…å­˜å†³å®šæ˜¯å¦ä½¿ç”¨ESMFold
    use_esmfold = False
    esmfold_instance = None
    
    # å¦‚æœæœ‰è¶³å¤Ÿå†…å­˜ï¼ˆ>16GBå¯ç”¨ï¼‰ï¼Œå°è¯•åŠ è½½ESMFold
    available_memory = total_memory - initial_allocated
    if available_memory > 16.0:
        print(f"å¯ç”¨å†…å­˜å……è¶³ï¼ˆ{available_memory:.1f}GBï¼‰ï¼Œå°è¯•åŠ è½½ESMFold...")
        esmfold_instance = load_esmfold_separately(device)
        use_esmfold = esmfold_instance is not None
    else:
        print(f"å¯ç”¨å†…å­˜ä¸è¶³ï¼ˆ{available_memory:.1f}GBï¼‰ï¼Œè·³è¿‡ESMFold")
    
    # 4. åˆ›å»ºè½»é‡çº§æ¨¡å‹
    model, original_config = create_lightweight_model(config, device, use_esmfold=False)
    
    if model is None:
        print("âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥ï¼Œé€€å‡º")
        return
    
    # 5. å¦‚æœæœ‰ESMFoldï¼Œå°è¯•é™„åŠ 
    if use_esmfold and esmfold_instance:
        if not smart_attach_esmfold(model, esmfold_instance):
            print("âš ï¸ ESMFoldé™„åŠ å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨æ— ESMFoldçš„æ¨¡å‹")
            use_esmfold = False
    
    check_memory_usage("æ¨¡å‹è®¾ç½®å®Œæˆ")
    
    # 6. åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=config.training.optimizer.lr,
        weight_decay=config.training.optimizer.weight_decay
    )
    
    # 7. åˆ›å»ºæ•°æ®é›†
    train_dataset, val_dataset = create_minimal_dataset(
        config, 
        shared_esmfold=esmfold_instance if use_esmfold else None,
        max_samples=15
    )
    
    if train_dataset is None:
        print("âŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥ï¼Œé€€å‡º")
        return
    
    # 8. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    collator = PeptideStructureCollator(config)
    train_loader = DataLoader(
        train_dataset,
        batch_size=1,
        shuffle=True,
        collate_fn=collator,
        num_workers=0,
        pin_memory=False
    )
    
    check_memory_usage("æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
    
    # 9. å¼€å§‹è®­ç»ƒ
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    print(f"ä½¿ç”¨ESMFold: {use_esmfold}")
    print(f"è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
    
    model.train()
    successful_steps = 0
    total_loss = 0.0
    
    for epoch in range(min(3, config.training.num_epochs)):  # é™åˆ¶epochæ•°
        print(f"\nEpoch {epoch + 1}")
        
        for batch_idx, batch in enumerate(train_loader):
            try:
                # å®šæœŸæ¸…ç†å†…å­˜
                if batch_idx % 5 == 0:
                    aggressive_memory_cleanup()
                
                # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
                loss, error = safe_train_step(model, batch, optimizer, device, config)
                
                if loss is not None:
                    # æ¢¯åº¦è£å‰ªå’Œä¼˜åŒ–å™¨æ­¥éª¤
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                    optimizer.step()
                    optimizer.zero_grad()
                    
                    total_loss += loss
                    successful_steps += 1
                    
                    print(f"  æ‰¹æ¬¡ {batch_idx}: æŸå¤± {loss:.4f}")
                else:
                    print(f"  æ‰¹æ¬¡ {batch_idx}: å¤±è´¥ - {error}")
                
                # å†…å­˜æ£€æŸ¥
                if batch_idx % 3 == 0:
                    check_memory_usage(f"Epoch{epoch+1}-Batch{batch_idx}")
                
            except Exception as e:
                print(f"  æ‰¹æ¬¡ {batch_idx}: å¼‚å¸¸ - {e}")
                optimizer.zero_grad()
                aggressive_memory_cleanup()
                continue
        
        # Epochç»“æŸæ¸…ç†
        aggressive_memory_cleanup()
    
    # 10. è®­ç»ƒç»“æœ
    if successful_steps > 0:
        avg_loss = total_loss / successful_steps
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"æˆåŠŸæ­¥éª¤: {successful_steps}")
        print(f"å¹³å‡æŸå¤±: {avg_loss:.4f}")
    else:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥ï¼Œæ²¡æœ‰æˆåŠŸçš„æ­¥éª¤")
    
    check_memory_usage("è®­ç»ƒå®Œæˆ")

if __name__ == "__main__":
    main() 