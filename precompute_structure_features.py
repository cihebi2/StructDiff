#!/usr/bin/env python3
"""
ç»“æ„ç‰¹å¾é¢„è®¡ç®—è„šæœ¬
å°†æ‰€æœ‰è®­ç»ƒæ•°æ®çš„ç»“æ„ç‰¹å¾æå‰è®¡ç®—å¹¶ç¼“å­˜ï¼Œé¿å…è®­ç»ƒæ—¶çš„ESMFoldè®¡ç®—ç“¶é¢ˆ
"""

import os
import sys
import torch
import pandas as pd
import numpy as np
import pickle
import hashlib
from tqdm import tqdm
import logging
from pathlib import Path
import gc
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import json

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:128'

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/home/qlyu/sequence/StructDiff-7.0.0')

from structdiff.models.esmfold_wrapper import ESMFoldWrapper

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('structure_precompute.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class StructureFeatureCache:
    """ç»“æ„ç‰¹å¾ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir="./structure_cache", max_memory_gb=20):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.max_memory_gb = max_memory_gb
        self.memory_cache = {}  # å†…å­˜ç¼“å­˜
        self.disk_cache_index = {}  # ç£ç›˜ç¼“å­˜ç´¢å¼•
        self.load_disk_index()
        
    def get_sequence_hash(self, sequence):
        """è·å–åºåˆ—çš„å“ˆå¸Œå€¼ä½œä¸ºç¼“å­˜é”®"""
        return hashlib.md5(sequence.encode()).hexdigest()
    
    def load_disk_index(self):
        """åŠ è½½ç£ç›˜ç¼“å­˜ç´¢å¼•"""
        index_file = self.cache_dir / "cache_index.json"
        if index_file.exists():
            with open(index_file, 'r') as f:
                self.disk_cache_index = json.load(f)
            logger.info(f"åŠ è½½ç£ç›˜ç¼“å­˜ç´¢å¼•: {len(self.disk_cache_index)} ä¸ªæ¡ç›®")
        else:
            self.disk_cache_index = {}
    
    def save_disk_index(self):
        """ä¿å­˜ç£ç›˜ç¼“å­˜ç´¢å¼•"""
        index_file = self.cache_dir / "cache_index.json"
        with open(index_file, 'w') as f:
            json.dump(self.disk_cache_index, f, indent=2)
    
    def get_cache_file_path(self, seq_hash):
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        # ä½¿ç”¨å“ˆå¸Œçš„å‰ä¸¤ä¸ªå­—ç¬¦ä½œä¸ºå­ç›®å½•ï¼Œé¿å…å•ä¸ªç›®å½•æ–‡ä»¶è¿‡å¤š
        subdir = seq_hash[:2]
        cache_subdir = self.cache_dir / subdir
        cache_subdir.mkdir(exist_ok=True)
        return cache_subdir / f"{seq_hash}.pkl"
    
    def has_cached_structure(self, sequence):
        """æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜ç»“æ„ç‰¹å¾"""
        seq_hash = self.get_sequence_hash(sequence)
        
        # æ£€æŸ¥å†…å­˜ç¼“å­˜
        if seq_hash in self.memory_cache:
            return True
        
        # æ£€æŸ¥ç£ç›˜ç¼“å­˜
        if seq_hash in self.disk_cache_index:
            cache_file = self.get_cache_file_path(seq_hash)
            return cache_file.exists()
        
        return False
    
    def get_cached_structure(self, sequence):
        """è·å–ç¼“å­˜çš„ç»“æ„ç‰¹å¾"""
        seq_hash = self.get_sequence_hash(sequence)
        
        # é¦–å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
        if seq_hash in self.memory_cache:
            return self.memory_cache[seq_hash]
        
        # ç„¶åæ£€æŸ¥ç£ç›˜ç¼“å­˜
        if seq_hash in self.disk_cache_index:
            cache_file = self.get_cache_file_path(seq_hash)
            if cache_file.exists():
                try:
                    with open(cache_file, 'rb') as f:
                        structure_features = pickle.load(f)
                    
                    # åŠ è½½åˆ°å†…å­˜ç¼“å­˜ï¼ˆå¦‚æœå†…å­˜å…è®¸ï¼‰
                    self.add_to_memory_cache(seq_hash, structure_features)
                    return structure_features
                except Exception as e:
                    logger.warning(f"åŠ è½½ç¼“å­˜æ–‡ä»¶å¤±è´¥ {cache_file}: {e}")
                    # åˆ é™¤æŸåçš„ç¼“å­˜æ–‡ä»¶
                    cache_file.unlink(missing_ok=True)
                    del self.disk_cache_index[seq_hash]
        
        return None
    
    def cache_structure(self, sequence, structure_features):
        """ç¼“å­˜ç»“æ„ç‰¹å¾"""
        seq_hash = self.get_sequence_hash(sequence)
        
        # ä¿å­˜åˆ°ç£ç›˜
        cache_file = self.get_cache_file_path(seq_hash)
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(structure_features, f)
            
            # æ›´æ–°ç´¢å¼•
            self.disk_cache_index[seq_hash] = {
                'sequence_length': len(sequence),
                'cache_file': str(cache_file.relative_to(self.cache_dir)),
                'timestamp': time.time()
            }
            
            # åŠ è½½åˆ°å†…å­˜ç¼“å­˜
            self.add_to_memory_cache(seq_hash, structure_features)
            
            return True
        except Exception as e:
            logger.error(f"ç¼“å­˜ç»“æ„ç‰¹å¾å¤±è´¥: {e}")
            return False
    
    def add_to_memory_cache(self, seq_hash, structure_features):
        """æ·»åŠ åˆ°å†…å­˜ç¼“å­˜"""
        # ç®€å•çš„å†…å­˜ç®¡ç†ï¼šå¦‚æœè¶…è¿‡é™åˆ¶ï¼Œæ¸…ç©ºå†…å­˜ç¼“å­˜
        current_memory_gb = self.estimate_memory_usage()
        if current_memory_gb > self.max_memory_gb:
            self.clear_memory_cache()
        
        self.memory_cache[seq_hash] = structure_features
    
    def estimate_memory_usage(self):
        """ä¼°è®¡å½“å‰å†…å­˜ä½¿ç”¨é‡"""
        if torch.cuda.is_available():
            return torch.cuda.memory_allocated() / 1e9
        return 0
    
    def clear_memory_cache(self):
        """æ¸…ç©ºå†…å­˜ç¼“å­˜"""
        self.memory_cache.clear()
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def get_cache_stats(self):
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'disk_cache_count': len(self.disk_cache_index),
            'memory_cache_count': len(self.memory_cache),
            'cache_dir_size_mb': sum(f.stat().st_size for f in self.cache_dir.rglob('*.pkl')) / 1e6
        }

def precompute_single_structure(esmfold_wrapper, sequence, seq_id, cache_manager):
    """é¢„è®¡ç®—å•ä¸ªåºåˆ—çš„ç»“æ„ç‰¹å¾"""
    try:
        # æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜
        if cache_manager.has_cached_structure(sequence):
            return seq_id, "cached", None
        
        # é¢„æµ‹ç»“æ„
        start_time = time.time()
        structure_features = esmfold_wrapper.predict_structure(sequence)
        prediction_time = time.time() - start_time
        
        if structure_features is not None:
            # ç¼“å­˜ç»“æ„ç‰¹å¾
            success = cache_manager.cache_structure(sequence, structure_features)
            if success:
                return seq_id, "computed", prediction_time
            else:
                return seq_id, "cache_failed", prediction_time
        else:
            return seq_id, "prediction_failed", prediction_time
            
    except Exception as e:
        logger.error(f"é¢„è®¡ç®—åºåˆ— {seq_id} å¤±è´¥: {e}")
        return seq_id, "error", str(e)

def precompute_batch_structures(esmfold_wrapper, sequences_batch, cache_manager):
    """æ‰¹é‡é¢„è®¡ç®—ç»“æ„ç‰¹å¾"""
    results = []
    
    for seq_id, sequence in sequences_batch:
        result = precompute_single_structure(esmfold_wrapper, sequence, seq_id, cache_manager)
        results.append(result)
        
        # å®šæœŸæ¸…ç†å†…å­˜
        if len(results) % 10 == 0:
            cache_manager.clear_memory_cache()
    
    return results

def load_dataset_sequences(data_path):
    """åŠ è½½æ•°æ®é›†åºåˆ—"""
    logger.info(f"åŠ è½½æ•°æ®é›†: {data_path}")
    
    try:
        df = pd.read_csv(data_path)
        sequences = []
        
        for idx, row in df.iterrows():
            if 'sequence' in row and pd.notna(row['sequence']):
                sequences.append((idx, row['sequence']))
            elif 'peptide_sequence' in row and pd.notna(row['peptide_sequence']):
                sequences.append((idx, row['peptide_sequence']))
        
        logger.info(f"åŠ è½½äº† {len(sequences)} ä¸ªåºåˆ—")
        return sequences
        
    except Exception as e:
        logger.error(f"åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        return []

def precompute_all_structures():
    """é¢„è®¡ç®—æ‰€æœ‰ç»“æ„ç‰¹å¾"""
    logger.info("ğŸš€ å¼€å§‹ç»“æ„ç‰¹å¾é¢„è®¡ç®—...")
    
    # æ£€æŸ¥GPUå¯ç”¨æ€§
    if not torch.cuda.is_available():
        logger.error("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•è¿è¡ŒESMFold")
        return False
    
    # é€‰æ‹©GPUè®¾å¤‡
    device = torch.device('cuda:2')  # ä½¿ç”¨GPU 2
    logger.info(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºç¼“å­˜ç®¡ç†å™¨
    cache_manager = StructureFeatureCache(
        cache_dir="./structure_cache",
        max_memory_gb=15  # ä¸ºESMFoldé¢„ç•™è¶³å¤Ÿå†…å­˜
    )
    
    # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
    stats = cache_manager.get_cache_stats()
    logger.info(f"ğŸ“Š ç¼“å­˜ç»Ÿè®¡: {stats}")
    
    try:
        # åˆå§‹åŒ–ESMFold
        logger.info("ğŸ”„ åˆå§‹åŒ–ESMFold...")
        esmfold_wrapper = ESMFoldWrapper(device=device)
        
        if not esmfold_wrapper.available:
            logger.error("âŒ ESMFoldåˆå§‹åŒ–å¤±è´¥")
            return False
        
        logger.info("âœ… ESMFoldåˆå§‹åŒ–æˆåŠŸ")
        logger.info(f"ESMFoldå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated(device) / 1e9:.1f}GB")
        
        # åŠ è½½æ•°æ®é›†
        datasets = [
            "/home/qlyu/sequence/StructDiff-7.0.0/data/processed/train.csv",
            "/home/qlyu/sequence/StructDiff-7.0.0/data/processed/val.csv"
        ]
        
        all_sequences = []
        for dataset_path in datasets:
            if os.path.exists(dataset_path):
                sequences = load_dataset_sequences(dataset_path)
                all_sequences.extend(sequences)
                logger.info(f"ä» {dataset_path} åŠ è½½äº† {len(sequences)} ä¸ªåºåˆ—")
        
        if not all_sequences:
            logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„åºåˆ—æ•°æ®")
            return False
        
        logger.info(f"ğŸ“Š æ€»å…±éœ€è¦å¤„ç† {len(all_sequences)} ä¸ªåºåˆ—")
        
        # æ£€æŸ¥å·²ç¼“å­˜çš„åºåˆ—
        cached_count = 0
        uncached_sequences = []
        
        for seq_id, sequence in all_sequences:
            if cache_manager.has_cached_structure(sequence):
                cached_count += 1
            else:
                uncached_sequences.append((seq_id, sequence))
        
        logger.info(f"ğŸ“Š å·²ç¼“å­˜: {cached_count}, éœ€è¦è®¡ç®—: {len(uncached_sequences)}")
        
        if not uncached_sequences:
            logger.info("âœ… æ‰€æœ‰åºåˆ—éƒ½å·²ç¼“å­˜ï¼Œæ— éœ€é‡æ–°è®¡ç®—")
            return True
        
        # æ‰¹é‡å¤„ç†æœªç¼“å­˜çš„åºåˆ—
        batch_size = 50  # æ¯æ‰¹å¤„ç†50ä¸ªåºåˆ—
        total_batches = (len(uncached_sequences) + batch_size - 1) // batch_size
        
        logger.info(f"ğŸ”„ å¼€å§‹æ‰¹é‡é¢„è®¡ç®—ï¼Œå…± {total_batches} æ‰¹")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_computed = 0
        total_failed = 0
        total_time = 0
        
        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(uncached_sequences))
            batch = uncached_sequences[start_idx:end_idx]
            
            logger.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} ({len(batch)} ä¸ªåºåˆ—)")
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡
            batch_start_time = time.time()
            results = precompute_batch_structures(esmfold_wrapper, batch, cache_manager)
            batch_time = time.time() - batch_start_time
            
            # ç»Ÿè®¡ç»“æœ
            batch_computed = sum(1 for _, status, _ in results if status == "computed")
            batch_failed = sum(1 for _, status, _ in results if status in ["prediction_failed", "error", "cache_failed"])
            
            total_computed += batch_computed
            total_failed += batch_failed
            total_time += batch_time
            
            # æ˜¾ç¤ºæ‰¹æ¬¡ç»“æœ
            avg_time_per_seq = batch_time / len(batch)
            logger.info(f"  âœ… æ‰¹æ¬¡å®Œæˆ: è®¡ç®— {batch_computed}, å¤±è´¥ {batch_failed}, å¹³å‡ {avg_time_per_seq:.2f}s/åºåˆ—")
            
            # ä¿å­˜ç¼“å­˜ç´¢å¼•
            if batch_idx % 5 == 0:  # æ¯5æ‰¹ä¿å­˜ä¸€æ¬¡
                cache_manager.save_disk_index()
            
            # æ˜¾ç¤ºè¿›åº¦
            progress = (batch_idx + 1) / total_batches * 100
            remaining_batches = total_batches - batch_idx - 1
            estimated_remaining_time = remaining_batches * (total_time / (batch_idx + 1)) / 60
            
            logger.info(f"ğŸ“ˆ è¿›åº¦: {progress:.1f}%, é¢„è®¡å‰©ä½™: {estimated_remaining_time:.1f}åˆ†é’Ÿ")
        
        # ä¿å­˜æœ€ç»ˆç¼“å­˜ç´¢å¼•
        cache_manager.save_disk_index()
        
        # æœ€ç»ˆç»Ÿè®¡
        logger.info("ğŸ‰ ç»“æ„ç‰¹å¾é¢„è®¡ç®—å®Œæˆ!")
        logger.info(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        logger.info(f"  æ€»åºåˆ—æ•°: {len(all_sequences)}")
        logger.info(f"  å·²ç¼“å­˜: {cached_count}")
        logger.info(f"  æ–°è®¡ç®—: {total_computed}")
        logger.info(f"  å¤±è´¥: {total_failed}")
        logger.info(f"  æ€»è€—æ—¶: {total_time / 60:.1f}åˆ†é’Ÿ")
        
        # æ˜¾ç¤ºæœ€ç»ˆç¼“å­˜ç»Ÿè®¡
        final_stats = cache_manager.get_cache_stats()
        logger.info(f"ğŸ“Š æœ€ç»ˆç¼“å­˜ç»Ÿè®¡: {final_stats}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ é¢„è®¡ç®—è¿‡ç¨‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        # æ¸…ç†èµ„æº
        if 'cache_manager' in locals():
            cache_manager.save_disk_index()
            cache_manager.clear_memory_cache()

def verify_cache_integrity():
    """éªŒè¯ç¼“å­˜å®Œæ•´æ€§"""
    logger.info("ğŸ” éªŒè¯ç¼“å­˜å®Œæ•´æ€§...")
    
    cache_manager = StructureFeatureCache()
    stats = cache_manager.get_cache_stats()
    
    logger.info(f"ğŸ“Š ç¼“å­˜ç»Ÿè®¡: {stats}")
    
    # éªŒè¯å‡ ä¸ªéšæœºæ ·æœ¬
    datasets = [
        "/home/qlyu/sequence/StructDiff-7.0.0/data/processed/train.csv",
        "/home/qlyu/sequence/StructDiff-7.0.0/data/processed/val.csv"
    ]
    
    sample_count = 0
    valid_count = 0
    
    for dataset_path in datasets:
        if os.path.exists(dataset_path):
            sequences = load_dataset_sequences(dataset_path)
            
            # æ£€æŸ¥å‰10ä¸ªåºåˆ—
            for seq_id, sequence in sequences[:10]:
                sample_count += 1
                if cache_manager.has_cached_structure(sequence):
                    structure = cache_manager.get_cached_structure(sequence)
                    if structure is not None:
                        valid_count += 1
                        logger.info(f"âœ… åºåˆ— {seq_id}: ç¼“å­˜æœ‰æ•ˆ")
                    else:
                        logger.warning(f"âš ï¸ åºåˆ— {seq_id}: ç¼“å­˜æŸå")
                else:
                    logger.warning(f"âŒ åºåˆ— {seq_id}: æœªç¼“å­˜")
    
    logger.info(f"ğŸ” éªŒè¯å®Œæˆ: {valid_count}/{sample_count} æœ‰æ•ˆ")
    return valid_count == sample_count

if __name__ == "__main__":
    try:
        # é¢„è®¡ç®—æ‰€æœ‰ç»“æ„ç‰¹å¾
        success = precompute_all_structures()
        
        if success:
            # éªŒè¯ç¼“å­˜å®Œæ•´æ€§
            verify_cache_integrity()
            logger.info("âœ… ç»“æ„ç‰¹å¾é¢„è®¡ç®—å’ŒéªŒè¯å®Œæˆ!")
        else:
            logger.error("âŒ ç»“æ„ç‰¹å¾é¢„è®¡ç®—å¤±è´¥!")
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.info("âš ï¸ ç”¨æˆ·ä¸­æ–­é¢„è®¡ç®—è¿‡ç¨‹")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ é¢„è®¡ç®—è„šæœ¬å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1) 