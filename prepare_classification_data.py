# prepare_classification_data.py - å‡†å¤‡æŠ—èŒè‚½åˆ†ç±»æ•°æ®
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import os
from pathlib import Path

def prepare_classification_data(input_file, output_dir="data/processed", test_size=0.2, val_size=0.2, random_state=42):
    """
    å‡†å¤‡æŠ—èŒè‚½åˆ†ç±»æ•°æ®
    
    Args:
        input_file: è¾“å…¥çš„CSVæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        test_size: æµ‹è¯•é›†æ¯”ä¾‹
        val_size: éªŒè¯é›†æ¯”ä¾‹ï¼ˆä»Žè®­ç»ƒé›†ä¸­åˆ†å‡ºï¼‰
        random_state: éšæœºç§å­
    """
    
    print(f"ðŸ”„ æ­£åœ¨å¤„ç†æ•°æ®æ–‡ä»¶: {input_file}")
    
    # è¯»å–æ•°æ®
    df = pd.read_csv(input_file)
    print(f"ðŸ“Š åŽŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
    
    # æ£€æŸ¥æ•°æ®æ ¼å¼
    required_columns = ['sequence', 'label', 'weight']
    for col in required_columns:
        if col not in df.columns:
            raise ValueError(f"ç¼ºå°‘å¿…è¦åˆ—: {col}")
    
    # æ•°æ®ç»Ÿè®¡
    print("\nðŸ“ˆ æ ‡ç­¾åˆ†å¸ƒ:")
    label_counts = df['label'].value_counts().sort_index()
    label_names = {0: 'æŠ—èŒè‚½', 1: 'æŠ—çœŸèŒè‚½', 2: 'æŠ—ç—…æ¯’è‚½'}
    for label, count in label_counts.items():
        print(f"  {label} ({label_names.get(label, 'æœªçŸ¥')}): {count} æ ·æœ¬")
    
    print("\nâš–ï¸ æƒé‡åˆ†å¸ƒ:")
    weight_counts = df['weight'].value_counts().sort_index()
    for weight, count in weight_counts.items():
        print(f"  æƒé‡ {weight}: {count} æ ·æœ¬")
    
    # åºåˆ—é•¿åº¦ç»Ÿè®¡
    df['seq_length'] = df['sequence'].str.len()
    print(f"\nðŸ“ åºåˆ—é•¿åº¦ç»Ÿè®¡:")
    print(f"  æœ€çŸ­: {df['seq_length'].min()}")
    print(f"  æœ€é•¿: {df['seq_length'].max()}")
    print(f"  å¹³å‡: {df['seq_length'].mean():.1f}")
    print(f"  ä¸­ä½æ•°: {df['seq_length'].median():.1f}")
    
    # æ•°æ®æ¸…æ´—
    print("\nðŸ§¹ æ•°æ®æ¸…æ´—...")
    original_size = len(df)
    
    # ç§»é™¤è¿‡çŸ­æˆ–è¿‡é•¿çš„åºåˆ—
    min_length = 5
    max_length = 100
    df = df[(df['seq_length'] >= min_length) & (df['seq_length'] <= max_length)]
    print(f"  ç§»é™¤é•¿åº¦ä¸åˆé€‚çš„åºåˆ—: {original_size - len(df)} æ¡")
    
    # ç§»é™¤åŒ…å«å¼‚å¸¸å­—ç¬¦çš„åºåˆ—
    valid_amino_acids = set('ACDEFGHIKLMNPQRSTVWY')
    def is_valid_sequence(seq):
        return all(aa in valid_amino_acids for aa in seq.upper())
    
    valid_mask = df['sequence'].apply(is_valid_sequence)
    df = df[valid_mask]
    print(f"  ç§»é™¤åŒ…å«å¼‚å¸¸å­—ç¬¦çš„åºåˆ—: {original_size - len(df)} æ¡")
    
    print(f"  æ¸…æ´—åŽæ•°æ®å½¢çŠ¶: {df.shape}")
    
    # å‡†å¤‡æœ€ç»ˆæ•°æ®æ ¼å¼ï¼Œä¿ç•™æ‰€æœ‰åŽŸæœ‰å­—æ®µ
    df_clean = df.copy()
    df_clean['sequence'] = df_clean['sequence'].str.upper()  # ç»Ÿä¸€å¤§å†™
    
    # åˆ†å±‚é‡‡æ ·ï¼Œç¡®ä¿å„ç±»åˆ«åœ¨è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†ä¸­éƒ½æœ‰ä»£è¡¨
    # é¦–å…ˆåˆ†å‡ºæµ‹è¯•é›†
    X = df_clean.drop(['label'], axis=1)  # ä¿ç•™é™¤labelå¤–çš„æ‰€æœ‰å­—æ®µ
    y = df_clean['label']
    
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, 
        test_size=test_size, 
        random_state=random_state, 
        stratify=y
    )
    
    # å†ä»Žå‰©ä½™æ•°æ®ä¸­åˆ†å‡ºéªŒè¯é›†
    val_size_adjusted = val_size / (1 - test_size)  # è°ƒæ•´éªŒè¯é›†æ¯”ä¾‹
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp,
        test_size=val_size_adjusted,
        random_state=random_state,
        stratify=y_temp
    )
    
    # é‡æ–°ç»„åˆæ•°æ®
    train_df = pd.concat([X_train, y_train], axis=1)
    val_df = pd.concat([X_val, y_val], axis=1)
    test_df = pd.concat([X_test, y_test], axis=1)
    
    print(f"\nðŸ“Š æ•°æ®é›†åˆ†å‰²ç»“æžœ:")
    print(f"  è®­ç»ƒé›†: {len(train_df)} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(val_df)} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(test_df)} æ ·æœ¬")
    
    # æ£€æŸ¥å„é›†åˆçš„æ ‡ç­¾åˆ†å¸ƒ
    for name, data in [('è®­ç»ƒé›†', train_df), ('éªŒè¯é›†', val_df), ('æµ‹è¯•é›†', test_df)]:
        print(f"\n{name}æ ‡ç­¾åˆ†å¸ƒ:")
        label_dist = data['label'].value_counts().sort_index()
        for label, count in label_dist.items():
            percentage = count / len(data) * 100
            print(f"  {label} ({label_names.get(label)}): {count} ({percentage:.1f}%)")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜æ•°æ®é›†
    train_path = output_path / "train.csv"
    val_path = output_path / "val.csv"
    test_path = output_path / "test.csv"
    
    train_df.to_csv(train_path, index=False)
    val_df.to_csv(val_path, index=False)
    test_df.to_csv(test_path, index=False)
    
    print(f"\nðŸ’¾ æ•°æ®å·²ä¿å­˜:")
    print(f"  è®­ç»ƒé›†: {train_path}")
    print(f"  éªŒè¯é›†: {val_path}")
    print(f"  æµ‹è¯•é›†: {test_path}")
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_samples': len(df_clean),
        'train_samples': len(train_df),
        'val_samples': len(val_df),
        'test_samples': len(test_df),
        'label_distribution': {
            name: {
                'train': int(train_df[train_df['label'] == label].shape[0]),
                'val': int(val_df[val_df['label'] == label].shape[0]),
                'test': int(test_df[test_df['label'] == label].shape[0])
            }
            for label, name in label_names.items()
            if label in df_clean['label'].values
        },
        'sequence_length': {
            'min': int(df_clean['sequence'].str.len().min()),
            'max': int(df_clean['sequence'].str.len().max()),
            'mean': float(df_clean['sequence'].str.len().mean()),
            'median': float(df_clean['sequence'].str.len().median())
        },
        'weight_distribution': {float(k): int(v) for k, v in df_clean['weight'].value_counts().items()}
    }
    
    import json
    stats_path = output_path / "dataset_stats.json"
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    print(f"  ç»Ÿè®¡ä¿¡æ¯: {stats_path}")
    
    return train_path, val_path, test_path

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='å‡†å¤‡æŠ—èŒè‚½åˆ†ç±»æ•°æ®')
    parser.add_argument('--input', type=str, required=True,
                       help='è¾“å…¥çš„CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='data/processed',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--test_size', type=float, default=0.2,
                       help='æµ‹è¯•é›†æ¯”ä¾‹')
    parser.add_argument('--val_size', type=float, default=0.2,
                       help='éªŒè¯é›†æ¯”ä¾‹')
    parser.add_argument('--random_state', type=int, default=42,
                       help='éšæœºç§å­')
    
    args = parser.parse_args()
    
    try:
        train_path, val_path, test_path = prepare_classification_data(
            input_file=args.input,
            output_dir=args.output_dir,
            test_size=args.test_size,
            val_size=args.val_size,
            random_state=args.random_state
        )
        
        print("\nâœ… æ•°æ®å‡†å¤‡å®Œæˆï¼")
        print("\nðŸš€ æŽ¥ä¸‹æ¥å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¼€å§‹è®­ç»ƒ:")
        print(f"cd /home/qlyu/StructDiff")
        print(f"python train_full.py --config configs/classification_config.yaml --output_dir outputs/classification_run")
        
    except Exception as e:
        print(f"\nâŒ æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 