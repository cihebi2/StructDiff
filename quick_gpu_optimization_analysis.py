#!/usr/bin/env python3
"""
å¿«é€ŸGPUä¼˜åŒ–åˆ†æè„šæœ¬
åˆ†æå½“å‰è®­ç»ƒçŠ¶æ€å¹¶æä¾›å…·ä½“çš„ä¼˜åŒ–å»ºè®®
"""

import subprocess
import json
import time
import psutil
import re
from datetime import datetime

def get_gpu_info():
    """è·å–GPUä¿¡æ¯"""
    try:
        result = subprocess.run([
            'nvidia-smi', 
            '--query-gpu=index,name,memory.used,memory.total,utilization.gpu,power.draw,power.limit,temperature.gpu',
            '--format=csv,noheader,nounits'
        ], capture_output=True, text=True)
        
        gpus = []
        for line in result.stdout.strip().split('\n'):
            if line.strip():
                parts = line.split(', ')
                gpus.append({
                    'index': int(parts[0]),
                    'name': parts[1],
                    'memory_used': int(parts[2]),
                    'memory_total': int(parts[3]),
                    'utilization': int(parts[4]),
                    'power_draw': float(parts[5]),
                    'power_limit': float(parts[6]),
                    'temperature': int(parts[7])
                })
        return gpus
    except Exception as e:
        print(f"è·å–GPUä¿¡æ¯å¤±è´¥: {e}")
        return []

def get_process_info():
    """è·å–å½“å‰è®­ç»ƒè¿›ç¨‹ä¿¡æ¯"""
    try:
        result = subprocess.run([
            'nvidia-smi', 
            '--query-compute-apps=pid,process_name,used_memory',
            '--format=csv,noheader,nounits'
        ], capture_output=True, text=True)
        
        processes = []
        for line in result.stdout.strip().split('\n'):
            if line.strip() and 'python' in line:
                parts = line.split(', ')
                pid = int(parts[0])
                
                # è·å–è¿›ç¨‹è¯¦ç»†ä¿¡æ¯
                try:
                    proc = psutil.Process(pid)
                    cmd = ' '.join(proc.cmdline())
                    
                    processes.append({
                        'pid': pid,
                        'name': parts[1],
                        'gpu_memory': int(parts[2]),
                        'command': cmd,
                        'cpu_percent': proc.cpu_percent(),
                        'memory_percent': proc.memory_percent()
                    })
                except:
                    pass
        
        return processes
    except Exception as e:
        print(f"è·å–è¿›ç¨‹ä¿¡æ¯å¤±è´¥: {e}")
        return []

def analyze_training_bottleneck():
    """åˆ†æè®­ç»ƒç“¶é¢ˆ"""
    print("ğŸ” GPUåˆ©ç”¨ç‡åˆ†ææŠ¥å‘Š")
    print("=" * 60)
    
    # è·å–GPUä¿¡æ¯
    gpus = get_gpu_info()
    processes = get_process_info()
    
    if not gpus:
        print("âŒ æ— æ³•è·å–GPUä¿¡æ¯")
        return
    
    print(f"ğŸ“Š GPUçŠ¶æ€ ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})")
    print("-" * 60)
    
    training_gpu = None
    for gpu in gpus:
        print(f"GPU {gpu['index']}: {gpu['name']}")
        print(f"  ğŸ’¾ å†…å­˜: {gpu['memory_used']}/{gpu['memory_total']} MB ({gpu['memory_used']/gpu['memory_total']*100:.1f}%)")
        print(f"  ğŸ”¥ åˆ©ç”¨ç‡: {gpu['utilization']}%")
        print(f"  âš¡ åŠŸç‡: {gpu['power_draw']:.0f}/{gpu['power_limit']:.0f} W ({gpu['power_draw']/gpu['power_limit']*100:.1f}%)")
        print(f"  ğŸŒ¡ï¸  æ¸©åº¦: {gpu['temperature']}Â°C")
        
        # æ‰¾åˆ°æ­£åœ¨è®­ç»ƒçš„GPU
        if gpu['memory_used'] > 10000:  # å¤§äº10GBå†…å­˜ä½¿ç”¨
            training_gpu = gpu
        print()
    
    if not training_gpu:
        print("âŒ æœªæ‰¾åˆ°æ­£åœ¨è®­ç»ƒçš„GPU")
        return
    
    print(f"ğŸ¯ æ£€æµ‹åˆ°è®­ç»ƒGPU: GPU {training_gpu['index']}")
    print("-" * 60)
    
    # åˆ†æç“¶é¢ˆ
    bottlenecks = []
    optimizations = []
    
    # 1. GPUåˆ©ç”¨ç‡åˆ†æ
    if training_gpu['utilization'] < 30:
        bottlenecks.append("GPUåˆ©ç”¨ç‡è¿‡ä½")
        optimizations.extend([
            "å¢åŠ æ‰¹æ¬¡å¤§å° (batch_size)",
            "å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)",
            "å¢åŠ æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•° (num_workers)",
            "å¯ç”¨å†…å­˜å›ºå®š (pin_memory=True)"
        ])
    
    # 2. åŠŸç‡ä½¿ç”¨åˆ†æ
    power_usage = training_gpu['power_draw'] / training_gpu['power_limit']
    if power_usage < 0.5:
        bottlenecks.append("GPUåŠŸç‡ä½¿ç”¨ä¸è¶³")
        optimizations.extend([
            "å¢åŠ è®¡ç®—å¯†åº¦",
            "ä¼˜åŒ–æ•°æ®åŠ è½½æµæ°´çº¿",
            "å‡å°‘CPU-GPUåŒæ­¥ç­‰å¾…"
        ])
    
    # 3. å†…å­˜ä½¿ç”¨åˆ†æ
    memory_usage = training_gpu['memory_used'] / training_gpu['memory_total']
    if memory_usage > 0.9:
        bottlenecks.append("GPUå†…å­˜ä½¿ç”¨è¿‡é«˜")
        optimizations.extend([
            "å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹",
            "å‡å°‘æ‰¹æ¬¡å¤§å°",
            "ä¼˜åŒ–å†…å­˜ç®¡ç†"
        ])
    elif memory_usage < 0.6:
        bottlenecks.append("GPUå†…å­˜ä½¿ç”¨ä¸è¶³")
        optimizations.extend([
            "å¢åŠ æ‰¹æ¬¡å¤§å°",
            "å¢åŠ æ¨¡å‹å¤æ‚åº¦",
            "å¯ç”¨æ•°æ®é¢„å–"
        ])
    
    # 4. è¿›ç¨‹åˆ†æ
    print("ğŸ” è®­ç»ƒè¿›ç¨‹åˆ†æ:")
    print("-" * 60)
    
    training_processes = [p for p in processes if p['gpu_memory'] > 1000]
    for proc in training_processes:
        print(f"PID {proc['pid']}: {proc['name']}")
        print(f"  ğŸ“„ å‘½ä»¤: {proc['command'][:80]}...")
        print(f"  ğŸ’¾ GPUå†…å­˜: {proc['gpu_memory']} MB")
        print(f"  ğŸ–¥ï¸  CPU: {proc['cpu_percent']:.1f}%")
        print(f"  ğŸ’¿ RAM: {proc['memory_percent']:.1f}%")
        print()
    
    # è¾“å‡ºåˆ†æç»“æœ
    print("ğŸš¨ æ£€æµ‹åˆ°çš„æ€§èƒ½ç“¶é¢ˆ:")
    print("-" * 60)
    for i, bottleneck in enumerate(bottlenecks, 1):
        print(f"{i}. {bottleneck}")
    
    print(f"\nğŸ’¡ æ¨èçš„ä¼˜åŒ–æªæ–½:")
    print("-" * 60)
    for i, opt in enumerate(set(optimizations), 1):
        print(f"{i}. {opt}")
    
    # å…·ä½“ä¼˜åŒ–å»ºè®®
    print(f"\nğŸ¯ å…·ä½“ä¼˜åŒ–å»ºè®®:")
    print("-" * 60)
    
    current_memory_gb = training_gpu['memory_used'] / 1024
    total_memory_gb = training_gpu['memory_total'] / 1024
    
    print(f"ğŸ“Š å½“å‰çŠ¶æ€:")
    print(f"  - GPUåˆ©ç”¨ç‡: {training_gpu['utilization']}%")
    print(f"  - åŠŸç‡ä½¿ç”¨: {training_gpu['power_draw']:.0f}W / {training_gpu['power_limit']:.0f}W")
    print(f"  - å†…å­˜ä½¿ç”¨: {current_memory_gb:.1f}GB / {total_memory_gb:.1f}GB")
    
    print(f"\nğŸš€ ç«‹å³å¯æ‰§è¡Œçš„ä¼˜åŒ–:")
    
    # æ‰¹æ¬¡å¤§å°å»ºè®®
    if training_gpu['utilization'] < 50:
        if current_memory_gb < 20:
            suggested_batch_size = 8
            print(f"1. å¢åŠ æ‰¹æ¬¡å¤§å°åˆ° {suggested_batch_size} (å½“å‰å¯èƒ½æ˜¯2)")
        else:
            suggested_batch_size = 6
            print(f"1. é€‚åº¦å¢åŠ æ‰¹æ¬¡å¤§å°åˆ° {suggested_batch_size}")
    
    # æ•°æ®åŠ è½½ä¼˜åŒ–
    if training_gpu['utilization'] < 30:
        print("2. å¯ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½:")
        print("   - num_workers=4")
        print("   - pin_memory=True")
        print("   - prefetch_factor=2")
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    if power_usage < 0.6:
        print("3. å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP):")
        print("   - å¯æå‡20-30%æ€§èƒ½")
        print("   - å‡å°‘å†…å­˜ä½¿ç”¨")
    
    # æ¢¯åº¦ç´¯ç§¯ä¼˜åŒ–
    print("4. ä¼˜åŒ–æ¢¯åº¦ç´¯ç§¯ç­–ç•¥:")
    print("   - å‡å°‘ç´¯ç§¯æ­¥æ•°ï¼Œå¢åŠ æ‰¹æ¬¡å¤§å°")
    print("   - å½“å‰: batch_size=2, accumulation=8")
    print("   - å»ºè®®: batch_size=8, accumulation=2")
    
    print(f"\nğŸ“ˆ é¢„æœŸæ€§èƒ½æå‡:")
    print("-" * 60)
    
    if training_gpu['utilization'] < 30:
        print("ğŸ¯ ç›®æ ‡GPUåˆ©ç”¨ç‡: 70-85%")
        print("ğŸ¯ ç›®æ ‡åŠŸç‡ä½¿ç”¨: 250-350W")
        print("ğŸ¯ é¢„æœŸè®­ç»ƒé€Ÿåº¦æå‡: 3-5å€")
        print("ğŸ¯ é¢„æœŸæ€»è®­ç»ƒæ—¶é—´ç¼©çŸ­: 60-80%")
    
    return {
        'gpu_info': training_gpu,
        'bottlenecks': bottlenecks,
        'optimizations': optimizations,
        'processes': training_processes
    }

def create_optimization_config():
    """åˆ›å»ºä¼˜åŒ–é…ç½®å»ºè®®"""
    print(f"\nâš™ï¸  ä¼˜åŒ–é…ç½®å»ºè®®:")
    print("-" * 60)
    
    config_suggestions = {
        "training": {
            "batch_size": 8,  # ä»2å¢åŠ åˆ°8
            "gradient_accumulation_steps": 2,  # ä»8å‡å°‘åˆ°2
            "num_workers": 4,  # ä»0å¢åŠ åˆ°4
            "pin_memory": True,  # å¯ç”¨
            "prefetch_factor": 2,  # å¯ç”¨é¢„å–
            "use_amp": True,  # å¯ç”¨æ··åˆç²¾åº¦
            "persistent_workers": True  # æŒä¹…åŒ–å·¥ä½œè¿›ç¨‹
        },
        "model": {
            "gradient_checkpointing": True,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
            "compile_model": True  # å¯ç”¨æ¨¡å‹ç¼–è¯‘
        },
        "optimization": {
            "optimizer": "AdamW",
            "learning_rate": 1e-4,
            "weight_decay": 0.01,
            "grad_clip_norm": 1.0
        }
    }
    
    print("å»ºè®®çš„é…ç½®å‚æ•°:")
    for section, params in config_suggestions.items():
        print(f"\n[{section}]")
        for key, value in params.items():
            print(f"  {key} = {value}")
    
    return config_suggestions

if __name__ == "__main__":
    try:
        print("ğŸš€ å¼€å§‹GPUä¼˜åŒ–åˆ†æ...")
        analysis = analyze_training_bottleneck()
        
        if analysis:
            config = create_optimization_config()
            
            # ä¿å­˜åˆ†æç»“æœ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = f"gpu_optimization_report_{timestamp}.json"
            
            with open(report_file, 'w') as f:
                json.dump({
                    'timestamp': timestamp,
                    'analysis': analysis,
                    'config_suggestions': config
                }, f, indent=2)
            
            print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            print("\nâœ… GPUä¼˜åŒ–åˆ†æå®Œæˆ!")
            
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc() 