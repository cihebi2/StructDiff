#!/usr/bin/env python3
"""
CFG + é•¿åº¦é‡‡æ ·å™¨é›†æˆé‡‡æ ·è„šæœ¬
å±•ç¤ºå¦‚ä½•ä½¿ç”¨Classifier-Free Guidanceå’Œé•¿åº¦åˆ†å¸ƒé‡‡æ ·å™¨è¿›è¡Œé«˜è´¨é‡è‚½æ®µç”Ÿæˆ
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Optional, Tuple
from pathlib import Path
import argparse
import yaml
from dataclasses import dataclass
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from structdiff.models.classifier_free_guidance import CFGConfig, ClassifierFreeGuidance
from structdiff.sampling.length_sampler import (
    LengthSamplerConfig, 
    AdaptiveLengthSampler,
    LengthConstrainedSampler,
    get_default_length_config
)
from structdiff.models.denoise import StructureAwareDenoiser
from structdiff.diffusion.gaussian_diffusion import GaussianDiffusion


@dataclass
class IntegratedSamplingConfig:
    """é›†æˆé‡‡æ ·é…ç½®"""
    # CFGé…ç½®
    cfg_guidance_scale: float = 2.5
    cfg_adaptive_guidance: bool = True
    cfg_multi_level_guidance: bool = False
    
    # é•¿åº¦é‡‡æ ·é…ç½®
    length_distribution: str = "normal"  # normal, uniform, gamma, beta
    length_mean: float = 25.0
    length_std: float = 8.0
    min_length: int = 8
    max_length: int = 50
    
    # ç”Ÿæˆé…ç½®
    batch_size: int = 16
    num_samples: int = 100
    num_inference_steps: int = 50
    
    # æ¡ä»¶é…ç½®
    peptide_types: List[str] = None
    temperature: float = 1.0
    
    # è¾“å‡ºé…ç½®
    output_file: str = "cfg_length_generated_peptides.fasta"
    save_intermediates: bool = False


class CFGLengthIntegratedSampler:
    """
    CFG + é•¿åº¦é‡‡æ ·å™¨é›†æˆé‡‡æ ·å™¨
    ç»“åˆåˆ†ç±»å™¨è‡ªç”±å¼•å¯¼å’Œè‡ªé€‚åº”é•¿åº¦æ§åˆ¶
    """
    
    def __init__(self, 
                 denoiser: StructureAwareDenoiser,
                 diffusion: GaussianDiffusion,
                 cfg_config: CFGConfig,
                 length_config: LengthSamplerConfig,
                 device: str = 'cuda'):
        
        self.denoiser = denoiser
        self.diffusion = diffusion
        self.device = device
        
        # CFGç»„ä»¶
        self.cfg = ClassifierFreeGuidance(cfg_config)
        self.cfg_config = cfg_config
        
        # é•¿åº¦é‡‡æ ·ç»„ä»¶
        self.length_sampler = AdaptiveLengthSampler(length_config)
        self.length_constrainer = LengthConstrainedSampler(self.length_sampler)
        self.length_config = length_config
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        self.denoiser = self.denoiser.to(device)
        self.length_sampler = self.length_sampler.to(device)
    
    def sample_with_cfg_and_length(self, 
                                  config: IntegratedSamplingConfig) -> Dict[str, List]:
        """
        ä½¿ç”¨CFGå’Œé•¿åº¦æ§åˆ¶è¿›è¡Œé‡‡æ ·
        
        Args:
            config: é›†æˆé‡‡æ ·é…ç½®
            
        Returns:
            ç”Ÿæˆç»“æœå­—å…¸
        """
        print(f"ğŸš€ å¼€å§‹CFG+é•¿åº¦æ§åˆ¶é›†æˆé‡‡æ ·")
        print(f"ğŸ“Š é…ç½®: CFGå¼•å¯¼å¼ºåº¦={config.cfg_guidance_scale}, é•¿åº¦åˆ†å¸ƒ={config.length_distribution}")
        
        results = {
            'sequences': [],
            'lengths': [],
            'peptide_types': [],
            'guidance_scales': [],
            'sampling_metadata': []
        }
        
        # å¤„ç†è‚½ç±»å‹
        if config.peptide_types is None:
            config.peptide_types = ['antimicrobial', 'antifungal', 'antiviral']
        
        total_batches = (config.num_samples + config.batch_size - 1) // config.batch_size
        
        for batch_idx in range(total_batches):
            current_batch_size = min(config.batch_size, config.num_samples - batch_idx * config.batch_size)
            
            print(f"ğŸ“¦ æ‰¹æ¬¡ {batch_idx + 1}/{total_batches}, å¤§å°: {current_batch_size}")
            
            # ç”Ÿæˆæ¡ä»¶
            conditions = self._generate_batch_conditions(current_batch_size, config)
            
            # é‡‡æ ·é•¿åº¦
            target_lengths = self._sample_batch_lengths(current_batch_size, conditions, config)
            
            # CFGå¼•å¯¼é‡‡æ ·
            batch_sequences = self._sample_batch_sequences(
                current_batch_size, conditions, target_lengths, config
            )
            
            # æ”¶é›†ç»“æœ
            self._collect_batch_results(
                batch_sequences, target_lengths, conditions, config, results
            )
        
        print(f"âœ… å®Œæˆé‡‡æ ·ï¼Œå…±ç”Ÿæˆ {len(results['sequences'])} ä¸ªåºåˆ—")
        return results
    
    def _generate_batch_conditions(self, 
                                  batch_size: int, 
                                  config: IntegratedSamplingConfig) -> Dict[str, torch.Tensor]:
        """ç”Ÿæˆæ‰¹æ¬¡æ¡ä»¶"""
        # éšæœºé€‰æ‹©è‚½ç±»å‹
        type_mapping = {'antimicrobial': 0, 'antifungal': 1, 'antiviral': 2}
        
        peptide_types = []
        for _ in range(batch_size):
            peptide_type_name = np.random.choice(config.peptide_types)
            peptide_types.append(type_mapping[peptide_type_name])
        
        conditions = {
            'peptide_type': torch.tensor(peptide_types, device=self.device),
            'peptide_type_names': [list(type_mapping.keys())[i] for i in peptide_types]
        }
        
        return conditions
    
    def _sample_batch_lengths(self, 
                             batch_size: int,
                             conditions: Dict[str, torch.Tensor],
                             config: IntegratedSamplingConfig) -> torch.Tensor:
        """é‡‡æ ·æ‰¹æ¬¡é•¿åº¦"""
        # æ ¹æ®é…ç½®è°ƒæ•´é•¿åº¦é‡‡æ ·å™¨
        if config.length_distribution == "normal":
            self.length_config.normal_mean = config.length_mean
            self.length_config.normal_std = config.length_std
        
        self.length_config.min_length = config.min_length
        self.length_config.max_length = config.max_length
        
        # é‡‡æ ·é•¿åº¦
        target_lengths = self.length_sampler.sample_lengths(
            batch_size=batch_size,
            conditions=conditions,
            distribution_type=config.length_distribution,
            temperature=config.temperature,
            device=self.device
        )
        
        return target_lengths
    
    def _sample_batch_sequences(self,
                               batch_size: int,
                               conditions: Dict[str, torch.Tensor],
                               target_lengths: torch.Tensor,
                               config: IntegratedSamplingConfig) -> torch.Tensor:
        """é‡‡æ ·æ‰¹æ¬¡åºåˆ—"""
        max_length = int(target_lengths.max().item())
        
        # åˆ›å»ºé•¿åº¦æ©ç 
        attention_mask = self.length_constrainer.create_length_mask(
            target_lengths, max_length, self.device
        )
        
        # åˆå§‹åŒ–å™ªå£°
        noise_shape = (batch_size, max_length, self.denoiser.hidden_dim)
        x_T = torch.randn(noise_shape, device=self.device)
        
        # CFGé‡‡æ ·è¿‡ç¨‹
        x_t = x_T
        timesteps = torch.linspace(
            self.diffusion.num_timesteps - 1, 0, 
            config.num_inference_steps, 
            dtype=torch.long, 
            device=self.device
        )
        
        for step_idx, t in enumerate(timesteps):
            t_batch = t.expand(batch_size)
            
            # è‡ªé€‚åº”å¼•å¯¼å¼ºåº¦
            if config.cfg_adaptive_guidance:
                guidance_scale = self.cfg.adaptive_guidance_scale(
                    step_idx, len(timesteps), config.cfg_guidance_scale
                )
            else:
                guidance_scale = config.cfg_guidance_scale
            
            # CFGå»å™ªæ­¥éª¤
            if config.cfg_multi_level_guidance:
                # å¤šçº§å¼•å¯¼
                guidance_scales = {
                    'peptide_type': guidance_scale,
                    'length': guidance_scale * 0.5,  # é•¿åº¦å¼•å¯¼å¼ºåº¦è¾ƒä½
                }
                
                noise_pred = self.cfg.multi_level_guidance(
                    lambda x, time, cond: self.denoiser(
                        x, time, attention_mask, conditions=cond
                    )[0],
                    x_t, t_batch, conditions, guidance_scales
                )
            else:
                # æ ‡å‡†CFG
                noise_pred = self.cfg.guided_denoising(
                    lambda x, time, cond: self.denoiser(
                        x, time, attention_mask, conditions=cond
                    )[0],
                    x_t, t_batch, conditions, guidance_scale
                )
            
            # æ‰©æ•£åå‘æ­¥éª¤
            x_t = self.diffusion.p_sample(noise_pred, x_t, t_batch)
            
            # åº”ç”¨é•¿åº¦çº¦æŸ
            x_t = x_t * attention_mask.unsqueeze(-1)
        
        # åå¤„ç†ï¼šè½¬æ¢ä¸ºåºåˆ—ID
        sequences = self._convert_embeddings_to_sequences(x_t, target_lengths)
        
        return sequences
    
    def _convert_embeddings_to_sequences(self, 
                                       embeddings: torch.Tensor,
                                       target_lengths: torch.Tensor) -> torch.Tensor:
        """å°†åµŒå…¥è½¬æ¢ä¸ºåºåˆ—IDï¼ˆç®€åŒ–å®ç°ï¼‰"""
        batch_size, max_length, hidden_dim = embeddings.shape
        
        # è¿™é‡Œåº”è¯¥ä½¿ç”¨å®é™…çš„è¯æ±‡è¡¨æ˜ å°„
        # ç®€åŒ–å®ç°ï¼šé€šè¿‡æœ€è¿‘é‚»æŸ¥æ‰¾æ˜ å°„åˆ°æ°¨åŸºé…¸ID
        
        # 20ä¸ªæ°¨åŸºé…¸çš„éšæœºæ˜ å°„ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨è®­ç»ƒå¥½çš„æ˜ å°„ï¼‰
        amino_acids = list(range(20))  # 0-19 å¯¹åº”20ç§æ°¨åŸºé…¸
        
        # ç®€åŒ–ï¼šæ ¹æ®åµŒå…¥çš„å‡å€¼æ˜ å°„åˆ°æ°¨åŸºé…¸
        sequence_ids = torch.zeros(batch_size, max_length, dtype=torch.long, device=embeddings.device)
        
        for i in range(batch_size):
            length = int(target_lengths[i].item())
            # ç®€åŒ–æ˜ å°„ï¼šä½¿ç”¨åµŒå…¥å‡å€¼çš„å“ˆå¸Œ
            for j in range(length):
                emb_hash = int(torch.sum(embeddings[i, j]).item()) % 20
                sequence_ids[i, j] = emb_hash
        
        return sequence_ids
    
    def _collect_batch_results(self,
                              sequences: torch.Tensor,
                              lengths: torch.Tensor,
                              conditions: Dict[str, torch.Tensor],
                              config: IntegratedSamplingConfig,
                              results: Dict[str, List]):
        """æ”¶é›†æ‰¹æ¬¡ç»“æœ"""
        batch_size = sequences.shape[0]
        
        for i in range(batch_size):
            length = int(lengths[i].item())
            sequence = sequences[i, :length].cpu().numpy()
            
            # è½¬æ¢ä¸ºæ°¨åŸºé…¸å­—ç¬¦ä¸²
            aa_sequence = self._ids_to_amino_acids(sequence)
            
            results['sequences'].append(aa_sequence)
            results['lengths'].append(length)
            results['peptide_types'].append(conditions['peptide_type_names'][i])
            results['guidance_scales'].append(config.cfg_guidance_scale)
            results['sampling_metadata'].append({
                'length_distribution': config.length_distribution,
                'temperature': config.temperature,
                'cfg_adaptive': config.cfg_adaptive_guidance
            })
    
    def _ids_to_amino_acids(self, sequence_ids: np.ndarray) -> str:
        """å°†åºåˆ—IDè½¬æ¢ä¸ºæ°¨åŸºé…¸å­—ç¬¦ä¸²"""
        # 20ç§æ°¨åŸºé…¸çš„æ˜ å°„
        id_to_aa = {
            0: 'A', 1: 'R', 2: 'N', 3: 'D', 4: 'C',
            5: 'Q', 6: 'E', 7: 'G', 8: 'H', 9: 'I',
            10: 'L', 11: 'K', 12: 'M', 13: 'F', 14: 'P',
            15: 'S', 16: 'T', 17: 'W', 18: 'Y', 19: 'V'
        }
        
        return ''.join([id_to_aa.get(int(id_val), 'X') for id_val in sequence_ids])
    
    def save_results(self, results: Dict[str, List], output_file: str):
        """ä¿å­˜ç”Ÿæˆç»“æœ"""
        print(f"ğŸ’¾ ä¿å­˜ç»“æœåˆ° {output_file}")
        
        with open(output_file, 'w') as f:
            for i, sequence in enumerate(results['sequences']):
                peptide_type = results['peptide_types'][i]
                length = results['lengths'][i]
                guidance_scale = results['guidance_scales'][i]
                
                header = f">peptide_{i:04d}|{peptide_type}|len={length}|cfg={guidance_scale:.1f}"
                f.write(f"{header}\n{sequence}\n")
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata_file = output_file.replace('.fasta', '_metadata.yaml')
        metadata = {
            'total_sequences': len(results['sequences']),
            'peptide_type_distribution': self._get_type_distribution(results['peptide_types']),
            'length_statistics': self._get_length_statistics(results['lengths']),
            'average_guidance_scale': np.mean(results['guidance_scales'])
        }
        
        with open(metadata_file, 'w') as f:
            yaml.dump(metadata, f, default_flow_style=False)
        
        print(f"ğŸ“Š å…ƒæ•°æ®ä¿å­˜åˆ° {metadata_file}")
    
    def _get_type_distribution(self, peptide_types: List[str]) -> Dict[str, int]:
        """è·å–è‚½ç±»å‹åˆ†å¸ƒ"""
        distribution = {}
        for ptype in peptide_types:
            distribution[ptype] = distribution.get(ptype, 0) + 1
        return distribution
    
    def _get_length_statistics(self, lengths: List[int]) -> Dict[str, float]:
        """è·å–é•¿åº¦ç»Ÿè®¡"""
        lengths_array = np.array(lengths)
        return {
            'mean': float(np.mean(lengths_array)),
            'std': float(np.std(lengths_array)),
            'min': int(np.min(lengths_array)),
            'max': int(np.max(lengths_array)),
            'median': float(np.median(lengths_array))
        }


def create_demo_models(device: str = 'cuda') -> Tuple[StructureAwareDenoiser, GaussianDiffusion]:
    """åˆ›å»ºæ¼”ç¤ºç”¨çš„æ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
    # ç®€åŒ–çš„å»å™ªå™¨é…ç½®
    denoiser_config = {
        'hidden_dim': 256,
        'num_heads': 8,
        'num_layers': 6,
        'dropout': 0.1,
        'use_cross_attention': True
    }
    
    # åˆ›å»ºCFGé…ç½®
    cfg_config = CFGConfig(
        dropout_prob=0.1,
        guidance_scale=2.5,
        adaptive_guidance=True
    )
    
    # åˆ›å»ºå»å™ªå™¨
    denoiser = StructureAwareDenoiser(
        seq_hidden_dim=256,
        struct_hidden_dim=256,
        denoiser_config=denoiser_config,
        cfg_config=cfg_config
    )
    
    # åˆ›å»ºæ‰©æ•£è¿‡ç¨‹
    diffusion = GaussianDiffusion(
        num_timesteps=1000,
        noise_schedule="cosine",
        beta_start=0.0001,
        beta_end=0.02
    )
    
    return denoiser, diffusion


def main():
    """ä¸»å‡½æ•°ï¼šCFG+é•¿åº¦é‡‡æ ·æ¼”ç¤º"""
    parser = argparse.ArgumentParser(description="CFG + é•¿åº¦é‡‡æ ·å™¨é›†æˆæ¼”ç¤º")
    parser.add_argument("--num_samples", type=int, default=50, help="ç”Ÿæˆåºåˆ—æ•°é‡")
    parser.add_argument("--batch_size", type=int, default=8, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--guidance_scale", type=float, default=2.5, help="CFGå¼•å¯¼å¼ºåº¦")
    parser.add_argument("--length_mean", type=float, default=25.0, help="é•¿åº¦åˆ†å¸ƒå‡å€¼")
    parser.add_argument("--length_std", type=float, default=8.0, help="é•¿åº¦åˆ†å¸ƒæ ‡å‡†å·®")
    parser.add_argument("--output", type=str, default="cfg_length_demo.fasta", help="è¾“å‡ºæ–‡ä»¶")
    parser.add_argument("--device", type=str, default="cuda", help="è®¡ç®—è®¾å¤‡")
    
    args = parser.parse_args()
    
    print("ğŸ”¬ CFG + é•¿åº¦é‡‡æ ·å™¨é›†æˆæ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºæ¨¡å‹
    print("ğŸ“¦ åˆå§‹åŒ–æ¨¡å‹...")
    denoiser, diffusion = create_demo_models(args.device)
    
    # åˆ›å»ºé…ç½®
    cfg_config = CFGConfig(
        dropout_prob=0.1,
        guidance_scale=args.guidance_scale,
        adaptive_guidance=True,
        multi_level_guidance=False
    )
    
    length_config = get_default_length_config()
    length_config.normal_mean = args.length_mean
    length_config.normal_std = args.length_std
    
    sampling_config = IntegratedSamplingConfig(
        cfg_guidance_scale=args.guidance_scale,
        length_mean=args.length_mean,
        length_std=args.length_std,
        batch_size=args.batch_size,
        num_samples=args.num_samples,
        output_file=args.output,
        peptide_types=['antimicrobial', 'antifungal', 'antiviral']
    )
    
    # åˆ›å»ºé›†æˆé‡‡æ ·å™¨
    print("ğŸ”§ åˆ›å»ºé›†æˆé‡‡æ ·å™¨...")
    sampler = CFGLengthIntegratedSampler(
        denoiser=denoiser,
        diffusion=diffusion,
        cfg_config=cfg_config,
        length_config=length_config,
        device=args.device
    )
    
    # æ‰§è¡Œé‡‡æ ·
    print("ğŸ¯ å¼€å§‹é‡‡æ ·...")
    results = sampler.sample_with_cfg_and_length(sampling_config)
    
    # ä¿å­˜ç»“æœ
    sampler.save_results(results, args.output)
    
    # æ˜¾ç¤ºç»Ÿè®¡
    print("\nğŸ“Š ç”Ÿæˆç»Ÿè®¡:")
    print(f"  æ€»åºåˆ—æ•°: {len(results['sequences'])}")
    print(f"  å¹³å‡é•¿åº¦: {np.mean(results['lengths']):.1f}Â±{np.std(results['lengths']):.1f}")
    print(f"  é•¿åº¦èŒƒå›´: {min(results['lengths'])}-{max(results['lengths'])}")
    
    type_dist = sampler._get_type_distribution(results['peptide_types'])
    print(f"  ç±»å‹åˆ†å¸ƒ: {type_dist}")
    
    print(f"\nâœ… æ¼”ç¤ºå®Œæˆï¼ç»“æœä¿å­˜åœ¨ {args.output}")


if __name__ == "__main__":
    main()