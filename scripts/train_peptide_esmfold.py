#!/usr/bin/env python3
"""
å¤šè‚½ç”Ÿæˆè®­ç»ƒè„šæœ¬ - å¯ç”¨ESMFoldç»“æ„é¢„æµ‹
"""

import os
import sys
import argparse
import logging
import gc
from pathlib import Path
from datetime import datetime
from typing import Dict, Optional, List, Tuple
import math
import json
from collections import defaultdict, Counter
from statistics import mean, stdev
import tempfile

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from torch.cuda.amp import GradScaler, autocast
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("Warning: wandb not available, training metrics will not be logged to W&B")

from tqdm import tqdm
import yaml
from omegaconf import OmegaConf
from Bio import SeqIO, Align
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
import numpy as np
import pandas as pd

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥æ‰€éœ€æ¨¡å—
from fix_esmfold_patch import apply_esmfold_patch
from structdiff.models.structdiff import StructDiff
from structdiff.models.esmfold_wrapper import ESMFoldWrapper
from structdiff.data.dataset import PeptideStructureDataset
from structdiff.data.collator import PeptideStructureCollator
from structdiff.utils.checkpoint import CheckpointManager
from structdiff.utils.logger import setup_logger, get_logger
from structdiff.utils.ema import EMA

# å°è¯•å¯¼å…¥modlampç”¨äºä¸ç¨³å®šæ€§æŒ‡æ•°è®¡ç®—
try:
    from modlamp.descriptors import GlobalDescriptor
    MODLAMP_AVAILABLE = True
    print("âœ… modlAMPå·²å®‰è£…ï¼Œç†åŒ–æ€§è´¨è®¡ç®—å¯ç”¨")
except ImportError:
    MODLAMP_AVAILABLE = False
    print("âš ï¸ modlamp not available, instability index will be skipped")
    print("âš ï¸ modlamp not available, instability index will be skipped")

logger = get_logger(__name__)


def parse_args():
    parser = argparse.ArgumentParser(description="Train StructDiff with ESMFold")
    parser.add_argument(
        "--config",
        type=str,
        default="configs/peptide_esmfold_config.yaml",
        help="Path to configuration file"
    )
    parser.add_argument(
        "--resume",
        type=str,
        default=None,
        help="Path to checkpoint to resume from"
    )
    parser.add_argument(
        "--gpu",
        type=int,
        default=0,
        help="GPU device to use"
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Run in debug mode with reduced data"
    )
    parser.add_argument(
        "--test-run",
        action="store_true",
        help="Test run with minimal epochs"
    )
    return parser.parse_args()


def setup_environment(args, config):
    """è®¾ç½®ç¯å¢ƒå’Œè®¾å¤‡"""
    # ä¼˜å…ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„GPUè®¾ç½®ï¼Œç„¶åæ˜¯å‘½ä»¤è¡Œå‚æ•°
    if hasattr(config, 'system') and hasattr(config.system, 'cuda_visible_devices'):
        gpu_id = config.system.cuda_visible_devices
        logger.info(f"ä»é…ç½®æ–‡ä»¶è¯»å–GPUè®¾ç½®: {gpu_id}")
    else:
        gpu_id = str(args.gpu)
        logger.info(f"ä½¿ç”¨å‘½ä»¤è¡ŒGPUå‚æ•°: {gpu_id}")
    
    # è®¾ç½®GPUç¯å¢ƒå˜é‡
    os.environ["CUDA_VISIBLE_DEVICES"] = gpu_id
    logger.info(f"è®¾ç½® CUDA_VISIBLE_DEVICES={gpu_id}")
    
    # åº”ç”¨ESMFoldè¡¥ä¸
    logger.info("åº”ç”¨ESMFoldå…¼å®¹æ€§è¡¥ä¸...")
    apply_esmfold_patch()
    logger.info("âœ“ ESMFoldè¡¥ä¸åº”ç”¨æˆåŠŸ")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    if torch.cuda.is_available():
        actual_gpu_id = torch.cuda.current_device()
        gpu_props = torch.cuda.get_device_properties(actual_gpu_id)
        logger.info(f"å®é™…ä½¿ç”¨GPU {actual_gpu_id}: {gpu_props.name}")
        logger.info(f"GPUå†…å­˜: {gpu_props.total_memory / 1e9:.1f}GB")
        
        # æ¸…ç†æ˜¾å­˜
        torch.cuda.empty_cache()
        logger.info(f"å½“å‰å†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1e9:.1f}GB")
        logger.info(f"å¯ç”¨å†…å­˜: {(gpu_props.total_memory - torch.cuda.memory_allocated()) / 1e9:.1f}GB")
    
    return device


def setup_shared_esmfold(config: Dict, device: torch.device):
    """åˆ›å»ºå…±äº«çš„ESMFoldå®ä¾‹ä»¥èŠ‚çœå†…å­˜"""
    shared_esmfold = None
    
    if (config.model.structure_encoder.get('use_esmfold', False) and 
        config.data.get('use_predicted_structures', False)):
        
        logger.info("æ­£åœ¨åˆ›å»ºå…±äº«ESMFoldå®ä¾‹...")
        
        # æ›´æ¿€è¿›çš„å†…å­˜æ¸…ç†ï¼Œä¸ºESMFoldè…¾å‡ºç©ºé—´
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
            # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:128'
            
            # å†æ¬¡æ¸…ç†
            torch.cuda.empty_cache()
            
            print(f"ğŸ§¹ ESMFoldåˆå§‹åŒ–å‰GPUå†…å­˜æ¸…ç†å®Œæˆ: {torch.cuda.memory_allocated()/1024**3:.2f}GB")
        
        try:
            # é¦–å…ˆå°è¯•GPU
            shared_esmfold = ESMFoldWrapper(device=device)
            
            if shared_esmfold.available:
                logger.info("âœ“ å…±äº«ESMFold GPUå®ä¾‹åˆ›å»ºæˆåŠŸ")
                logger.info(f"ESMFoldå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1e9:.1f}GB")
            else:
                logger.error("âŒ ESMFold GPUå®ä¾‹åˆ›å»ºå¤±è´¥")
                shared_esmfold = None
                
        except Exception as gpu_error:
            logger.warning(f"âš ï¸ ESMFold GPUåˆå§‹åŒ–å¤±è´¥: {gpu_error}")
            
            try:
                # å°è¯•CPU fallback
                logger.info("ğŸ”„ å°è¯•ä½¿ç”¨CPUåˆ›å»ºESMFold...")
                shared_esmfold = ESMFoldWrapper(device='cpu')
                if shared_esmfold.available:
                    logger.info("âœ… ESMFold CPUå®ä¾‹åˆ›å»ºæˆåŠŸ")
                else:
                    shared_esmfold = None
            except Exception as cpu_error:
                logger.error(f"âŒ ESMFold CPUåˆå§‹åŒ–ä¹Ÿå¤±è´¥: {cpu_error}")
                shared_esmfold = None
    
    return shared_esmfold


def clear_memory():
    """æ¸…ç†GPUå†…å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()


def create_data_loaders(config: Dict, shared_esmfold, debug: bool = False):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    logger.info("æ­£åœ¨åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    
    try:
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = PeptideStructureDataset(
            config.data.train_path,
            config,
            is_training=True,
            cache_dir=config.data.get('structure_cache_dir', './cache/train'),
            shared_esmfold=shared_esmfold
        )
        
        val_dataset = PeptideStructureDataset(
            config.data.val_path,
            config,
            is_training=False,
            cache_dir=config.data.get('structure_cache_dir', './cache/val'),
            shared_esmfold=shared_esmfold
        )
        
        # Debugæ¨¡å¼ä½¿ç”¨å­é›†
        if debug:
            train_subset_size = min(20, len(train_dataset))  # è¿›ä¸€æ­¥å‡å°‘åˆ°20ä¸ªæ ·æœ¬
            val_subset_size = min(10, len(val_dataset))     # å‡å°‘åˆ°10ä¸ªæ ·æœ¬
            
            # ç›´æ¥ä¿®æ”¹æ•°æ®é›†çš„æ•°æ®ï¼Œè€Œä¸æ˜¯ä½¿ç”¨Subset
            train_dataset.data = train_dataset.data.head(train_subset_size)
            val_dataset.data = val_dataset.data.head(val_subset_size)
            
            logger.info(f"Debugæ¨¡å¼: è®­ç»ƒ{train_subset_size}, éªŒè¯{val_subset_size}")
        
        logger.info(f"è®­ç»ƒæ•°æ®é›†: {len(train_dataset)} æ ·æœ¬")
        logger.info(f"éªŒè¯æ•°æ®é›†: {len(val_dataset)} æ ·æœ¬")
        
        # åˆ›å»ºæ•°æ®æ•´ç†å™¨
        collator = PeptideStructureCollator(config)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - ä½¿ç”¨æ›´ä¿å®ˆçš„è®¾ç½®
        train_loader = DataLoader(
            train_dataset,
            batch_size=config.data.batch_size,
            shuffle=True,
            num_workers=0,  # CRITICAL FIX: ä½¿ç”¨0é¿å…å¤šè¿›ç¨‹ç¼“å­˜ç«äº‰é—®é¢˜
            pin_memory=False,  # ç¦ç”¨pin_memoryèŠ‚çœå†…å­˜
            collate_fn=collator,
            drop_last=True
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=config.data.batch_size,
            shuffle=False,  
            num_workers=0,  # CRITICAL FIX: ä½¿ç”¨0é¿å…å¤šè¿›ç¨‹ç¼“å­˜ç«äº‰é—®é¢˜
            pin_memory=False,  # ç¦ç”¨pin_memoryèŠ‚çœå†…å­˜
            collate_fn=collator,
            drop_last=False
        )
        
        return train_loader, val_loader
        
    except Exception as e:
        logger.error(f"æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥: {e}")
        raise


def setup_model_and_training(config: Dict, device: torch.device, shared_esmfold):
    """è®¾ç½®æ¨¡å‹å’Œè®­ç»ƒç»„ä»¶"""
    logger.info("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
    
    try:
        # å¤‡ä»½åŸå§‹é…ç½®
        original_use_esmfold = config.model.structure_encoder.get('use_esmfold', False)
        
        # å¦‚æœå·²æœ‰å…±äº«å®ä¾‹ï¼Œä¸´æ—¶ç¦ç”¨æ¨¡å‹å†…éƒ¨çš„ESMFoldåŠ è½½
        if shared_esmfold and shared_esmfold.available:
            logger.info("ä¸´æ—¶ç¦ç”¨æ¨¡å‹å†…éƒ¨ESMFoldåŠ è½½ä»¥é¿å…å†…å­˜ä¸è¶³...")
            config.model.structure_encoder.use_esmfold = False
        
        # åˆ›å»ºæ¨¡å‹
        model = StructDiff(config).to(device)
        
        # æ¢å¤é…ç½®å¹¶è®¾ç½®å…±äº«å®ä¾‹
        config.model.structure_encoder.use_esmfold = original_use_esmfold
        
        # å¦‚æœæœ‰å…±äº«çš„ESMFoldå®ä¾‹ï¼Œæ‰‹åŠ¨è®¾ç½®åˆ°æ¨¡å‹ä¸­
        if shared_esmfold and shared_esmfold.available:
            logger.info("æ­£åœ¨å°†å…±äº« ESMFold å®ä¾‹è®¾ç½®åˆ°æ¨¡å‹ä¸­...")
            if hasattr(model.structure_encoder, 'esmfold') or hasattr(model.structure_encoder, '_esmfold'):
                # è®¾ç½®ESMFoldå®ä¾‹
                model.structure_encoder.esmfold = shared_esmfold
                model.structure_encoder._esmfold = shared_esmfold
                # ç¡®ä¿ESMFoldè¢«æ ‡è®°ä¸ºå¯ç”¨
                model.structure_encoder.use_esmfold = True
                logger.info("âœ“ å…±äº« ESMFold å®ä¾‹å·²è®¾ç½®åˆ°æ¨¡å‹ä¸­")
            else:
                # å¦‚æœæ¨¡å‹ç»“æ„ä¸åŒï¼Œå°è¯•ç›´æ¥è®¾ç½®å±æ€§
                setattr(model.structure_encoder, 'esmfold', shared_esmfold)
                setattr(model.structure_encoder, 'use_esmfold', True)
                logger.info("âœ“ å…±äº« ESMFold å®ä¾‹å·²å¼ºåˆ¶è®¾ç½®åˆ°æ¨¡å‹ä¸­")
            
            clear_memory()
        
        logger.info(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # åˆ›å»ºä¼˜åŒ–å™¨ - åªæ’é™¤ESMFoldå‚æ•°ï¼Œä¿ç•™å…¶ä»–æ‰€æœ‰å‚æ•°
        trainable_params = []
        esmfold_params = 0
        total_params = 0
        sequence_encoder_params = 0
        other_params = 0
        
        for name, param in model.named_parameters():
            total_params += param.numel()
            
            # åªæ’é™¤çœŸæ­£çš„ESMFoldæ¨¡å‹å‚æ•°ï¼Œä¿ç•™å…¶ä»–æ‰€æœ‰å‚æ•°
            if ('structure_encoder.esmfold.' in name or 
                'structure_encoder._esmfold.' in name or
                name.startswith('esmfold.')):
                esmfold_params += param.numel()
                param.requires_grad = False  # å†»ç»“ESMFoldå‚æ•°
                logger.debug(f"å†»ç»“ESMFoldå‚æ•°: {name}")
            else:
                trainable_params.append(param)
                if 'sequence_encoder' in name:
                    sequence_encoder_params += param.numel()
                else:
                    other_params += param.numel()
        
        trainable_param_count = sum(p.numel() for p in trainable_params)
        
        logger.info(f"å‚æ•°ç»Ÿè®¡:")
        logger.info(f"  æ€»å‚æ•°: {total_params:,}")
        logger.info(f"  ESMFoldå‚æ•°(å†»ç»“): {esmfold_params:,}")
        logger.info(f"  åºåˆ—ç¼–ç å™¨å‚æ•°: {sequence_encoder_params:,}")
        logger.info(f"  å…¶ä»–å¯è®­ç»ƒå‚æ•°: {other_params:,}")
        logger.info(f"  å¯è®­ç»ƒå‚æ•°æ€»è®¡: {trainable_param_count:,}")
        
        if trainable_param_count < 1000000:  # å°‘äº100ä¸‡å‚æ•°
            logger.warning(f"âš ï¸ å¯è®­ç»ƒå‚æ•°è¿‡å°‘ ({trainable_param_count:,})ï¼Œå¯èƒ½å½±å“è®­ç»ƒæ•ˆæœ")
            
            # æ‰“å°å‰10ä¸ªå¯è®­ç»ƒå‚æ•°çš„åç§°
            logger.info("å¯è®­ç»ƒå‚æ•°ç¤ºä¾‹:")
            for i, (name, param) in enumerate(model.named_parameters()):
                if param.requires_grad:
                    logger.info(f"  {name}: {param.shape}")
                    if i >= 9:  # åªæ˜¾ç¤ºå‰10ä¸ª
                        break
        
        optimizer = torch.optim.AdamW(
            trainable_params,  # åªä¼˜åŒ–éESMFoldå‚æ•°
            lr=config.training.optimizer.lr,
            betas=config.training.optimizer.betas,
            weight_decay=config.training.optimizer.weight_decay,
            eps=config.training.optimizer.eps
        )
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=config.training.num_epochs,
            eta_min=config.training.scheduler.min_lr
        )
        
        # åˆ›å»ºEMA
        ema = None
        if config.training.use_ema:
            ema = EMA(
                model, 
                decay=config.training.ema_decay,
                device=device
            )
        
        logger.info(f"æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ŒGPUå†…å­˜: {torch.cuda.memory_allocated() / 1e9:.1f}GB")
        
        return model, optimizer, scheduler, ema
        
    except Exception as e:
        logger.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•å®Œå…¨ç¦ç”¨ESMFold
        logger.info("å°è¯•ç¦ç”¨ESMFoldé‡æ–°åˆå§‹åŒ–æ¨¡å‹...")
        try:
            config.model.structure_encoder.use_esmfold = False
            config.data.use_predicted_structures = False
            model = StructDiff(config).to(device)
            logger.info("âœ“ æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼ˆæœªä½¿ç”¨ESMFoldï¼‰")
            
            # åˆ›å»ºåŸºç¡€ç»„ä»¶
            optimizer = torch.optim.AdamW(
                model.parameters(),
                lr=config.training.optimizer.lr,
                betas=config.training.optimizer.betas,
                weight_decay=config.training.optimizer.weight_decay,
                eps=config.training.optimizer.eps
            )
            
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=config.training.num_epochs,
                eta_min=config.training.scheduler.min_lr
            )
            
            ema = None
            if config.training.use_ema:
                ema = EMA(
                    model, 
                    decay=config.training.ema_decay,
                    device=device
                )
            
            return model, optimizer, scheduler, ema
            
        except Exception as e2:
            logger.error(f"ç¦ç”¨ESMFoldåä»ç„¶å¤±è´¥: {e2}")
            raise


def move_to_device(obj, device):
    """é€’å½’åœ°å°†å¯¹è±¡ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
    if isinstance(obj, torch.Tensor):
        return obj.to(device)
    elif isinstance(obj, dict):
        return {k: move_to_device(v, device) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [move_to_device(item, device) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(move_to_device(item, device) for item in obj)
    else:
        return obj


def train_step(
    model, batch, optimizer, scaler, config, device, 
    gradient_accumulation_steps, step
):
    """å•æ­¥è®­ç»ƒ - ä¿®å¤ç‰ˆæœ¬"""
    try:
        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡ - ä½¿ç”¨æ›´å¼ºé²æ£’çš„å‡½æ•°
        batch = move_to_device(batch, device)
        
        # æ£€æŸ¥batchçš„åŸºæœ¬å­—æ®µ
        if 'sequences' not in batch or 'attention_mask' not in batch:
            logger.warning(f"Batch missing required fields")
            return None, float('inf')
        
        # æ£€æŸ¥å¼ é‡å½¢çŠ¶ä¸€è‡´æ€§
        seq_shape = batch['sequences'].shape
        mask_shape = batch['attention_mask'].shape
        
        if seq_shape != mask_shape:
            logger.warning(f"Shape mismatch: sequences {seq_shape} vs attention_mask {mask_shape}")
            # ä¿®æ­£attention_maskçš„å½¢çŠ¶
            if mask_shape[1] != seq_shape[1]:
                min_len = min(mask_shape[1], seq_shape[1])
                batch['sequences'] = batch['sequences'][:, :min_len]
                batch['attention_mask'] = batch['attention_mask'][:, :min_len]
                logger.info(f"Adjusted shapes to: {batch['sequences'].shape}")
        
        # æ£€æŸ¥ç»“æ„æ•°æ®çš„å½¢çŠ¶ä¸€è‡´æ€§
        if 'structures' in batch and batch['structures'] is not None:
            expected_struct_len = batch['sequences'].shape[1] - 2  # é™¤å»CLS/SEP
            
            for key, value in batch['structures'].items():
                if value is None:
                    continue
                    
                # å¯¹äºç»“æ„ç‰¹å¾ï¼Œç¬¬äºŒä¸ªç»´åº¦åº”è¯¥ä¸åºåˆ—é•¿åº¦-2åŒ¹é…
                if len(value.shape) >= 2:
                    actual_len = value.shape[1]
                    if actual_len != expected_struct_len:
                        logger.warning(f"Structure '{key}' length mismatch: {actual_len} vs expected {expected_struct_len}")
                        
                        # æˆªæ–­æˆ–å¡«å……ç»“æ„ç‰¹å¾
                        if actual_len > expected_struct_len:
                            if len(value.shape) == 2:
                                batch['structures'][key] = value[:, :expected_struct_len]
                            elif len(value.shape) == 3:
                                if 'matrix' in key or 'map' in key:
                                    batch['structures'][key] = value[:, :expected_struct_len, :expected_struct_len]
                                else:
                                    batch['structures'][key] = value[:, :expected_struct_len, :]
                        elif actual_len < expected_struct_len:
                            pad_size = expected_struct_len - actual_len
                            if len(value.shape) == 2:
                                batch['structures'][key] = F.pad(value, (0, pad_size), value=0)
                            elif len(value.shape) == 3:
                                if 'matrix' in key or 'map' in key:
                                    batch['structures'][key] = F.pad(value, (0, pad_size, 0, pad_size), value=0)
                                else:
                                    batch['structures'][key] = F.pad(value, (0, 0, 0, pad_size), value=0)
        
        # é‡‡æ ·æ—¶é—´æ­¥
        batch_size = batch['sequences'].shape[0]
        timesteps = torch.randint(
            0, config.diffusion.num_timesteps,
            (batch_size,), device=device
        )
        
        # å‰å‘ä¼ æ’­
        if config.training.use_amp:
            with autocast():
                outputs = model(
                    sequences=batch['sequences'],
                    attention_mask=batch['attention_mask'],
                    timesteps=timesteps,
                    structures=batch.get('structures'),
                    conditions=batch.get('conditions'),
                    return_loss=True
                )
        else:
            outputs = model(
                sequences=batch['sequences'],
                attention_mask=batch['attention_mask'],
                timesteps=timesteps,
                structures=batch.get('structures'),
                conditions=batch.get('conditions'),
                return_loss=True
            )
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æŸå¤±
        if 'total_loss' not in outputs:
            logger.warning("Model output missing 'total_loss', checking for alternatives...")
            if 'diffusion_loss' in outputs:
                outputs['total_loss'] = outputs['diffusion_loss']
            elif 'loss' in outputs:
                outputs['total_loss'] = outputs['loss']
            else:
                logger.error("No loss found in model outputs")
                return None, float('inf')
        
        loss = outputs['total_loss'] / gradient_accumulation_steps
        
        # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºNaN
        if torch.isnan(loss) or torch.isinf(loss):
            logger.warning(f"Invalid loss detected: {loss}")
            return None, float('inf')
        
        # åå‘ä¼ æ’­
        if config.training.use_amp:
            scaler.scale(loss).backward()
        else:
            loss.backward()
        
        return outputs, loss.item() * gradient_accumulation_steps
        
    except Exception as e:
        logger.error(f"è®­ç»ƒæ­¥éª¤å¤±è´¥: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return None, float('inf')


def generate_and_validate(config, device):
    """è®­ç»ƒå®Œæˆåçš„ç”Ÿæˆå’ŒéªŒè¯åŠŸèƒ½"""
    logger.info("ğŸš€ å¼€å§‹ç”Ÿæˆå’ŒéªŒè¯åŠŸèƒ½...")
    
    try:
        # æŸ¥æ‰¾æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹ - å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
        possible_checkpoint_paths = []
        
        # å°è¯•ä»é…ç½®ä¸­è·å–è¾“å‡ºç›®å½•
        if hasattr(config, 'experiment') and hasattr(config.experiment, 'output_dir'):
            base_dir = Path(config.experiment.output_dir) / config.experiment.name
            possible_checkpoint_paths.extend([
                base_dir / "checkpoints" / "best_model.pth",  # ä¿®æ­£æ‰©å±•å
                base_dir / "checkpoints" / "best_model.pt",   # ä¿ç•™å…¼å®¹æ€§
                base_dir / "checkpoints" / "latest.pth",      # æ·»åŠ latestæ£€æŸ¥ç‚¹
                base_dir / "best_model.pth",
                base_dir / "best_model.pt"
            ])
        
        if hasattr(config, 'training') and hasattr(config.training, 'output_dir'):
            base_dir = Path(config.training.output_dir)
            possible_checkpoint_paths.extend([
                base_dir / "checkpoints" / "best_model.pth",
                base_dir / "checkpoints" / "best_model.pt",
                base_dir / "checkpoints" / "latest.pth",
                base_dir / "best_model.pth",
                base_dir / "best_model.pt"
            ])
        
        # æ·»åŠ é»˜è®¤è·¯å¾„
        possible_checkpoint_paths.extend([
            Path("./outputs/checkpoints/best_model.pth"),
            Path("./outputs/checkpoints/best_model.pt"),
            Path("./outputs/checkpoints/latest.pth"),
            Path("./outputs/best_model.pth"),
            Path("./outputs/best_model.pt"),
            Path("./checkpoints/best_model.pth"),
            Path("./checkpoints/best_model.pt"),
            Path("./checkpoints/latest.pth"),
            Path("./best_model.pth"),
            Path("./best_model.pt")
        ])
        
        best_checkpoint = None
        for checkpoint_path in possible_checkpoint_paths:
            if checkpoint_path.exists():
                best_checkpoint = checkpoint_path
                break
        
        if best_checkpoint is None:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œè·³è¿‡ç”ŸæˆéªŒè¯")
            logger.info(f"å°è¯•çš„è·¯å¾„: {[str(p) for p in possible_checkpoint_paths]}")
            return
        
        logger.info(f"ğŸ“‚ åŠ è½½æœ€ä½³æ¨¡å‹: {best_checkpoint}")
        
        # é‡æ–°åˆ›å»ºå…±äº«ESMFoldå®ä¾‹
        shared_esmfold = setup_shared_esmfold(config, device)
        
        # åˆ›å»ºæ¨¡å‹å¹¶åŠ è½½æƒé‡
        model, _, _, _ = setup_model_and_training(config, device, shared_esmfold)
        
        # åŠ è½½æ£€æŸ¥ç‚¹ - å¤„ç†PyTorch 2.6çš„å®‰å…¨é™åˆ¶
        try:
            checkpoint = torch.load(best_checkpoint, map_location=device, weights_only=False)
        except Exception as e:
            logger.warning(f"ä½¿ç”¨weights_only=FalseåŠ è½½å¤±è´¥: {e}")
            # å°è¯•ä½¿ç”¨å®‰å…¨å…¨å±€å˜é‡
            try:
                from omegaconf.listconfig import ListConfig
                from omegaconf.dictconfig import DictConfig
                torch.serialization.add_safe_globals([ListConfig, DictConfig])
                checkpoint = torch.load(best_checkpoint, map_location=device)
            except Exception as e2:
                logger.error(f"æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e2}")
                return
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ’é™¤çš„é”®ä¿¡æ¯
        excluded_keys = checkpoint.get('excluded_keys', [])
        if excluded_keys:
            logger.info(f"ğŸ“ æ£€æŸ¥ç‚¹æ’é™¤äº†ä»¥ä¸‹å‚æ•°: {excluded_keys}")
            logger.info("ğŸ’¡ è¿™äº›å‚æ•°å°†ä½¿ç”¨æ¨¡å‹çš„é»˜è®¤åˆå§‹åŒ–å€¼")
        
        # åŠ è½½æ¨¡å‹çŠ¶æ€å­—å…¸ï¼Œå…è®¸éƒ¨åˆ†åŒ¹é…
        missing_keys, unexpected_keys = model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        
        if missing_keys:
            logger.info(f"âš ï¸ ç¼ºå¤±çš„å‚æ•°é”® (å°†ä½¿ç”¨é»˜è®¤å€¼): {len(missing_keys)} ä¸ª")
            # åªæ˜¾ç¤ºå‰å‡ ä¸ªï¼Œé¿å…æ—¥å¿—è¿‡é•¿
            if len(missing_keys) <= 5:
                for key in missing_keys:
                    logger.info(f"   - {key}")
            else:
                for key in missing_keys[:3]:
                    logger.info(f"   - {key}")
                logger.info(f"   ... è¿˜æœ‰ {len(missing_keys)-3} ä¸ª")
        
        if unexpected_keys:
            logger.info(f"âš ï¸ æ„å¤–çš„å‚æ•°é”®: {len(unexpected_keys)} ä¸ª")
            if len(unexpected_keys) <= 5:
                for key in unexpected_keys:
                    logger.info(f"   - {key}")
        
        model.eval()
        
        logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå‡†å¤‡å¼€å§‹ç”ŸæˆéªŒè¯")
        
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = PeptideEvaluator(model, config, device, shared_esmfold)
        
        # åŠ è½½å‚è€ƒåºåˆ—ç”¨äºç›¸ä¼¼æ€§è®¡ç®—
        reference_sequences = {}
        try:
            # åŠ è½½è®­ç»ƒæ•°æ®ä½œä¸ºå‚è€ƒ
            train_data_path = "./data/processed/train.csv"
            if os.path.exists(train_data_path):
                train_df = pd.read_csv(train_data_path)
                
                # æŒ‰è‚½ç±»å‹åˆ†ç»„å‚è€ƒåºåˆ—
                type_mapping = {'antimicrobial': 0, 'antifungal': 1, 'antiviral': 2}
                for peptide_type, type_id in type_mapping.items():
                    type_sequences = train_df[train_df['label'] == type_id]['sequence'].tolist()
                    reference_sequences[peptide_type] = type_sequences[:200]  # é™åˆ¶æ•°é‡ä»¥æé«˜æ•ˆç‡
                    logger.info(f"åŠ è½½ {len(reference_sequences[peptide_type])} æ¡ {peptide_type} å‚è€ƒåºåˆ—")
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®ï¼Œè·³è¿‡ç›¸ä¼¼æ€§è®¡ç®—")
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½å‚è€ƒåºåˆ—å¤±è´¥: {e}")
        
        # å¯¹æ¯ç§è‚½ç±»å‹è¿›è¡Œè¯„ä¼°
        peptide_types = ['antimicrobial', 'antifungal', 'antiviral']
        all_results = {}
        
        for peptide_type in peptide_types:
            logger.info(f"ğŸ§¬ å¼€å§‹è¯„ä¼° {peptide_type} å¤šè‚½...")
            
            try:
                results, sequences = evaluator.comprehensive_evaluation(
                    peptide_type=peptide_type,
                    sample_num=50,  # ç”Ÿæˆ50ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°ï¼ˆå‡å°‘æ•°é‡ä»¥åŠ å¿«é€Ÿåº¦ï¼‰
                    max_length=50,
                    reference_sequences=reference_sequences.get(peptide_type, None)
                )
                
                all_results[peptide_type] = results
                
                # ä¿å­˜ç”Ÿæˆçš„åºåˆ—
                if hasattr(config, 'experiment') and hasattr(config.experiment, 'output_dir'):
                    output_dir = Path(config.experiment.output_dir) / config.experiment.name
                elif hasattr(config, 'training') and hasattr(config.training, 'output_dir'):
                    output_dir = Path(config.training.output_dir)
                else:
                    output_dir = Path("./outputs")
                
                output_dir.mkdir(parents=True, exist_ok=True)
                
                fasta_file = output_dir / f"generated_{peptide_type}_sequences.fasta"
                evaluator.save_sequences_to_fasta(sequences, fasta_file, peptide_type)
                
                logger.info(f"âœ… {peptide_type} è¯„ä¼°å®Œæˆï¼Œç”Ÿæˆ {len(sequences)} æ¡åºåˆ—")
                
            except Exception as e:
                logger.error(f"âŒ {peptide_type} è¯„ä¼°å¤±è´¥: {e}")
                continue
        
        # ä¿å­˜æ‰€æœ‰ç»“æœ
        results_file = output_dir / "generation_evaluation_results.json"
        with open(results_file, 'w') as f:
            json.dump(all_results, f, indent=2)
        
        # æ‰“å°ç»“æœæ‘˜è¦
        logger.info("\n" + "="*80)
        logger.info("ğŸ¯ ç”Ÿæˆè¯„ä¼°ç»“æœæ‘˜è¦ - ä¸“ä¸šç”Ÿç‰©å­¦æŒ‡æ ‡")
        logger.info("="*80)
        
        for peptide_type, results in all_results.items():
            logger.info(f"\nğŸ“Š {peptide_type.upper()} å¤šè‚½è¯„ä¼°ç»“æœ:")
            logger.info("-" * 50)
            
            # 1. ä¼ªå›°æƒ‘åº¦
            if 'pseudo_perplexity' in results:
                pp = results['pseudo_perplexity']
                logger.info(f"ğŸ§® ä¼ªå›°æƒ‘åº¦ (Pseudo-Perplexity):")
                logger.info(f"   å¹³å‡å€¼: {pp.get('mean_pseudo_perplexity', 0):.4f} Â± {pp.get('std_pseudo_perplexity', 0):.4f}")
                logger.info(f"   æœ‰æ•ˆåºåˆ—: {pp.get('valid_sequences', 0)}")
            
            # 2. Shannonä¿¡æ¯ç†µ
            if 'shannon_entropy' in results:
                se = results['shannon_entropy']
                logger.info(f"ğŸ“Š Shannonä¿¡æ¯ç†µ:")
                logger.info(f"   åºåˆ—å¹³å‡ç†µ: {se.get('mean_sequence_entropy', 0):.4f} Â± {se.get('std_sequence_entropy', 0):.4f}")
                logger.info(f"   æ•´ä½“ç†µ: {se.get('overall_entropy', 0):.4f} / {se.get('max_possible_entropy', 4.32):.2f}")
            
            # 3. ä¸ç¨³å®šæ€§æŒ‡æ•°
            if 'instability_index' in results:
                ii = results['instability_index']
                logger.info(f"ğŸ§ª ä¸ç¨³å®šæ€§æŒ‡æ•° (Instability Index):")
                logger.info(f"   å¹³å‡å€¼: {ii.get('mean_instability_index', 0):.4f} Â± {ii.get('std_instability_index', 0):.4f}")
                stable = ii.get('stable_peptides', 0)
                unstable = ii.get('unstable_peptides', 0)
                total = stable + unstable
                if total > 0:
                    logger.info(f"   ç¨³å®šè‚½ (â‰¤40): {stable}/{total} ({stable/total*100:.1f}%)")
                    logger.info(f"   ä¸ç¨³å®šè‚½ (>40): {unstable}/{total} ({unstable/total*100:.1f}%)")
            
            # 4. BLOSUM62ç›¸ä¼¼æ€§
            if 'blosum62_similarity' in results:
                bs = results['blosum62_similarity']
                logger.info(f"ğŸ” BLOSUM62ç›¸ä¼¼æ€§å¾—åˆ†:")
                logger.info(f"   å¹³å‡ç›¸ä¼¼æ€§: {bs.get('mean_similarity_score', 0):.4f} Â± {bs.get('std_similarity_score', 0):.4f}")
                if 'max_similarity_score' in bs:
                    logger.info(f"   æœ€é«˜ç›¸ä¼¼æ€§: {bs['max_similarity_score']:.4f}")
                    logger.info(f"   æœ€ä½ç›¸ä¼¼æ€§: {bs['min_similarity_score']:.4f}")
            
            # 5. å¤šæ ·æ€§åˆ†æ
            if 'diversity_analysis' in results:
                da = results['diversity_analysis']
                logger.info(f"ğŸ“ˆ å¤šæ ·æ€§åˆ†æ:")
                logger.info(f"   å”¯ä¸€æ€§æ¯”ä¾‹: {da.get('uniqueness_ratio', 0):.4f}")
                logger.info(f"   æ€»åºåˆ—æ•°: {da.get('total_sequences', 0)}")
                logger.info(f"   å”¯ä¸€åºåˆ—æ•°: {da.get('unique_sequences', 0)}")
                logger.info(f"   é‡å¤åºåˆ—æ•°: {da.get('duplicate_sequences', 0)}")
                
                if 'length_distribution' in da:
                    ld = da['length_distribution']
                    logger.info(f"   é•¿åº¦åˆ†å¸ƒ: {ld.get('mean_length', 0):.1f} Â± {ld.get('std_length', 0):.1f}")
                    logger.info(f"   é•¿åº¦èŒƒå›´: {ld.get('min_length', 0)}-{ld.get('max_length', 0)}")
                
                gini = da.get('amino_acid_gini_coefficient', 0)
                logger.info(f"   æ°¨åŸºé…¸åˆ†å¸ƒå‡åŒ€æ€§ (Gini): {gini:.4f} (0=å‡åŒ€, 1=ä¸å‡åŒ€)")
            
            # 6. åŸºæœ¬æœ‰æ•ˆæ€§
            if 'validity' in results:
                v = results['validity']
                logger.info(f"âœ… åºåˆ—æœ‰æ•ˆæ€§:")
                logger.info(f"   æœ‰æ•ˆç‡: {v.get('validity_rate', 0):.4f}")
                logger.info(f"   æœ‰æ•ˆåºåˆ—: {v.get('valid_sequences', 0)}")
                logger.info(f"   æ— æ•ˆåºåˆ—: {v.get('invalid_sequences', 0)}")
            
            # 7. pLDDTåˆ†æ•°ï¼ˆè®ºæ–‡æŒ‡æ ‡ï¼‰
            if 'plddt_scores' in results:
                plddt = results['plddt_scores']
                logger.info(f"ğŸ§¬ pLDDTåˆ†æ•° (ç»“æ„ç½®ä¿¡åº¦):")
                logger.info(f"   å¹³å‡pLDDT: {plddt.get('mean_plddt', 0):.4f} Â± {plddt.get('std_plddt', 0):.4f}")
                logger.info(f"   æœ‰æ•ˆåºåˆ—: {plddt.get('valid_sequences', 0)}")
            
            # 8. ç†åŒ–æ€§è´¨ï¼ˆè®ºæ–‡æŒ‡æ ‡ï¼‰
            if 'physicochemical_properties' in results:
                props = results['physicochemical_properties']
                logger.info(f"âš™ï¸ ç†åŒ–æ€§è´¨:")
                
                if 'charge' in props:
                    charge = props['charge']
                    logger.info(f"   ç”µè· (pH=7.4): {charge.get('mean_charge', 0):.4f} Â± {charge.get('std_charge', 0):.4f}")
                
                if 'isoelectric_point' in props:
                    iep = props['isoelectric_point']
                    logger.info(f"   ç­‰ç”µç‚¹: {iep.get('mean_isoelectric_point', 0):.4f} Â± {iep.get('std_isoelectric_point', 0):.4f}")
                
                if 'hydrophobicity' in props:
                    hydro = props['hydrophobicity']
                    logger.info(f"   ç–æ°´æ€§ (Eisenberg): {hydro.get('mean_hydrophobicity', 0):.4f} Â± {hydro.get('std_hydrophobicity', 0):.4f}")
                
                if 'aromaticity' in props:
                    aroma = props['aromaticity']
                    logger.info(f"   èŠ³é¦™æ€§: {aroma.get('mean_aromaticity', 0):.4f} Â± {aroma.get('std_aromaticity', 0):.4f}")
            
            # 9. å¤–éƒ¨åˆ†ç±»å™¨æ´»æ€§ï¼ˆè®ºæ–‡æŒ‡æ ‡ï¼‰
            if 'external_classifier_activity' in results:
                activity = results['external_classifier_activity']
                logger.info(f"ğŸ¯ å¤–éƒ¨åˆ†ç±»å™¨æ´»æ€§:")
                logger.info(f"   é¢„æµ‹æ´»æ€§æ¯”ä¾‹: {activity.get('predicted_active_ratio', 0):.4f}")
                logger.info(f"   é¢„æµ‹æ´»æ€§åºåˆ—: {activity.get('predicted_active', 0)}")
                logger.info(f"   é¢„æµ‹éæ´»æ€§åºåˆ—: {activity.get('predicted_inactive', 0)}")
                logger.info(f"   åˆ†ç±»å™¨ç±»å‹: {activity.get('classifier_type', 'unknown')}")
            
            # 10. æ€»ç»“ç»Ÿè®¡
            if 'summary' in results:
                s = results['summary']
                logger.info(f"ğŸ“‹ æ€»ç»“:")
                logger.info(f"   ç”ŸæˆæˆåŠŸç‡: {s.get('generation_success_rate', 0):.4f}")
                logger.info(f"   è‚½ç±»å‹: {s.get('peptide_type', 'unknown')}")
        
        logger.info(f"\nğŸ“ è¯¦ç»†ç»“æœä¿å­˜åˆ°: {results_file}")
        
        # æ·»åŠ å…³é”®æŒ‡æ ‡æ±‡æ€»è¡¨æ ¼
        logger.info("\n" + "="*80)
        logger.info("ğŸ“ˆ å…³é”®æŒ‡æ ‡æ±‡æ€»è¡¨æ ¼ (Key Metrics Summary)")
        logger.info("="*80)
        logger.info("æŒ‡æ ‡è¯´æ˜: â†“=è¶Šä½è¶Šå¥½, â†‘=è¶Šé«˜è¶Šå¥½")
        logger.info("-"*80)
        logger.info(f"{'è‚½ç±»å‹':<15} {'Perplexityâ†“':<12} {'pLDDTâ†‘':<10} {'Instabilityâ†“':<12} {'Similarityâ†“':<12} {'Activityâ†‘':<10}")
        logger.info("-"*80)
        
        for peptide_type, results in all_results.items():
            # æå–å…³é”®æŒ‡æ ‡ï¼Œä½¿ç”¨æ›´å‡†ç¡®çš„é”®å
            perplexity = results.get('pseudo_perplexity', {}).get('mean_pseudo_perplexity', 0.0)
            if perplexity == 0.0:  # å¤‡ç”¨é”®å
                perplexity = results.get('pseudo_perplexity', {}).get('mean_perplexity', 0.0)
                if perplexity == 0.0:
                    perplexity = results.get('pseudo_perplexity', {}).get('mean', 0.0)
            
            plddt = results.get('plddt_scores', {}).get('mean_plddt', 0.0)
            instability = results.get('instability_index', {}).get('mean_instability', 0.0)
            if instability == 0.0:  # å¤‡ç”¨é”®å
                instability = results.get('instability_index', {}).get('mean', 0.0)
                if instability == 0.0:  # å†æ¬¡å¤‡ç”¨
                    instability_data = results.get('instability_index', {})
                    if isinstance(instability_data, dict) and 'mean_instability_index' in instability_data:
                        instability = instability_data['mean_instability_index']
            if instability == 0.0:  # å¤‡ç”¨é”®å
                instability = results.get('instability_index', {}).get('mean', 0.0)
            
            # ä¿®å¤ç›¸ä¼¼æ€§å¾—åˆ†çš„é”®å
            similarity = results.get('blosum62_similarity', {}).get('mean_similarity_score', 0.0)
            if similarity == 0.0:  # å¤‡ç”¨é”®å
                similarity = results.get('similarity_scores', {}).get('mean_similarity', 0.0)
            
            activity = results.get('external_classifier_activity', {}).get('predicted_active_ratio', 0.0)
            
            # è°ƒè¯•è¾“å‡º
            logger.debug(f"å…³é”®æŒ‡æ ‡ - {peptide_type}: perplexity={perplexity}, plddt={plddt}, instability={instability}, similarity={similarity}, activity={activity}")
            
            logger.info(f"{peptide_type:<15} {perplexity:<12.2f} {plddt:<10.2f} {instability:<12.2f} {similarity:<12.4f} {activity:<10.3f}")
        
        logger.info("-"*80)
        logger.info("ğŸ“Š æŒ‡æ ‡è§£é‡Š:")
        logger.info("  â€¢ Perplexityâ†“: ä¼ªå›°æƒ‘åº¦ï¼Œè¶Šä½è¡¨ç¤ºåºåˆ—è¶Šç¬¦åˆè›‹ç™½è´¨è¯­è¨€æ¨¡å‹é¢„æœŸ")
        logger.info("  â€¢ pLDDTâ†‘: ç»“æ„ç½®ä¿¡åº¦ï¼Œè¶Šé«˜è¡¨ç¤ºé¢„æµ‹çš„3Dç»“æ„è¶Šå¯é ")
        logger.info("  â€¢ Instabilityâ†“: ä¸ç¨³å®šæ€§æŒ‡æ•°ï¼Œè¶Šä½è¡¨ç¤ºè›‹ç™½è´¨è¶Šç¨³å®š")
        logger.info("  â€¢ Similarityâ†“: ä¸è®­ç»ƒé›†ç›¸ä¼¼åº¦ï¼Œè¶Šä½è¡¨ç¤ºç”Ÿæˆåºåˆ—è¶Šæ–°é¢–")
        logger.info("  â€¢ Activityâ†‘: é¢„æµ‹æ´»æ€§æ¯”ä¾‹ï¼Œè¶Šé«˜è¡¨ç¤ºåŠŸèƒ½æ€§åºåˆ—è¶Šå¤š")
        logger.info("="*80)
        
        logger.info("ğŸ‰ ä¸“ä¸šç”Ÿç‰©å­¦è¯„ä¼°å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆå’ŒéªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")


class PeptideEvaluator:
    """å¤šè‚½ç”Ÿæˆè¯„ä¼°å™¨ - åŒ…å«ä¸“ä¸šç”Ÿç‰©å­¦è¯„ä¼°æŒ‡æ ‡"""
    
    def __init__(self, model, config, device, esmfold_wrapper=None):
        self.model = model
        self.config = config
        self.device = device
        self.esmfold_wrapper = esmfold_wrapper
        
        # æ°¨åŸºé…¸å­—æ¯è¡¨
        self.amino_acids = 'ACDEFGHIKLMNPQRSTVWY'
        
        # è‚½ç±»å‹æ˜ å°„
        self.peptide_type_map = {
            'antimicrobial': 0,
            'antifungal': 1,
            'antiviral': 2
        }
        
        # åˆå§‹åŒ–ESM2æ¨¡å‹ç”¨äºä¼ªå›°æƒ‘åº¦è®¡ç®—
        self.esm_tokenizer = None
        self.esm_model = None
        self._init_esm_model()
        
        # åˆå§‹åŒ–BLOSUM62æ¯”å¯¹å™¨
        self.aligner = None
        self._init_aligner()
    
    def _init_esm_model(self):
        """åˆå§‹åŒ–ESM2æ¨¡å‹ç”¨äºä¼ªå›°æƒ‘åº¦è®¡ç®—"""
        try:
            logger.info("ğŸ”¬ åˆå§‹åŒ–ESM2æ¨¡å‹ç”¨äºä¼ªå›°æƒ‘åº¦è®¡ç®—...")
            from transformers import EsmTokenizer, EsmModel
            self.esm_tokenizer = EsmTokenizer.from_pretrained('facebook/esm2_t6_8M_UR50D')
            self.esm_model = EsmModel.from_pretrained('facebook/esm2_t6_8M_UR50D').to(self.device)
            self.esm_model.eval()
            logger.info("âœ… ESM2æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ ESM2æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.esm_tokenizer = None
            self.esm_model = None
    
    def _init_aligner(self):
        """åˆå§‹åŒ–BLOSUM62æ¯”å¯¹å™¨"""
        try:
            self.aligner = Align.PairwiseAligner()
            self.aligner.substitution_matrix = Align.substitution_matrices.load("BLOSUM62")
            self.aligner.open_gap_score = -10
            self.aligner.extend_gap_score = -0.5
            logger.info("âœ… BLOSUM62æ¯”å¯¹å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ BLOSUM62æ¯”å¯¹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.aligner = None
    
    def generate_sequences(self, peptide_type='antimicrobial', sample_num=100, max_length=50):
        """ç”Ÿæˆå¤šè‚½åºåˆ—"""
        logger.info(f"ğŸ§¬ ç”Ÿæˆ {sample_num} æ¡ {peptide_type} åºåˆ—...")
        
        sequences = []
        batch_size = min(16, sample_num)  # å°æ‰¹é‡ç”Ÿæˆé¿å…å†…å­˜é—®é¢˜
        
        with torch.no_grad():
            for i in tqdm(range(0, sample_num, batch_size), desc="Generating"):
                current_batch_size = min(batch_size, sample_num - i)
                
                try:
                    # ç”Ÿæˆéšæœºé•¿åº¦
                    lengths = torch.randint(10, max_length, (current_batch_size,))
                    
                    # åˆ›å»ºæ¡ä»¶
                    conditions = None
                    if peptide_type in self.peptide_type_map:
                        type_id = self.peptide_type_map[peptide_type]
                        conditions = {
                            'peptide_type': torch.tensor([type_id] * current_batch_size, device=self.device)
                        }
                    
                    # ç”Ÿæˆåºåˆ—
                    for j in range(current_batch_size):
                        # è·å–å½“å‰åºåˆ—çš„é•¿åº¦
                        seq_length = lengths[j].item()
                        noise_shape = (1, seq_length + 2, 320)  # +2 for CLS/SEP tokens
                        
                        # ç®€åŒ–çš„ç”Ÿæˆè¿‡ç¨‹ - å®é™…åº”è¯¥ä½¿ç”¨DDPMé‡‡æ ·
                        noise = torch.randn(noise_shape, device=self.device)
                        
                        # åˆ›å»ºattention mask
                        attention_mask = torch.ones(1, seq_length + 2, device=self.device)
                        
                        # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆ
                        timesteps = torch.randint(0, self.config.diffusion.num_timesteps, (1,), device=self.device)
                        
                        # ä¸ºå•ä¸ªåºåˆ—åˆ›å»ºæ¡ä»¶
                        single_conditions = None
                        if conditions is not None:
                            single_conditions = {
                                'peptide_type': conditions['peptide_type'][:1]  # åªå–ç¬¬ä¸€ä¸ª
                            }
                        
                        # å¯¹äºç”Ÿæˆï¼Œæˆ‘ä»¬éœ€è¦ç›´æ¥ä½¿ç”¨denoiserï¼Œè€Œä¸æ˜¯å®Œæ•´çš„forwardæ–¹æ³•
                        # å› ä¸ºæˆ‘ä»¬ä»å™ªå£°åµŒå…¥å¼€å§‹ï¼Œè€Œä¸æ˜¯ä»token IDså¼€å§‹
                        denoised_embeddings, cross_attention_weights = self.model.denoiser(
                            noisy_embeddings=noise,
                            timesteps=timesteps,
                            attention_mask=attention_mask,
                            structure_features=None,  # æš‚æ—¶ä¸ä½¿ç”¨ç»“æ„ç‰¹å¾
                            conditions=single_conditions
                        )
                        
                        outputs = {
                            'denoised_embeddings': denoised_embeddings,
                            'cross_attention_weights': cross_attention_weights
                        }
                        
                        # è§£ç åºåˆ— (ç®€åŒ–ç‰ˆæœ¬)
                        sequence = self._decode_sequence(outputs, seq_length)
                        if sequence and len(sequence) >= 5:  # æœ€å°é•¿åº¦æ£€æŸ¥
                            sequences.append(sequence)
                
                except Exception as e:
                    logger.warning(f"ç”Ÿæˆæ‰¹æ¬¡ {i} å¤±è´¥: {e}")
                    continue
        
        logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(sequences)} æ¡åºåˆ—")
        return sequences
    
    def _decode_sequence(self, outputs, target_length):
        """è§£ç æ¨¡å‹è¾“å‡ºä¸ºæ°¨åŸºé…¸åºåˆ—"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„è§£ç è¿‡ç¨‹
        # å®é™…å®ç°åº”è¯¥ä½¿ç”¨è®­ç»ƒå¥½çš„è§£ç å™¨
        
        try:
            # éšæœºç”Ÿæˆåºåˆ—ä½œä¸ºå ä½ç¬¦
            # å®é™…åº”è¯¥ä»æ¨¡å‹è¾“å‡ºä¸­è§£ç 
            import random
            length = min(target_length, 50)
            sequence = ''.join(random.choices(self.amino_acids, k=length))
            return sequence
        except:
            return None
    
    def evaluate_diversity(self, sequences):
        """è®¡ç®—åºåˆ—å¤šæ ·æ€§"""
        if len(sequences) < 2:
            return {'uniqueness': 0.0, 'entropy': 0.0}
        
        # å”¯ä¸€æ€§
        unique_sequences = set(sequences)
        uniqueness = len(unique_sequences) / len(sequences)
        
        # ä¿¡æ¯ç†µ
        all_chars = ''.join(sequences)
        char_counts = Counter(all_chars)
        total_chars = len(all_chars)
        
        entropy = 0.0
        for count in char_counts.values():
            prob = count / total_chars
            if prob > 0:
                entropy -= prob * math.log2(prob)
        
        return {
            'uniqueness': uniqueness,
            'entropy': entropy,
            'total_sequences': len(sequences),
            'unique_sequences': len(unique_sequences)
        }
    
    def evaluate_length_distribution(self, sequences):
        """è¯„ä¼°é•¿åº¦åˆ†å¸ƒ"""
        lengths = [len(seq) for seq in sequences]
        
        return {
            'mean_length': mean(lengths),
            'std_length': stdev(lengths) if len(lengths) > 1 else 0.0,
            'min_length': min(lengths),
            'max_length': max(lengths)
        }
    
    def evaluate_amino_acid_composition(self, sequences):
        """è¯„ä¼°æ°¨åŸºé…¸ç»„æˆ"""
        all_chars = ''.join(sequences)
        char_counts = Counter(all_chars)
        total_chars = len(all_chars)
        
        composition = {}
        for aa in self.amino_acids:
            composition[f'freq_{aa}'] = char_counts.get(aa, 0) / total_chars
        
        return composition
    
    def evaluate_validity(self, sequences):
        """è¯„ä¼°åºåˆ—æœ‰æ•ˆæ€§"""
        valid_sequences = []
        invalid_count = 0
        
        for seq in sequences:
            # æ£€æŸ¥æ˜¯å¦åªåŒ…å«æ ‡å‡†æ°¨åŸºé…¸
            if all(aa in self.amino_acids for aa in seq):
                valid_sequences.append(seq)
            else:
                invalid_count += 1
        
        validity_rate = len(valid_sequences) / len(sequences) if sequences else 0.0
        
        return {
            'validity_rate': validity_rate,
            'valid_sequences': len(valid_sequences),
            'invalid_sequences': invalid_count
        }
    
    def evaluate_pseudo_perplexity(self, sequences):
        """
        è®¡ç®—ä¼ªå›°æƒ‘åº¦ï¼ˆPseudo-Perplexityï¼‰
        
        åŸç†ï¼šé€ä¸ªæ©ç åºåˆ—ä¸­çš„æ°¨åŸºé…¸ï¼Œç”¨ESM2æ¨¡å‹é¢„æµ‹è¢«æ©ç çš„æ°¨åŸºé…¸ï¼Œè®¡ç®—é¢„æµ‹æŸå¤±çš„æŒ‡æ•°
        """
        if self.esm_tokenizer is None or self.esm_model is None:
            logger.warning("âš ï¸ ESM2æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè·³è¿‡ä¼ªå›°æƒ‘åº¦è®¡ç®—")
            return {'mean_pseudo_perplexity': 0.0, 'std_pseudo_perplexity': 0.0}
        
        logger.info("ğŸ§® è®¡ç®—ä¼ªå›°æƒ‘åº¦...")
        pseudo_perplexities = []
        
        with torch.no_grad():
            for seq in tqdm(sequences, desc="Computing pseudo-perplexity"):
                try:
                    # å¯¹åºåˆ—è¿›è¡Œç¼–ç 
                    inputs = self.esm_tokenizer(seq, return_tensors='pt', padding=True, truncation=True)
                    input_ids = inputs['input_ids'].to(self.device)
                    attention_mask = inputs['attention_mask'].to(self.device)
                    
                    seq_len = input_ids.size(1)
                    total_loss = 0.0
                    valid_positions = 0
                    
                    # é€ä¸ªä½ç½®è¿›è¡Œæ©ç é¢„æµ‹
                    for pos in range(1, seq_len - 1):  # è·³è¿‡CLSå’ŒSEP token
                        if attention_mask[0, pos] == 1:  # åªå¤„ç†æœ‰æ•ˆä½ç½®
                            # åˆ›å»ºæ©ç ç‰ˆæœ¬
                            masked_input = input_ids.clone()
                            original_token = masked_input[0, pos].item()
                            masked_input[0, pos] = self.esm_tokenizer.mask_token_id
                            
                            # é¢„æµ‹
                            outputs = self.esm_model(masked_input, attention_mask=attention_mask)
                            logits = outputs.last_hidden_state[0, pos]  # è·å–æ©ç ä½ç½®çš„logits
                            
                            # è®¡ç®—äº¤å‰ç†µæŸå¤±
                            loss = F.cross_entropy(logits.unsqueeze(0), torch.tensor([original_token], device=self.device))
                            total_loss += loss.item()
                            valid_positions += 1
                    
                    if valid_positions > 0:
                        avg_loss = total_loss / valid_positions
                        pseudo_perplexity = math.exp(avg_loss)
                        pseudo_perplexities.append(pseudo_perplexity)
                
                except Exception as e:
                    logger.warning(f"è®¡ç®—åºåˆ—ä¼ªå›°æƒ‘åº¦å¤±è´¥: {e}")
                    continue
        
        if pseudo_perplexities:
            return {
                'mean_pseudo_perplexity': mean(pseudo_perplexities),
                'std_pseudo_perplexity': stdev(pseudo_perplexities) if len(pseudo_perplexities) > 1 else 0.0,
                'valid_sequences': len(pseudo_perplexities)
            }
        else:
            return {'mean_pseudo_perplexity': 0.0, 'std_pseudo_perplexity': 0.0, 'valid_sequences': 0}
    
    def evaluate_shannon_entropy(self, sequences):
        """
        è®¡ç®—ä¿¡æ¯ç†µï¼ˆShannon Entropyï¼‰
        
        åŸç†ï¼šè®¡ç®—åºåˆ—ä¸­æ°¨åŸºé…¸åˆ†å¸ƒçš„Shannonç†µï¼šH = -Î£ p(aa) * log2(p(aa))
        """
        logger.info("ğŸ“Š è®¡ç®—Shannonä¿¡æ¯ç†µ...")
        
        # è®¡ç®—æ¯ä¸ªåºåˆ—çš„ç†µ
        sequence_entropies = []
        for seq in sequences:
            aa_counts = Counter(seq)
            total_aa = len(seq)
            
            entropy = 0.0
            for count in aa_counts.values():
                prob = count / total_aa
                if prob > 0:
                    entropy -= prob * math.log2(prob)
            
            sequence_entropies.append(entropy)
        
        # è®¡ç®—æ•´ä½“æ°¨åŸºé…¸åˆ†å¸ƒçš„ç†µ
        all_aa = ''.join(sequences)
        overall_aa_counts = Counter(all_aa)
        total_aa = len(all_aa)
        
        overall_entropy = 0.0
        for count in overall_aa_counts.values():
            prob = count / total_aa
            if prob > 0:
                overall_entropy -= prob * math.log2(prob)
        
        return {
            'mean_sequence_entropy': mean(sequence_entropies) if sequence_entropies else 0.0,
            'std_sequence_entropy': stdev(sequence_entropies) if len(sequence_entropies) > 1 else 0.0,
            'overall_entropy': overall_entropy,
            'max_possible_entropy': math.log2(20)  # 20ç§æ°¨åŸºé…¸çš„æœ€å¤§ç†µ
        }
    
    def evaluate_instability_index(self, sequences):
        """
        è®¡ç®—ä¸ç¨³å®šæ€§æŒ‡æ•°ï¼ˆInstability Indexï¼‰
        
        åŸç†ï¼šåŸºäºGuruprasadç­‰äººæå‡ºçš„ç®—æ³•ï¼Œè€ƒè™‘ç›¸é‚»æ°¨åŸºé…¸å¯¹çš„ç¨³å®šæ€§è´¡çŒ®
        """
        if not MODLAMP_AVAILABLE:
            logger.warning("âš ï¸ modlampæœªå®‰è£…ï¼Œè·³è¿‡ä¸ç¨³å®šæ€§æŒ‡æ•°è®¡ç®—")
            return {'mean_instability_index': 0.0, 'std_instability_index': 0.0}
        
        logger.info("ğŸ§ª è®¡ç®—ä¸ç¨³å®šæ€§æŒ‡æ•°...")
        
        # åˆ›å»ºä¸´æ—¶FASTAæ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode='w', suffix='.fasta', delete=False) as tmp_file:
            for i, seq in enumerate(sequences):
                tmp_file.write(f">seq_{i}\n{seq}\n")
            tmp_file_path = tmp_file.name
        
        try:
            # ä½¿ç”¨modlampè®¡ç®—ä¸ç¨³å®šæ€§æŒ‡æ•°
            desc = GlobalDescriptor(tmp_file_path)
            desc.instability_index()
            instability_scores = desc.descriptor.flatten()
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(tmp_file_path)
            
            return {
                'mean_instability_index': mean(instability_scores),
                'std_instability_index': stdev(instability_scores) if len(instability_scores) > 1 else 0.0,
                'stable_peptides': sum(1 for score in instability_scores if score <= 40),
                'unstable_peptides': sum(1 for score in instability_scores if score > 40)
            }
        
        except Exception as e:
            logger.warning(f"è®¡ç®—ä¸ç¨³å®šæ€§æŒ‡æ•°å¤±è´¥: {e}")
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(tmp_file_path):
                os.unlink(tmp_file_path)
            return {'mean_instability_index': 0.0, 'std_instability_index': 0.0}
    
    def evaluate_similarity_to_training(self, sequences, reference_sequences=None):
        """
        è®¡ç®—ä¸è®­ç»ƒé›†çš„ç›¸ä¼¼æ€§å¾—åˆ†ï¼ˆBLOSUM62æ¯”å¯¹ï¼‰
        
        åŸç†ï¼šä½¿ç”¨BLOSUM62æ›¿æ¢çŸ©é˜µå¯¹ç”Ÿæˆåºåˆ—ä¸è®­ç»ƒé›†ä¸­çœŸå®åºåˆ—è¿›è¡Œå…¨å±€æ¯”å¯¹
        """
        if self.aligner is None:
            logger.warning("âš ï¸ BLOSUM62æ¯”å¯¹å™¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡ç›¸ä¼¼æ€§è®¡ç®—")
            return {'mean_similarity_score': 0.0, 'std_similarity_score': 0.0}
        
        if reference_sequences is None:
            logger.warning("âš ï¸ æœªæä¾›å‚è€ƒåºåˆ—ï¼Œè·³è¿‡ç›¸ä¼¼æ€§è®¡ç®—")
            return {'mean_similarity_score': 0.0, 'std_similarity_score': 0.0}
        
        logger.info("ğŸ” è®¡ç®—BLOSUM62ç›¸ä¼¼æ€§å¾—åˆ†...")
        
        similarity_scores = []
        
        for gen_seq in tqdm(sequences, desc="Computing similarity scores"):
            seq_scores = []
            
            # ä¸å‚è€ƒåºåˆ—é›†åˆä¸­çš„æ¯ä¸ªåºåˆ—è¿›è¡Œæ¯”å¯¹
            for ref_seq in reference_sequences[:100]:  # é™åˆ¶å‚è€ƒåºåˆ—æ•°é‡ä»¥æé«˜æ•ˆç‡
                try:
                    alignments = self.aligner.align(gen_seq, ref_seq)
                    if alignments:
                        score = alignments.score
                        # æ ‡å‡†åŒ–å¾—åˆ†ï¼ˆé™¤ä»¥è¾ƒé•¿åºåˆ—çš„é•¿åº¦ï¼‰
                        normalized_score = score / max(len(gen_seq), len(ref_seq))
                        seq_scores.append(normalized_score)
                except Exception as e:
                    continue
            
            if seq_scores:
                # å–æœ€é«˜ç›¸ä¼¼æ€§å¾—åˆ†
                max_similarity = max(seq_scores)
                similarity_scores.append(max_similarity)
        
        if similarity_scores:
            return {
                'mean_similarity_score': mean(similarity_scores),
                'std_similarity_score': stdev(similarity_scores) if len(similarity_scores) > 1 else 0.0,
                'max_similarity_score': max(similarity_scores),
                'min_similarity_score': min(similarity_scores)
            }
        else:
            return {'mean_similarity_score': 0.0, 'std_similarity_score': 0.0}
    
    def evaluate_diversity_metrics(self, sequences):
        """
        å¤šæ ·æ€§è¯„ä¼°
        
        åŒ…æ‹¬ï¼šå»é‡æ¯”ä¾‹ã€é•¿åº¦åˆ†å¸ƒã€æ°¨åŸºé…¸é¢‘ç‡åˆ†æ
        """
        logger.info("ğŸ“ˆ è®¡ç®—å¤šæ ·æ€§æŒ‡æ ‡...")
        
        # å»é‡æ¯”ä¾‹
        unique_sequences = set(sequences)
        uniqueness_ratio = len(unique_sequences) / len(sequences) if sequences else 0.0
        
        # é•¿åº¦åˆ†å¸ƒ
        lengths = [len(seq) for seq in sequences]
        length_stats = {
            'mean_length': mean(lengths) if lengths else 0.0,
            'std_length': stdev(lengths) if len(lengths) > 1 else 0.0,
            'min_length': min(lengths) if lengths else 0,
            'max_length': max(lengths) if lengths else 0,
            'length_range': max(lengths) - min(lengths) if lengths else 0
        }
        
        # æ°¨åŸºé…¸é¢‘ç‡åˆ†æ
        all_aa = ''.join(sequences)
        aa_counts = Counter(all_aa)
        total_aa = len(all_aa)
        
        aa_frequencies = {}
        for aa in self.amino_acids:
            aa_frequencies[f'freq_{aa}'] = aa_counts.get(aa, 0) / total_aa if total_aa > 0 else 0.0
        
        # è®¡ç®—æ°¨åŸºé…¸ä½¿ç”¨çš„å‡åŒ€æ€§ï¼ˆåŸºå°¼ç³»æ•°ï¼‰
        frequencies = [aa_counts.get(aa, 0) / total_aa for aa in self.amino_acids if total_aa > 0]
        if frequencies:
            # ç®€åŒ–çš„åŸºå°¼ç³»æ•°è®¡ç®—
            frequencies.sort()
            n = len(frequencies)
            gini = sum((2 * i - n - 1) * freq for i, freq in enumerate(frequencies, 1)) / (n * sum(frequencies))
        else:
            gini = 0.0
        
        return {
            'uniqueness_ratio': uniqueness_ratio,
            'total_sequences': len(sequences),
            'unique_sequences': len(unique_sequences),
            'duplicate_sequences': len(sequences) - len(unique_sequences),
            'length_distribution': length_stats,
            'amino_acid_frequencies': aa_frequencies,
            'amino_acid_gini_coefficient': gini  # 0è¡¨ç¤ºå®Œå…¨å‡åŒ€ï¼Œ1è¡¨ç¤ºå®Œå…¨ä¸å‡åŒ€
        }
    
    def evaluate_plddt_scores(self, sequences):
        """
        è®¡ç®—pLDDTåˆ†æ•° (Predicted Local-Distance Difference Test)
        
        ä½¿ç”¨ESMFoldé¢„æµ‹3Dç»“æ„å¹¶è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°
        """
        if self.esmfold_wrapper is None or not hasattr(self.esmfold_wrapper, 'fold_sequence'):
            logger.warning("âš ï¸ ESMFoldæœªå¯ç”¨ï¼Œè·³è¿‡pLDDTè®¡ç®—")
            return {'mean_plddt': 0.0, 'std_plddt': 0.0, 'valid_sequences': 0}
        
        logger.info("ğŸ§¬ è®¡ç®—pLDDTåˆ†æ•°...")
        plddt_scores = []
        
        for seq in tqdm(sequences, desc="Computing pLDDT scores"):
            try:
                # ä½¿ç”¨ESMFoldé¢„æµ‹ç»“æ„
                result = self.esmfold_wrapper.fold_sequence(seq)
                
                if result and 'plddt' in result:
                    # å–æ‰€æœ‰æ®‹åŸºpLDDTåˆ†æ•°çš„å¹³å‡å€¼
                    mean_plddt = float(result['plddt'].mean())
                    plddt_scores.append(mean_plddt)
                
            except Exception as e:
                logger.warning(f"è®¡ç®—åºåˆ—pLDDTå¤±è´¥: {e}")
                continue
        
        if plddt_scores:
            return {
                'mean_plddt': mean(plddt_scores),
                'std_plddt': stdev(plddt_scores) if len(plddt_scores) > 1 else 0.0,
                'valid_sequences': len(plddt_scores)
            }
        else:
            return {'mean_plddt': 0.0, 'std_plddt': 0.0, 'valid_sequences': 0}
    
    def _compute_simple_physicochemical_properties(self, sequences):
        """
        ç®€åŒ–çš„ç†åŒ–æ€§è´¨è®¡ç®—ï¼ˆä¸ä¾èµ–modlampï¼‰
        """
        # æ°¨åŸºé…¸å±æ€§è¡¨
        aa_charge = {'R': 1, 'K': 1, 'H': 0.5, 'D': -1, 'E': -1}
        aa_hydrophobicity = {
            'A': 0.62, 'R': -2.53, 'N': -0.78, 'D': -0.90, 'C': 0.29,
            'Q': -0.85, 'E': -0.74, 'G': 0.48, 'H': -0.40, 'I': 1.38,
            'L': 1.06, 'K': -1.50, 'M': 0.64, 'F': 1.19, 'P': 0.12,
            'S': -0.18, 'T': -0.05, 'W': 0.81, 'Y': 0.26, 'V': 1.08
        }
        aromatic_aa = set('FWY')
        
        charges, hydrophobicities, isoelectric_points, aromaticities = [], [], [], []
        
        for seq in sequences:
            # å‡€ç”µè·
            charge = sum(aa_charge.get(aa, 0) for aa in seq)
            charges.append(charge)
            
            # å¹³å‡ç–æ°´æ€§
            hydro = [aa_hydrophobicity.get(aa, 0) for aa in seq]
            avg_hydro = mean(hydro) if hydro else 0
            hydrophobicities.append(avg_hydro)
            
            # ç®€åŒ–ç­‰ç”µç‚¹ä¼°ç®—
            basic_count = sum(1 for aa in seq if aa in 'RKH')
            acidic_count = sum(1 for aa in seq if aa in 'DE')
            if basic_count > acidic_count:
                iep = 8.5 + basic_count * 0.5
            elif acidic_count > basic_count:
                iep = 6.0 - acidic_count * 0.3
            else:
                iep = 7.0
            isoelectric_points.append(max(3.0, min(11.0, iep)))
            
            # èŠ³é¦™æ€§
            aromatic_ratio = sum(1 for aa in seq if aa in aromatic_aa) / len(seq)
            aromaticities.append(aromatic_ratio)
        
        return {
            'charge': {
                'mean_charge': mean(charges),
                'std_charge': stdev(charges) if len(charges) > 1 else 0.0
            },
            'isoelectric_point': {
                'mean_isoelectric_point': mean(isoelectric_points),
                'std_isoelectric_point': stdev(isoelectric_points) if len(isoelectric_points) > 1 else 0.0
            },
            'hydrophobicity': {
                'mean_hydrophobicity': mean(hydrophobicities),
                'std_hydrophobicity': stdev(hydrophobicities) if len(hydrophobicities) > 1 else 0.0
            },
            'aromaticity': {
                'mean_aromaticity': mean(aromaticities),
                'std_aromaticity': stdev(aromaticities) if len(aromaticities) > 1 else 0.0
            }
        }

    def evaluate_physicochemical_properties(self, sequences):
        """
        è®¡ç®—ç†åŒ–æ€§è´¨
        
        åŒ…æ‹¬ï¼šç”µè·ã€ç­‰ç”µç‚¹ã€ç–æ°´æ€§ã€èŠ³é¦™æ€§
        """
        if not MODLAMP_AVAILABLE:
            logger.warning("âš ï¸ modlampæœªå®‰è£…ï¼Œä½¿ç”¨ç®€åŒ–çš„ç†åŒ–æ€§è´¨è®¡ç®—")
            return self._compute_simple_physicochemical_properties(sequences)
        
        logger.info("âš™ï¸ è®¡ç®—ç†åŒ–æ€§è´¨...")
        
        # åˆ›å»ºä¸´æ—¶FASTAæ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode='w', suffix='.fasta', delete=False) as tmp_file:
            for i, seq in enumerate(sequences):
                tmp_file.write(f">seq_{i}\n{seq}\n")
            tmp_file_path = tmp_file.name
        
        try:
            desc = GlobalDescriptor(tmp_file_path)
            
            # è®¡ç®—å„é¡¹ç†åŒ–æ€§è´¨
            properties = {}
            
            # 1. ç”µè· (pH=7.4, Bjellqvistæ–¹æ³•)
            try:
                desc.charge(ph=7.4, amide=True)  # amide=Trueä½¿ç”¨Bjellqvistæ–¹æ³•
                charges = desc.descriptor.flatten()
                properties['charge'] = {
                    'mean_charge': mean(charges),
                    'std_charge': stdev(charges) if len(charges) > 1 else 0.0
                }
            except:
                properties['charge'] = {'mean_charge': 0.0, 'std_charge': 0.0}
            
            # 2. ç­‰ç”µç‚¹
            try:
                desc.isoelectric_point(amide=True)  # ä½¿ç”¨Bjellqvistæ–¹æ³•
                ieps = desc.descriptor.flatten()
                properties['isoelectric_point'] = {
                    'mean_isoelectric_point': mean(ieps),
                    'std_isoelectric_point': stdev(ieps) if len(ieps) > 1 else 0.0
                }
            except:
                properties['isoelectric_point'] = {'mean_isoelectric_point': 0.0, 'std_isoelectric_point': 0.0}
            
            # 3. ç–æ°´æ€§ (Eisenberg scale, window=7)
            try:
                desc.hydrophobic_ratio(scale='eisenberg', window=7)
                hydrophobicities = desc.descriptor.flatten()
                properties['hydrophobicity'] = {
                    'mean_hydrophobicity': mean(hydrophobicities),
                    'std_hydrophobicity': stdev(hydrophobicities) if len(hydrophobicities) > 1 else 0.0
                }
            except:
                properties['hydrophobicity'] = {'mean_hydrophobicity': 0.0, 'std_hydrophobicity': 0.0}
            
            # 4. èŠ³é¦™æ€§ (Phe, Trp, Tyrå«é‡)
            try:
                desc.aromaticity()
                aromaticities = desc.descriptor.flatten()
                properties['aromaticity'] = {
                    'mean_aromaticity': mean(aromaticities),
                    'std_aromaticity': stdev(aromaticities) if len(aromaticities) > 1 else 0.0
                }
            except:
                properties['aromaticity'] = {'mean_aromaticity': 0.0, 'std_aromaticity': 0.0}
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(tmp_file_path)
            
            return properties
        
        except Exception as e:
            logger.warning(f"è®¡ç®—ç†åŒ–æ€§è´¨å¤±è´¥: {e}")
            if os.path.exists(tmp_file_path):
                os.unlink(tmp_file_path)
            return {
                'charge': {'mean_charge': 0.0, 'std_charge': 0.0},
                'isoelectric_point': {'mean_isoelectric_point': 0.0, 'std_isoelectric_point': 0.0},
                'hydrophobicity': {'mean_hydrophobicity': 0.0, 'std_hydrophobicity': 0.0},
                'aromaticity': {'mean_aromaticity': 0.0, 'std_aromaticity': 0.0}
            }
    
    def evaluate_external_classifier_activity(self, sequences, peptide_type):
        """
        ä½¿ç”¨å¤–éƒ¨åˆ†ç±»å™¨è¯„ä¼°æ´»æ€§
        """
        logger.info(f"ğŸ¯ è¯„ä¼° {peptide_type} æ´»æ€§ï¼ˆå¤–éƒ¨åˆ†ç±»å™¨ï¼‰...")
        
        try:
            # å¯¼å…¥ç®€å•åˆ†ç±»å™¨
            from structdiff.utils.external_classifiers import get_activity_classifier
            
            # è·å–åˆ†ç±»å™¨
            classifier = get_activity_classifier(peptide_type)
            
            # è¿›è¡Œé¢„æµ‹
            results = classifier.predict_activity(sequences)
            
            logger.info(f"âœ… å¤–éƒ¨åˆ†ç±»å™¨é¢„æµ‹å®Œæˆï¼Œæ´»æ€§æ¯”ä¾‹: {results['predicted_active_ratio']:.3f}")
            return results
            
        except Exception as e:
            logger.warning(f"âš ï¸ å¤–éƒ¨åˆ†ç±»å™¨è°ƒç”¨å¤±è´¥: {e}ï¼Œè¿”å›å ä½ç¬¦ç»“æœ")
            
            return {
                'predicted_active_ratio': 0.0,
                'total_sequences': len(sequences),
                'predicted_active': 0,
                'predicted_inactive': len(sequences),
                'classifier_type': f'{peptide_type}_external_classifier'
            }
    
    def comprehensive_evaluation(self, peptide_type='antimicrobial', sample_num=100, max_length=50, reference_sequences=None):
        """
        ç»¼åˆè¯„ä¼° - åŒ…å«æ‰€æœ‰ä¸“ä¸šç”Ÿç‰©å­¦æŒ‡æ ‡
        
        Args:
            peptide_type: è‚½ç±»å‹
            sample_num: ç”Ÿæˆæ ·æœ¬æ•°é‡
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            reference_sequences: å‚è€ƒåºåˆ—ï¼ˆç”¨äºç›¸ä¼¼æ€§è®¡ç®—ï¼‰
        """
        logger.info(f"ğŸ”¬ å¼€å§‹ {peptide_type} å¤šè‚½ç»¼åˆè¯„ä¼°...")
        
        # ç”Ÿæˆåºåˆ—
        sequences = self.generate_sequences(peptide_type, sample_num, max_length)
        
        if not sequences:
            logger.warning("âš ï¸ æœªç”Ÿæˆä»»ä½•æœ‰æ•ˆåºåˆ—")
            return {}, []
        
        logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(sequences)} æ¡åºåˆ—ï¼Œå¼€å§‹è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        results = {}
        
        # 1. ä¼ªå›°æƒ‘åº¦ï¼ˆPseudo-Perplexityï¼‰
        try:
            pseudo_perplexity_metrics = self.evaluate_pseudo_perplexity(sequences)
            results['pseudo_perplexity'] = pseudo_perplexity_metrics
        except Exception as e:
            logger.warning(f"ä¼ªå›°æƒ‘åº¦è®¡ç®—å¤±è´¥: {e}")
            results['pseudo_perplexity'] = {'mean_pseudo_perplexity': 0.0, 'std_pseudo_perplexity': 0.0}
        
        # 2. Shannonä¿¡æ¯ç†µ
        try:
            shannon_entropy_metrics = self.evaluate_shannon_entropy(sequences)
            results['shannon_entropy'] = shannon_entropy_metrics
        except Exception as e:
            logger.warning(f"Shannonç†µè®¡ç®—å¤±è´¥: {e}")
            results['shannon_entropy'] = {'mean_sequence_entropy': 0.0, 'overall_entropy': 0.0}
        
        # 3. ä¸ç¨³å®šæ€§æŒ‡æ•°
        try:
            instability_metrics = self.evaluate_instability_index(sequences)
            results['instability_index'] = instability_metrics
        except Exception as e:
            logger.warning(f"ä¸ç¨³å®šæ€§æŒ‡æ•°è®¡ç®—å¤±è´¥: {e}")
            results['instability_index'] = {'mean_instability_index': 0.0, 'std_instability_index': 0.0}
        
        # 4. BLOSUM62ç›¸ä¼¼æ€§å¾—åˆ†
        try:
            similarity_metrics = self.evaluate_similarity_to_training(sequences, reference_sequences)
            results['blosum62_similarity'] = similarity_metrics
        except Exception as e:
            logger.warning(f"BLOSUM62ç›¸ä¼¼æ€§è®¡ç®—å¤±è´¥: {e}")
            results['blosum62_similarity'] = {'mean_similarity_score': 0.0, 'std_similarity_score': 0.0}
        
        # 5. å¤šæ ·æ€§è¯„ä¼°
        try:
            diversity_metrics = self.evaluate_diversity_metrics(sequences)
            results['diversity_analysis'] = diversity_metrics
        except Exception as e:
            logger.warning(f"å¤šæ ·æ€§åˆ†æå¤±è´¥: {e}")
            results['diversity_analysis'] = {'uniqueness_ratio': 0.0}
        
        # 6. åŸºæœ¬æœ‰æ•ˆæ€§æ£€æŸ¥
        try:
            validity_metrics = self.evaluate_validity(sequences)
            results['validity'] = validity_metrics
        except Exception as e:
            logger.warning(f"æœ‰æ•ˆæ€§æ£€æŸ¥å¤±è´¥: {e}")
            results['validity'] = {'validity_rate': 0.0}
        
        # 7. pLDDTåˆ†æ•°ï¼ˆè®ºæ–‡ä¸­çš„æŒ‡æ ‡ï¼‰
        try:
            plddt_metrics = self.evaluate_plddt_scores(sequences)
            results['plddt_scores'] = plddt_metrics
        except Exception as e:
            logger.warning(f"pLDDTåˆ†æ•°è®¡ç®—å¤±è´¥: {e}")
            results['plddt_scores'] = {'mean_plddt': 0.0, 'std_plddt': 0.0}
        
        # 8. ç†åŒ–æ€§è´¨ï¼ˆè®ºæ–‡ä¸­çš„æŒ‡æ ‡ï¼‰
        try:
            physicochemical_metrics = self.evaluate_physicochemical_properties(sequences)
            results['physicochemical_properties'] = physicochemical_metrics
        except Exception as e:
            logger.warning(f"ç†åŒ–æ€§è´¨è®¡ç®—å¤±è´¥: {e}")
            results['physicochemical_properties'] = {
                'charge': {'mean_charge': 0.0, 'std_charge': 0.0},
                'isoelectric_point': {'mean_isoelectric_point': 0.0, 'std_isoelectric_point': 0.0},
                'hydrophobicity': {'mean_hydrophobicity': 0.0, 'std_hydrophobicity': 0.0},
                'aromaticity': {'mean_aromaticity': 0.0, 'std_aromaticity': 0.0}
            }
        
        # 9. å¤–éƒ¨åˆ†ç±»å™¨æ´»æ€§è¯„ä¼°ï¼ˆè®ºæ–‡ä¸­çš„æŒ‡æ ‡ï¼‰
        try:
            activity_metrics = self.evaluate_external_classifier_activity(sequences, peptide_type)
            results['external_classifier_activity'] = activity_metrics
        except Exception as e:
            logger.warning(f"å¤–éƒ¨åˆ†ç±»å™¨æ´»æ€§è¯„ä¼°å¤±è´¥: {e}")
            results['external_classifier_activity'] = {
                'predicted_active_ratio': 0.0,
                'total_sequences': len(sequences),
                'predicted_active': 0,
                'predicted_inactive': len(sequences)
            }
        
        # å»é‡å¹¶å‡†å¤‡æœ€ç»ˆç»Ÿè®¡
        unique_sequences = list(set(sequences))
        results['summary'] = {
            'total_generated': len(sequences),
            'unique_sequences': len(unique_sequences),
            'peptide_type': peptide_type,
            'generation_success_rate': len(sequences) / sample_num if sample_num > 0 else 0.0
        }
        
        logger.info(f"âœ… {peptide_type} ç»¼åˆè¯„ä¼°å®Œæˆ")
        return results, unique_sequences
    
    def save_sequences_to_fasta(self, sequences, output_file, peptide_type):
        """ä¿å­˜åºåˆ—åˆ°FASTAæ–‡ä»¶"""
        records = []
        for i, seq in enumerate(sequences):
            record = SeqRecord(
                Seq(seq),
                id=f"generated_{peptide_type}_{i+1}",
                description=f"Generated {peptide_type} peptide"
            )
            records.append(record)
        
        SeqIO.write(records, output_file, "fasta")
        logger.info(f"ğŸ’¾ åºåˆ—å·²ä¿å­˜åˆ°: {output_file}")


def main():
    args = parse_args()
    
    # åŠ è½½é…ç½®
    config = OmegaConf.load(args.config)
    
    # æµ‹è¯•è¿è¡Œé…ç½®
    if args.test_run:
        config.training.num_epochs = 3
        config.training.validate_every = 1
        config.training.save_every = 50
        logger.info("æµ‹è¯•è¿è¡Œæ¨¡å¼ï¼š3ä¸ªepochs")
    
    # Debugæ¨¡å¼é…ç½®
    if args.debug:
        config.training.save_every = max(config.training.save_every, 10)  # Debugæ¨¡å¼ä¸‹è‡³å°‘æ¯10ä¸ªepochä¿å­˜ä¸€æ¬¡
        logger.info("Debugæ¨¡å¼ï¼šè°ƒæ•´æ£€æŸ¥ç‚¹ä¿å­˜é¢‘ç‡ä»¥å¹³è¡¡é€Ÿåº¦å’Œå®‰å…¨æ€§")
    
    # è®¾ç½®ç¯å¢ƒ
    device = setup_environment(args, config)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(config.experiment.output_dir) / config.experiment.name
    output_dir.mkdir(parents=True, exist_ok=True)
    
    checkpoint_dir = output_dir / "checkpoints"
    log_dir = output_dir / "logs" 
    tensorboard_dir = output_dir / "tensorboard"
    
    checkpoint_dir.mkdir(exist_ok=True)
    log_dir.mkdir(exist_ok=True)
    tensorboard_dir.mkdir(exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—
    log_file = log_dir / f"train_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    setup_logger(str(log_file))
    
    # åˆå§‹åŒ–Weights & Biases
    if config.wandb.enabled and WANDB_AVAILABLE:
        wandb.init(
            project=config.wandb.project,
            name=f"{config.experiment.name}_{datetime.now().strftime('%m%d_%H%M')}",
            config=OmegaConf.to_container(config, resolve=True),
            tags=config.wandb.tags,
            notes=config.wandb.notes
        )
    elif config.wandb.enabled and not WANDB_AVAILABLE:
        logger.warning("Weights & Biases requested but not available")
    
    # è®¾ç½®å…±äº«ESMFold
    shared_esmfold = setup_shared_esmfold(config, device)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader = create_data_loaders(config, shared_esmfold, args.debug)
    
    # è®¾ç½®æ¨¡å‹å’Œè®­ç»ƒç»„ä»¶
    model, optimizer, scheduler, ema = setup_model_and_training(config, device, shared_esmfold)
    
    # è®¾ç½®tensorboardå’Œæ£€æŸ¥ç‚¹ç®¡ç†å™¨
    writer = SummaryWriter(log_dir=str(tensorboard_dir))
    checkpoint_manager = CheckpointManager(str(checkpoint_dir), config.training.max_checkpoints)
    
    # è®­ç»ƒå¾ªç¯
    logger.info(f"å¼€å§‹è®­ç»ƒ {config.training.num_epochs} ä¸ªepochs...")
    logger.info(f"æœ‰æ•ˆæ‰¹é‡å¤§å°: {config.data.batch_size * config.training.gradient_accumulation_steps}")
    
    # æ˜¾ç¤ºæ£€æŸ¥ç‚¹ä¿å­˜é…ç½®
    save_every = config.training.get('save_every', 10)
    logger.info(f"ğŸ’¾ æ£€æŸ¥ç‚¹ä¿å­˜é…ç½®:")
    logger.info(f"   å®šæœŸä¿å­˜é¢‘ç‡: æ¯ {save_every} ä¸ªepoch")
    logger.info(f"   æœ€ä½³æ¨¡å‹ä¿å­˜: å¯ç”¨ (åŸºäºéªŒè¯æŸå¤±)")
    logger.info(f"   Debugæ¨¡å¼: {'æ˜¯' if args.debug else 'å¦'}")
    if args.debug:
        logger.info(f"   Debugæ¨¡å¼ä¸‹ä¿å­˜é¢‘ç‡å·²è°ƒæ•´ä¸ºè‡³å°‘æ¯10ä¸ªepoch")
    
    global_step = 0
    best_val_loss = float('inf')
    scaler = GradScaler() if config.training.use_amp else None
    
    for epoch in range(config.training.num_epochs):
        logger.info(f"ğŸš€ å¼€å§‹ Epoch {epoch+1}/{config.training.num_epochs}")
        
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        epoch_loss = 0.0
        num_batches = 0
        
        pbar = tqdm(train_loader, desc=f"Training Epoch {epoch+1}")
        
        for batch_idx, batch in enumerate(pbar):
            try:
                outputs, loss = train_step(
                    model, batch, optimizer, scaler, config, device,
                    config.training.gradient_accumulation_steps, global_step
                )
                
                if outputs is None:
                    continue
                
                epoch_loss += loss
                num_batches += 1
                
                # æ¢¯åº¦ç´¯ç§¯å’Œæ›´æ–°
                if (batch_idx + 1) % config.training.gradient_accumulation_steps == 0:
                    if config.training.use_amp:
                        scaler.unscale_(optimizer)
                        torch.nn.utils.clip_grad_norm_(model.parameters(), config.training.gradient_clip)
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        torch.nn.utils.clip_grad_norm_(model.parameters(), config.training.gradient_clip)
                        optimizer.step()
                    
                    optimizer.zero_grad()
                    
                    if ema is not None:
                        ema.update()
                    
                    global_step += 1
                
                # è®°å½•æ—¥å¿—
                if global_step % config.logging.log_every == 0:
                    avg_loss = epoch_loss / max(num_batches, 1)
                    writer.add_scalar("train/loss", avg_loss, global_step)
                    writer.add_scalar("train/lr", scheduler.get_last_lr()[0], global_step)
                    
                    if config.wandb.enabled and WANDB_AVAILABLE:
                        wandb.log({
                            "train/loss": avg_loss,
                            "train/lr": scheduler.get_last_lr()[0],
                            "epoch": epoch
                        }, step=global_step)
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'loss': f"{loss:.4f}",
                    'lr': f"{scheduler.get_last_lr()[0]:.2e}",
                    'mem': f"{torch.cuda.memory_allocated() / 1e9:.1f}GB" if torch.cuda.is_available() else "CPU"
                })
                
                # å®šæœŸæ¸…ç†å†…å­˜
                if (batch_idx + 1) % 50 == 0:
                    clear_memory()
                
            except Exception as e:
                logger.error(f"è®­ç»ƒæ­¥éª¤ {batch_idx} å‡ºé”™: {e}")
                clear_memory()
                continue
        
        logger.info(f"âœ… Epoch {epoch+1} è®­ç»ƒå®Œæˆï¼Œå¹³å‡æŸå¤±: {epoch_loss/max(num_batches, 1):.4f}")
        
        # æ›´æ–°å­¦ä¹ ç‡
        logger.info("ğŸ“ˆ æ›´æ–°å­¦ä¹ ç‡...")
        scheduler.step()
        logger.info(f"å½“å‰å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]:.2e}")
        
        # éªŒè¯
        if (epoch + 1) % config.training.validate_every == 0:
            logger.info("ğŸ” å¼€å§‹éªŒè¯...")
            model.eval()
            val_loss = 0.0
            val_batches = 0
            
            with torch.no_grad():
                for batch in tqdm(val_loader, desc="Validation"):
                    try:
                        batch = move_to_device(batch, device)
                        
                        # æ£€æŸ¥batchçš„åŸºæœ¬å­—æ®µ
                        if 'sequences' not in batch or 'attention_mask' not in batch:
                            continue
                        
                        batch_size = batch['sequences'].shape[0]
                        timesteps = torch.randint(
                            0, config.diffusion.num_timesteps,
                            (batch_size,), device=device
                        )
                        
                        if config.training.use_amp:
                            with autocast():
                                outputs = model(
                                    sequences=batch['sequences'],
                                    attention_mask=batch['attention_mask'],
                                    timesteps=timesteps,
                                    structures=batch.get('structures'),
                                    conditions=batch.get('conditions'),
                                    return_loss=True
                                )
                        else:
                            outputs = model(
                                sequences=batch['sequences'],
                                attention_mask=batch['attention_mask'],
                                timesteps=timesteps,
                                structures=batch.get('structures'),
                                conditions=batch.get('conditions'),
                                return_loss=True
                            )
                        
                        val_loss += outputs['total_loss'].item()
                        val_batches += 1
                        
                    except Exception as e:
                        logger.warning(f"éªŒè¯æ­¥éª¤å‡ºé”™: {e}")
                        continue
            
            if val_batches > 0:
                avg_val_loss = val_loss / val_batches
                writer.add_scalar("val/loss", avg_val_loss, epoch)
                
                if config.wandb.enabled and WANDB_AVAILABLE:
                    wandb.log({"val/loss": avg_val_loss}, step=global_step)
                
                logger.info(f"âœ… éªŒè¯å®Œæˆï¼ŒéªŒè¯æŸå¤±: {avg_val_loss:.4f}")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if avg_val_loss < best_val_loss:
                    logger.info(f"ğŸ¯ å‘ç°æ›´å¥½çš„æ¨¡å‹ï¼éªŒè¯æŸå¤±ä» {best_val_loss:.4f} é™åˆ° {avg_val_loss:.4f}")
                    best_val_loss = avg_val_loss
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹
                    logger.info("ğŸ’¾ å‡†å¤‡ä¿å­˜æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹...")
                    
                    # è·å–æ¨¡å‹çŠ¶æ€å­—å…¸ï¼Œä½†æ’é™¤ESMFoldå‚æ•°ä»¥å‡å°‘æ–‡ä»¶å¤§å°
                    model_state_dict = model.state_dict()
                    
                    # è¿‡æ»¤æ‰ESMFoldç›¸å…³çš„å‚æ•°ï¼ˆè¿™äº›å‚æ•°å¾ˆå¤§ä¸”å¯ä»¥é‡æ–°åŠ è½½ï¼‰
                    filtered_state_dict = {}
                    for key, value in model_state_dict.items():
                        # è·³è¿‡ESMFoldç›¸å…³å‚æ•°
                        if not any(esmfold_key in key for esmfold_key in [
                            'structure_encoder.esmfold', 
                            'structure_encoder._esmfold',
                            'esmfold'
                        ]):
                            filtered_state_dict[key] = value
                    
                    logger.info(f"åŸå§‹å‚æ•°æ•°é‡: {len(model_state_dict)}, è¿‡æ»¤å: {len(filtered_state_dict)}")
                    
                    checkpoint = {
                        'epoch': epoch,
                        'global_step': global_step,
                        'model_state_dict': filtered_state_dict,  # ä½¿ç”¨è¿‡æ»¤åçš„çŠ¶æ€å­—å…¸
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict(),
                        'val_loss': avg_val_loss,
                        'config': config,
                        'excluded_keys': ['structure_encoder.esmfold', 'structure_encoder._esmfold'],  # è®°å½•æ’é™¤çš„é”®
                        'is_debug': args.debug  # è®°å½•æ˜¯å¦ä¸ºdebugæ¨¡å¼
                    }
                    
                    if ema is not None:
                        checkpoint['ema_state_dict'] = ema.state_dict()
                    
                    logger.info("ğŸ’¾ å¼€å§‹ä¿å­˜æ£€æŸ¥ç‚¹åˆ°ç£ç›˜...")
                    try:
                        checkpoint_manager.save_checkpoint(checkpoint, epoch, is_best=True)
                        logger.info(f"âœ… æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜æˆåŠŸï¼")
                    except Exception as e:
                        logger.error(f"âŒ æ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥: {e}")
                        # ç»§ç»­è®­ç»ƒï¼Œä¸è¦å› ä¸ºä¿å­˜å¤±è´¥è€Œä¸­æ–­
                else:
                    logger.info(f"ğŸ“Š å½“å‰éªŒè¯æŸå¤± {avg_val_loss:.4f} æœªè¶…è¿‡æœ€ä½³ {best_val_loss:.4f}")
            else:
                logger.warning("âš ï¸ éªŒè¯æ‰¹æ¬¡ä¸º0ï¼Œè·³è¿‡éªŒè¯")
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆä¸ä»…ä»…æ˜¯æœ€ä½³æ¨¡å‹ï¼‰
        if (epoch + 1) % config.training.get('save_every', 10) == 0:
            logger.info(f"ğŸ’¾ å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ (Epoch {epoch+1})...")
            
            # è·å–æ¨¡å‹çŠ¶æ€å­—å…¸ï¼Œä½†æ’é™¤ESMFoldå‚æ•°ä»¥å‡å°‘æ–‡ä»¶å¤§å°
            model_state_dict = model.state_dict()
            
            # è¿‡æ»¤æ‰ESMFoldç›¸å…³çš„å‚æ•°
            filtered_state_dict = {}
            for key, value in model_state_dict.items():
                if not any(esmfold_key in key for esmfold_key in [
                    'structure_encoder.esmfold', 
                    'structure_encoder._esmfold',
                    'esmfold'
                ]):
                    filtered_state_dict[key] = value
            
            checkpoint = {
                'epoch': epoch,
                'global_step': global_step,
                'model_state_dict': filtered_state_dict,
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'val_loss': best_val_loss,  # ä½¿ç”¨å½“å‰æœ€ä½³éªŒè¯æŸå¤±
                'config': config,
                'excluded_keys': ['structure_encoder.esmfold', 'structure_encoder._esmfold'],
                'is_debug': args.debug,
                'checkpoint_type': 'periodic'  # æ ‡è®°ä¸ºå®šæœŸæ£€æŸ¥ç‚¹
            }
            
            if ema is not None:
                checkpoint['ema_state_dict'] = ema.state_dict()
            
            try:
                checkpoint_manager.save_checkpoint(checkpoint, epoch, is_best=False)
                logger.info(f"âœ… å®šæœŸæ£€æŸ¥ç‚¹ä¿å­˜æˆåŠŸï¼")
            except Exception as e:
                logger.error(f"âŒ å®šæœŸæ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥: {e}")
        
        logger.info(f"ğŸ Epoch {epoch+1} å®Œå…¨ç»“æŸï¼Œå‡†å¤‡ä¸‹ä¸€ä¸ªepoch...")
        
        # å¼ºåˆ¶æ¸…ç†å†…å­˜
        clear_memory()
    
    writer.close()
    logger.info("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    
    if config.wandb.enabled and WANDB_AVAILABLE:
        wandb.finish()

    # æ¸…ç†è®­ç»ƒæœŸé—´å ç”¨çš„èµ„æºï¼Œä¸ºéªŒè¯é˜¶æ®µåšå‡†å¤‡
    logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†è®­ç»ƒèµ„æºï¼Œä¸ºç”Ÿæˆå’ŒéªŒè¯é˜¶æ®µé‡Šæ”¾å†…å­˜...")
    
    # åˆ é™¤ä¸»è¦çš„è®­ç»ƒå¯¹è±¡
    try:
        del model
        del optimizer
        del scheduler
        if 'ema' in locals() and ema is not None:
            del ema
        del train_loader
        del val_loader
        if 'shared_esmfold' in locals() and shared_esmfold is not None:
            del shared_esmfold
        logger.info("ğŸ—‘ï¸ è®­ç»ƒå¯¹è±¡å·²åˆ é™¤ã€‚")
    except NameError as e:
        logger.warning(f"æ¸…ç†éƒ¨åˆ†è®­ç»ƒå¯¹è±¡æ—¶å‡ºé”™ï¼ˆå¯èƒ½æœªå®šä¹‰ï¼‰: {e}")

    # è°ƒç”¨åƒåœ¾å›æ”¶å’ŒCUDAç¼“å­˜æ¸…ç†
    clear_memory()
    
    if torch.cuda.is_available():
        logger.info(f"âœ… è®­ç»ƒèµ„æºæ¸…ç†å®Œæ¯•. å½“å‰GPUå†…å­˜: {torch.cuda.memory_allocated(0) / 1e9:.2f}GB")
        logger.info(f"   é¢„ç•™å†…å­˜: {torch.cuda.memory_reserved(0) / 1e9:.2f}GB")

    # è®­ç»ƒå®Œæˆåè¿›è¡Œç”Ÿæˆå’ŒéªŒè¯
    logger.info("ğŸš€ å¼€å§‹ç”Ÿæˆå’ŒéªŒè¯...")
    generate_and_validate(config, device)


if __name__ == "__main__":
    main() 