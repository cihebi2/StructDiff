#!/usr/bin/env python3
"""
åˆ†ç¦»å¼è®­ç»ƒè„šæœ¬
åŸºäºCPL-Diffçš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼š
1. é˜¶æ®µ1ï¼šå›ºå®šESMç¼–ç å™¨ï¼Œè®­ç»ƒå»å™ªå™¨
2. é˜¶æ®µ2ï¼šå›ºå®šå»å™ªå™¨ï¼Œè®­ç»ƒåºåˆ—è§£ç å™¨
"""

import os
import sys
import argparse
import logging
from pathlib import Path
from typing import Dict, Optional

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import AutoTokenizer
import yaml
from omegaconf import OmegaConf

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# æ·»åŠ structdiffåŒ…è·¯å¾„
structdiff_path = project_root / "structdiff"
if str(structdiff_path.parent) not in sys.path:
    sys.path.insert(0, str(structdiff_path.parent))

from structdiff.models.structdiff import StructDiff
from structdiff.diffusion.gaussian_diffusion import GaussianDiffusion
from structdiff.data.dataset import PeptideStructureDataset
from structdiff.training.separated_training import SeparatedTrainingManager, SeparatedTrainingConfig
from structdiff.training.length_controller import create_length_controller_from_data, LengthAwareDataCollator
from structdiff.utils.logger import setup_logger, get_logger
from structdiff.utils.config import load_config

# å…¨å±€æ—¥å¿—è®°å½•å™¨
logger = get_logger(__name__)


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="åˆ†ç¦»å¼è®­ç»ƒè„šæœ¬")
    
    # åŸºç¡€é…ç½®
    parser.add_argument(
        "--config", 
        type=str, 
        default="configs/separated_training_production.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--data-dir",
        type=str,
        default="./data/processed",
        help="æ•°æ®ç›®å½•"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="./outputs/separated_training",
        help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        help="è®¾å¤‡ (cuda/cpu/auto)"
    )
    
    # è®­ç»ƒé…ç½®è¦†ç›–
    parser.add_argument("--stage1-epochs", type=int, help="é˜¶æ®µ1è®­ç»ƒè½®æ•°")
    parser.add_argument("--stage2-epochs", type=int, help="é˜¶æ®µ2è®­ç»ƒè½®æ•°")
    parser.add_argument("--stage1-lr", type=float, help="é˜¶æ®µ1å­¦ä¹ ç‡")
    parser.add_argument("--stage2-lr", type=float, help="é˜¶æ®µ2å­¦ä¹ ç‡")
    parser.add_argument("--batch-size", type=int, help="æ‰¹æ¬¡å¤§å°")
    
    # åŠŸèƒ½å¼€å…³
    parser.add_argument("--use-cfg", action="store_true", help="ä½¿ç”¨åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼")
    parser.add_argument("--use-length-control", action="store_true", help="ä½¿ç”¨é•¿åº¦æ§åˆ¶")
    parser.add_argument("--use-amp", action="store_true", help="ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
    parser.add_argument("--use-ema", action="store_true", help="ä½¿ç”¨EMA")
    
    # é˜¶æ®µæ§åˆ¶
    parser.add_argument("--stage", type=str, choices=['1', '2', 'both'], default='both',
                       help="è®­ç»ƒé˜¶æ®µ (1: åªè®­ç»ƒé˜¶æ®µ1, 2: åªè®­ç»ƒé˜¶æ®µ2, both: å®Œæ•´è®­ç»ƒ)")
    parser.add_argument("--stage1-checkpoint", type=str, help="é˜¶æ®µ1æ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆç”¨äºé˜¶æ®µ2è®­ç»ƒï¼‰")
    
    # è°ƒè¯•é€‰é¡¹
    parser.add_argument("--debug", action="store_true", help="è°ƒè¯•æ¨¡å¼")
    parser.add_argument("--dry-run", action="store_true", help="å¹²è¿è¡Œï¼ˆä¸å®é™…è®­ç»ƒï¼‰")
    
    return parser.parse_args()


def setup_device(config: Dict, device_arg: str = "auto") -> str:
    """è®¾ç½®è®¾å¤‡å’ŒGPUèµ„æºåˆ†é…"""
    # ä¼˜å…ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾å¤‡è®¾ç½®
    if hasattr(config, 'resources') and hasattr(config.resources, 'device'):
        device = config.resources.device
    elif device_arg != "auto":
        device = device_arg
    else:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    if device.startswith("cuda") and not torch.cuda.is_available():
        logger.warning("CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
        device = "cpu"
    
    # è®¾ç½®å¯è§GPU
    if hasattr(config, 'resources') and hasattr(config.resources, 'available_gpus'):
        available_gpus = config.resources.available_gpus
        # è®¾ç½®CUDA_VISIBLE_DEVICESç¯å¢ƒå˜é‡
        import os
        os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, available_gpus))
        logger.info(f"è®¾ç½®å¯è§GPU: {available_gpus}")
    
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # å¦‚æœé…ç½®äº†é˜¶æ®µç‰¹å®šçš„GPUåˆ†é…ï¼Œè®°å½•ä¿¡æ¯
    if hasattr(config, 'resources'):
        if hasattr(config.resources, 'stage1_gpus'):
            logger.info(f"é˜¶æ®µ1 GPUåˆ†é…: {config.resources.stage1_gpus}")
        if hasattr(config.resources, 'stage2_gpus'):
            logger.info(f"é˜¶æ®µ2 GPUåˆ†é…: {config.resources.stage2_gpus}")
    
    return device


def create_model_and_diffusion(config: Dict) -> tuple:
    """åˆ›å»ºæ¨¡å‹å’Œæ‰©æ•£è¿‡ç¨‹"""
    # åˆ›å»ºæ¨¡å‹
    model = StructDiff(config.model)
    
    # åˆ›å»ºæ‰©æ•£è¿‡ç¨‹
    diffusion = GaussianDiffusion(
        num_timesteps=config.diffusion.num_timesteps,
        noise_schedule=config.diffusion.noise_schedule,
        beta_start=config.diffusion.beta_start,
        beta_end=config.diffusion.beta_end
    )
    
    return model, diffusion


def create_data_loaders(config: Dict, 
                       training_config: SeparatedTrainingConfig,
                       tokenizer) -> tuple:
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    
    # åˆ›å»ºé•¿åº¦æ§åˆ¶å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
    length_controller = None
    if training_config.use_length_control:
        train_data_path = Path(training_config.data_dir) / "train.csv"
        if train_data_path.exists():
            length_controller = create_length_controller_from_data(
                str(train_data_path),
                min_length=training_config.min_length,
                max_length=training_config.max_length
            )
            logger.info("âœ“ åˆ›å»ºé•¿åº¦æ§åˆ¶å™¨æˆåŠŸ")
        else:
            logger.warning(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {train_data_path}")
    
    # åˆ›å»ºæ•°æ®æ•´ç†å™¨
    collator = LengthAwareDataCollator(
        length_controller=length_controller,
        tokenizer=tokenizer,
        use_length_control=training_config.use_length_control
    )
    
    # è·å–æ•°æ®åŠ è½½é…ç½®
    num_workers = config.data.get('num_workers', 2)
    pin_memory = config.data.get('pin_memory', True)
    
    # æ£€æŸ¥ç»“æ„ç‰¹å¾è®¾ç½®
    use_structures = config.data.get('use_predicted_structures', False)
    structure_cache_dir = config.data.get('structure_cache_dir', './cache')
    
    if use_structures:
        logger.info(f"âœ“ å¯ç”¨ç»“æ„ç‰¹å¾ï¼Œç¼“å­˜ç›®å½•: {structure_cache_dir}")
        if not Path(structure_cache_dir).exists():
            logger.warning(f"ç»“æ„ç¼“å­˜ç›®å½•ä¸å­˜åœ¨: {structure_cache_dir}")
    else:
        logger.info("ç»“æ„ç‰¹å¾å·²ç¦ç”¨")
    
    # è®­ç»ƒæ•°æ®é›†
    train_dataset = PeptideStructureDataset(
        data_path=str(Path(training_config.data_dir) / "train.csv"),
        config=config,
        is_training=True,
        cache_dir=str(Path(structure_cache_dir) / "train") if use_structures else None
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=training_config.stage1_batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=pin_memory,
        collate_fn=collator
    )
    
    # éªŒè¯æ•°æ®é›†
    val_dataset = None
    val_loader = None
    val_data_path = Path(training_config.data_dir) / "val.csv"
    if val_data_path.exists():
        val_dataset = PeptideStructureDataset(
            data_path=str(val_data_path),
            config=config,
            is_training=False,
            cache_dir=str(Path(structure_cache_dir) / "val") if use_structures else None
        )
        val_loader = DataLoader(
            val_dataset,
            batch_size=training_config.stage1_batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=pin_memory,
            collate_fn=collator
        )
        logger.info("âœ“ åˆ›å»ºéªŒè¯æ•°æ®é›†æˆåŠŸ")
    else:
        logger.warning("éªŒè¯æ•°æ®é›†ä¸å­˜åœ¨ï¼Œå°†è·³è¿‡éªŒè¯")
    
    logger.info(f"è®­ç»ƒæ•°æ®é›†å¤§å°: {len(train_dataset)}")
    if val_dataset:
        logger.info(f"éªŒè¯æ•°æ®é›†å¤§å°: {len(val_dataset)}")
    
    return train_loader, val_loader


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logger(
        level=logging.DEBUG if args.debug else logging.INFO,
        log_file=Path(args.output_dir) / "training.log"
    )
    
    logger.info("ğŸš€ å¼€å§‹åˆ†ç¦»å¼è®­ç»ƒ")
    logger.info(f"å‚æ•°: {vars(args)}")
    
    # è®¾ç½®è®¾å¤‡ï¼ˆåœ¨åŠ è½½é…ç½®ä¹‹åï¼‰
    
    # åŠ è½½é…ç½®
    if Path(args.config).exists():
        config = load_config(args.config)
        logger.info(f"åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
    else:
        # ä½¿ç”¨é»˜è®¤é…ç½®
        config = OmegaConf.create({
            "model": {
                "type": "StructDiff",
                "sequence_encoder": {
                    "pretrained_model": "facebook/esm2_t6_8M_UR50D",
                    "freeze_encoder": False
                },
                "denoiser": {
                    "hidden_dim": 768,
                    "num_layers": 12,
                    "num_heads": 12
                }
            },
            "diffusion": {
                "num_timesteps": 1000,
                "noise_schedule": "sqrt",
                "beta_start": 0.0001,
                "beta_end": 0.02
            },
            "data": {
                "max_length": 50,
                "min_length": 5,
                "use_predicted_structures": False
            }
        })
        logger.warning("ä½¿ç”¨é»˜è®¤é…ç½®")
    
    # è®¾ç½®è®¾å¤‡ï¼ˆåœ¨é…ç½®åŠ è½½ä¹‹åï¼‰
    device = setup_device(config, args.device)
    
    # ä»ä¸»é…ç½®ä¸­æå–è®­ç»ƒå’Œè¯„ä¼°ç›¸å…³çš„é…ç½®
    train_params = config.get('separated_training', {})
    stage1_params = train_params.get('stage1', {})
    stage2_params = train_params.get('stage2', {})
    evaluation_config = config.get('evaluation', {})
    enhancements_config = config.get('training_enhancements', {})

    # åˆ›å»ºè®­ç»ƒé…ç½®ï¼Œä¼˜å…ˆä½¿ç”¨YAMLä¸­çš„å€¼ï¼Œç„¶åæ‰æ˜¯dataclassçš„é»˜è®¤å€¼
    training_config = SeparatedTrainingConfig(
        data_dir=args.data_dir,
        output_dir=args.output_dir,
        checkpoint_dir=str(Path(args.output_dir) / "checkpoints"),
        
        # é˜¶æ®µ1
        stage1_epochs=stage1_params.get('epochs', 200),
        stage1_lr=stage1_params.get('learning_rate', 1e-4),
        stage1_batch_size=stage1_params.get('batch_size', 32),
        stage1_gradient_clip=stage1_params.get('gradient_clip', 1.0),
        stage1_warmup_steps=stage1_params.get('warmup_steps', 1000),

        # é˜¶æ®µ2
        stage2_epochs=stage2_params.get('epochs', 100),
        stage2_lr=stage2_params.get('learning_rate', 5e-5),
        stage2_batch_size=stage2_params.get('batch_size', 64),
        stage2_gradient_clip=stage2_params.get('gradient_clip', 0.5),
        stage2_warmup_steps=stage2_params.get('warmup_steps', 500),

        # åŠŸèƒ½å¼€å…³ (å‘½ä»¤è¡Œä¼˜å…ˆ)
        use_cfg=args.use_cfg or config.get('classifier_free_guidance', {}).get('enabled', True),
        use_length_control=args.use_length_control or config.get('length_control', {}).get('enabled', True),
        use_amp=args.use_amp or enhancements_config.get('use_amp', True),
        use_ema=args.use_ema or enhancements_config.get('use_ema', True),
        ema_decay=enhancements_config.get('ema_decay', 0.9999),

        # è¯„ä¼°é…ç½®
        enable_evaluation=evaluation_config.get('enabled', True),
        evaluate_every=evaluation_config.get('evaluate_every', 5),
        evaluation_metrics=evaluation_config.get('metrics', None),
        evaluation_num_samples=evaluation_config.get('generation', {}).get('num_samples', 1000),
        evaluation_guidance_scale=evaluation_config.get('generation', {}).get('guidance_scale', 2.0),
        auto_generate_after_training=True  # é»˜è®¤å¯ç”¨
    )
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
    if args.stage1_epochs:
        training_config.stage1_epochs = args.stage1_epochs
    if args.stage2_epochs:
        training_config.stage2_epochs = args.stage2_epochs
    if args.stage1_lr:
        training_config.stage1_lr = args.stage1_lr
    if args.stage2_lr:
        training_config.stage2_lr = args.stage2_lr
    if args.batch_size:
        training_config.stage1_batch_size = args.batch_size
        training_config.stage2_batch_size = args.batch_size
    
    logger.info(f"è®­ç»ƒé…ç½®: {training_config}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    
    if args.dry_run:
        logger.info("ğŸ” å¹²è¿è¡Œæ¨¡å¼ï¼Œä¸è¿›è¡Œå®é™…è®­ç»ƒ")
        return
    
    try:
        # åˆ›å»ºåˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(
            config.model.sequence_encoder.pretrained_model
        )
        logger.info("âœ“ åˆ›å»ºåˆ†è¯å™¨æˆåŠŸ")
        
        # åˆ›å»ºæ¨¡å‹å’Œæ‰©æ•£è¿‡ç¨‹
        model, diffusion = create_model_and_diffusion(config)
        model = model.to(device)
        logger.info("âœ“ åˆ›å»ºæ¨¡å‹æˆåŠŸ")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader = create_data_loaders(config, training_config, tokenizer)
        logger.info("âœ“ åˆ›å»ºæ•°æ®åŠ è½½å™¨æˆåŠŸ")
        
        # åˆ›å»ºè®­ç»ƒç®¡ç†å™¨
        trainer = SeparatedTrainingManager(
            config=training_config,
            model=model,
            diffusion=diffusion,
            device=device,
            tokenizer=tokenizer
        )
        logger.info("âœ“ åˆ›å»ºè®­ç»ƒç®¡ç†å™¨æˆåŠŸ")
        
        # æ‰§è¡Œè®­ç»ƒ
        if args.stage == 'both':
            # å®Œæ•´çš„ä¸¤é˜¶æ®µè®­ç»ƒ
            logger.info("ğŸ¯ å¼€å§‹å®Œæ•´çš„ä¸¤é˜¶æ®µè®­ç»ƒ")
            final_stats = trainer.run_complete_training(train_loader, val_loader)
            
        elif args.stage == '1':
            # åªè®­ç»ƒé˜¶æ®µ1
            logger.info("ğŸ¯ åªæ‰§è¡Œé˜¶æ®µ1è®­ç»ƒ")
            stage1_stats = trainer.train_stage1(train_loader, val_loader)
            final_stats = {'stage1': stage1_stats}
            
        elif args.stage == '2':
            # åªè®­ç»ƒé˜¶æ®µ2
            if args.stage1_checkpoint:
                logger.info(f"ğŸ¯ åŠ è½½é˜¶æ®µ1æ£€æŸ¥ç‚¹å¹¶æ‰§è¡Œé˜¶æ®µ2è®­ç»ƒ: {args.stage1_checkpoint}")
                trainer.load_stage1_checkpoint(args.stage1_checkpoint)
            else:
                logger.warning("é˜¶æ®µ2è®­ç»ƒéœ€è¦é˜¶æ®µ1æ£€æŸ¥ç‚¹ï¼Œä½†æœªæä¾›")
                
            stage2_stats = trainer.train_stage2(train_loader, val_loader)
            final_stats = {'stage2': stage2_stats}
        
        # è¾“å‡ºè®­ç»ƒæ‘˜è¦
        summary = trainer.get_training_summary()
        logger.info("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        logger.info("è®­ç»ƒæ‘˜è¦:")
        for stage, stats in summary.items():
            logger.info(f"  {stage}:")
            for key, value in stats.items():
                logger.info(f"    {key}: {value}")
        
        # ä¿å­˜æœ€ç»ˆæ‘˜è¦
        summary_path = Path(args.output_dir) / "training_summary.json"
        import json
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2)
        logger.info(f"è®­ç»ƒæ‘˜è¦ä¿å­˜åˆ°: {summary_path}")
        
    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise


if __name__ == "__main__":
    main()