#!/usr/bin/env python3
"""
åˆ†ç¦»å¼è®­ç»ƒè„šæœ¬
åŸºäºCPL-Diffçš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼š
1. é˜¶æ®µ1ï¼šå›ºå®šESMç¼–ç å™¨ï¼Œè®­ç»ƒå»å™ªå™¨
2. é˜¶æ®µ2ï¼šå›ºå®šå»å™ªå™¨ï¼Œè®­ç»ƒåºåˆ—è§£ç å™¨
"""

import os
import sys
import argparse
import logging
from pathlib import Path
from typing import Dict, Optional

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import AutoTokenizer
import yaml
from omegaconf import OmegaConf

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from structdiff.models.structdiff import StructDiff
from structdiff.diffusion.gaussian_diffusion import GaussianDiffusion
from structdiff.data.dataset import PeptideStructureDataset
from structdiff.training.separated_training import SeparatedTrainingManager, SeparatedTrainingConfig
from structdiff.training.length_controller import create_length_controller_from_data, LengthAwareDataCollator
from structdiff.utils.logger import setup_logger, get_logger
from structdiff.utils.config import load_config


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="åˆ†ç¦»å¼è®­ç»ƒè„šæœ¬")
    
    # åŸºç¡€é…ç½®
    parser.add_argument(
        "--config", 
        type=str, 
        default="configs/separated_training.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--data-dir",
        type=str,
        default="./data/processed",
        help="æ•°æ®ç›®å½•"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="./outputs/separated_training",
        help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        help="è®¾å¤‡ (cuda/cpu/auto)"
    )
    
    # è®­ç»ƒé…ç½®è¦†ç›–
    parser.add_argument("--stage1-epochs", type=int, help="é˜¶æ®µ1è®­ç»ƒè½®æ•°")
    parser.add_argument("--stage2-epochs", type=int, help="é˜¶æ®µ2è®­ç»ƒè½®æ•°")
    parser.add_argument("--stage1-lr", type=float, help="é˜¶æ®µ1å­¦ä¹ ç‡")
    parser.add_argument("--stage2-lr", type=float, help="é˜¶æ®µ2å­¦ä¹ ç‡")
    parser.add_argument("--batch-size", type=int, help="æ‰¹æ¬¡å¤§å°")
    
    # åŠŸèƒ½å¼€å…³
    parser.add_argument("--use-cfg", action="store_true", help="ä½¿ç”¨åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼")
    parser.add_argument("--use-length-control", action="store_true", help="ä½¿ç”¨é•¿åº¦æ§åˆ¶")
    parser.add_argument("--use-amp", action="store_true", help="ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
    parser.add_argument("--use-ema", action="store_true", help="ä½¿ç”¨EMA")
    
    # é˜¶æ®µæ§åˆ¶
    parser.add_argument("--stage", type=str, choices=['1', '2', 'both'], default='both',
                       help="è®­ç»ƒé˜¶æ®µ (1: åªè®­ç»ƒé˜¶æ®µ1, 2: åªè®­ç»ƒé˜¶æ®µ2, both: å®Œæ•´è®­ç»ƒ)")
    parser.add_argument("--stage1-checkpoint", type=str, help="é˜¶æ®µ1æ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆç”¨äºé˜¶æ®µ2è®­ç»ƒï¼‰")
    
    # è°ƒè¯•é€‰é¡¹
    parser.add_argument("--debug", action="store_true", help="è°ƒè¯•æ¨¡å¼")
    parser.add_argument("--dry-run", action="store_true", help="å¹²è¿è¡Œï¼ˆä¸å®é™…è®­ç»ƒï¼‰")
    
    return parser.parse_args()


def setup_device(device_arg: str) -> str:
    """è®¾ç½®è®¾å¤‡"""
    if device_arg == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    else:
        device = device_arg
    
    if device == "cuda" and not torch.cuda.is_available():
        logger.warning("CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
        device = "cpu"
    
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    return device


def create_model_and_diffusion(config: Dict) -> tuple:
    """åˆ›å»ºæ¨¡å‹å’Œæ‰©æ•£è¿‡ç¨‹"""
    # åˆ›å»ºæ¨¡å‹
    model = StructDiff(config.model)
    
    # åˆ›å»ºæ‰©æ•£è¿‡ç¨‹
    diffusion = GaussianDiffusion(config.diffusion)
    
    return model, diffusion


def create_data_loaders(config: Dict, 
                       training_config: SeparatedTrainingConfig,
                       tokenizer) -> tuple:
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    
    # åˆ›å»ºé•¿åº¦æ§åˆ¶å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
    length_controller = None
    if training_config.use_length_control:
        train_data_path = Path(training_config.data_dir) / "train.csv"
        if train_data_path.exists():
            length_controller = create_length_controller_from_data(
                str(train_data_path),
                min_length=training_config.min_length,
                max_length=training_config.max_length
            )
            logger.info("âœ“ åˆ›å»ºé•¿åº¦æ§åˆ¶å™¨æˆåŠŸ")
        else:
            logger.warning(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {train_data_path}")
    
    # åˆ›å»ºæ•°æ®æ•´ç†å™¨
    collator = LengthAwareDataCollator(
        length_controller=length_controller,
        tokenizer=tokenizer,
        use_length_control=training_config.use_length_control
    )
    
    # è®­ç»ƒæ•°æ®é›†
    train_dataset = PeptideStructureDataset(
        data_path=str(Path(training_config.data_dir) / "train.csv"),
        config=config,
        is_training=True
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=training_config.stage1_batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True,
        collate_fn=collator
    )
    
    # éªŒè¯æ•°æ®é›†
    val_dataset = None
    val_loader = None
    val_data_path = Path(training_config.data_dir) / "val.csv"
    if val_data_path.exists():
        val_dataset = PeptideStructureDataset(
            data_path=str(val_data_path),
            config=config,
            is_training=False
        )
        val_loader = DataLoader(
            val_dataset,
            batch_size=training_config.stage1_batch_size,
            shuffle=False,
            num_workers=4,
            pin_memory=True,
            collate_fn=collator
        )
        logger.info("âœ“ åˆ›å»ºéªŒè¯æ•°æ®é›†æˆåŠŸ")
    else:
        logger.warning("éªŒè¯æ•°æ®é›†ä¸å­˜åœ¨ï¼Œå°†è·³è¿‡éªŒè¯")
    
    logger.info(f"è®­ç»ƒæ•°æ®é›†å¤§å°: {len(train_dataset)}")
    if val_dataset:
        logger.info(f"éªŒè¯æ•°æ®é›†å¤§å°: {len(val_dataset)}")
    
    return train_loader, val_loader


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logger(
        level=logging.DEBUG if args.debug else logging.INFO,
        log_file=Path(args.output_dir) / "training.log"
    )
    logger = get_logger(__name__)
    
    logger.info("ğŸš€ å¼€å§‹åˆ†ç¦»å¼è®­ç»ƒ")
    logger.info(f"å‚æ•°: {vars(args)}")
    
    # è®¾ç½®è®¾å¤‡
    device = setup_device(args.device)
    
    # åŠ è½½é…ç½®
    if Path(args.config).exists():
        config = load_config(args.config)
        logger.info(f"åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
    else:
        # ä½¿ç”¨é»˜è®¤é…ç½®
        config = OmegaConf.create({
            "model": {
                "type": "StructDiff",
                "sequence_encoder": {
                    "pretrained_model": "facebook/esm2_t6_8M_UR50D",
                    "freeze_encoder": False
                },
                "denoiser": {
                    "hidden_dim": 768,
                    "num_layers": 12,
                    "num_heads": 12
                }
            },
            "diffusion": {
                "num_timesteps": 1000,
                "noise_schedule": "sqrt",
                "beta_start": 0.0001,
                "beta_end": 0.02
            }
        })
        logger.warning("ä½¿ç”¨é»˜è®¤é…ç½®")
    
    # åˆ›å»ºè®­ç»ƒé…ç½®
    training_config = SeparatedTrainingConfig(
        data_dir=args.data_dir,
        output_dir=args.output_dir,
        checkpoint_dir=str(Path(args.output_dir) / "checkpoints"),
        use_cfg=args.use_cfg,
        use_length_control=args.use_length_control,
        use_amp=args.use_amp,
        use_ema=args.use_ema
    )
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
    if args.stage1_epochs:
        training_config.stage1_epochs = args.stage1_epochs
    if args.stage2_epochs:
        training_config.stage2_epochs = args.stage2_epochs
    if args.stage1_lr:
        training_config.stage1_lr = args.stage1_lr
    if args.stage2_lr:
        training_config.stage2_lr = args.stage2_lr
    if args.batch_size:
        training_config.stage1_batch_size = args.batch_size
        training_config.stage2_batch_size = args.batch_size
    
    logger.info(f"è®­ç»ƒé…ç½®: {training_config}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    
    if args.dry_run:
        logger.info("ğŸ” å¹²è¿è¡Œæ¨¡å¼ï¼Œä¸è¿›è¡Œå®é™…è®­ç»ƒ")
        return
    
    try:
        # åˆ›å»ºåˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(
            config.model.sequence_encoder.pretrained_model
        )
        logger.info("âœ“ åˆ›å»ºåˆ†è¯å™¨æˆåŠŸ")
        
        # åˆ›å»ºæ¨¡å‹å’Œæ‰©æ•£è¿‡ç¨‹
        model, diffusion = create_model_and_diffusion(config)
        model = model.to(device)
        logger.info("âœ“ åˆ›å»ºæ¨¡å‹æˆåŠŸ")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader = create_data_loaders(config, training_config, tokenizer)
        logger.info("âœ“ åˆ›å»ºæ•°æ®åŠ è½½å™¨æˆåŠŸ")
        
        # åˆ›å»ºè®­ç»ƒç®¡ç†å™¨
        trainer = SeparatedTrainingManager(
            config=training_config,
            model=model,
            diffusion=diffusion,
            device=device
        )
        logger.info("âœ“ åˆ›å»ºè®­ç»ƒç®¡ç†å™¨æˆåŠŸ")
        
        # æ‰§è¡Œè®­ç»ƒ
        if args.stage == 'both':
            # å®Œæ•´çš„ä¸¤é˜¶æ®µè®­ç»ƒ
            logger.info("ğŸ¯ å¼€å§‹å®Œæ•´çš„ä¸¤é˜¶æ®µè®­ç»ƒ")
            final_stats = trainer.run_complete_training(train_loader, val_loader)
            
        elif args.stage == '1':
            # åªè®­ç»ƒé˜¶æ®µ1
            logger.info("ğŸ¯ åªæ‰§è¡Œé˜¶æ®µ1è®­ç»ƒ")
            stage1_stats = trainer.train_stage1(train_loader, val_loader)
            final_stats = {'stage1': stage1_stats}
            
        elif args.stage == '2':
            # åªè®­ç»ƒé˜¶æ®µ2
            if args.stage1_checkpoint:
                logger.info(f"ğŸ¯ åŠ è½½é˜¶æ®µ1æ£€æŸ¥ç‚¹å¹¶æ‰§è¡Œé˜¶æ®µ2è®­ç»ƒ: {args.stage1_checkpoint}")
                trainer.load_stage1_checkpoint(args.stage1_checkpoint)
            else:
                logger.warning("é˜¶æ®µ2è®­ç»ƒéœ€è¦é˜¶æ®µ1æ£€æŸ¥ç‚¹ï¼Œä½†æœªæä¾›")
                
            stage2_stats = trainer.train_stage2(train_loader, val_loader)
            final_stats = {'stage2': stage2_stats}
        
        # è¾“å‡ºè®­ç»ƒæ‘˜è¦
        summary = trainer.get_training_summary()
        logger.info("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        logger.info("è®­ç»ƒæ‘˜è¦:")
        for stage, stats in summary.items():
            logger.info(f"  {stage}:")
            for key, value in stats.items():
                logger.info(f"    {key}: {value}")
        
        # ä¿å­˜æœ€ç»ˆæ‘˜è¦
        summary_path = Path(args.output_dir) / "training_summary.json"
        import json
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2)
        logger.info(f"è®­ç»ƒæ‘˜è¦ä¿å­˜åˆ°: {summary_path}")
        
    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise


if __name__ == "__main__":
    main()