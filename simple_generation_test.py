#!/usr/bin/env python3
"""
ç®€åŒ–çš„è‚½æ®µç”Ÿæˆæµ‹è¯•è„šæœ¬ - ä½¿ç”¨å›ºå®šé•¿åº¦
"""

import os
import sys
import torch
import numpy as np
from pathlib import Path
from omegaconf import OmegaConf
import json
from collections import Counter
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from structdiff.models.structdiff import StructDiff
from structdiff.utils.logger import setup_logger, get_logger

logger = get_logger(__name__)


class SimplePeptideGenerator:
    """ç®€åŒ–çš„è‚½æ®µç”Ÿæˆå™¨ï¼Œä½¿ç”¨å›ºå®šé•¿åº¦"""
    
    def __init__(self, model_path: str, device: str = 'cuda'):
        self.device = torch.device(device)
        self.model_path = model_path
        
        # åˆ›å»ºä¸è®­ç»ƒæ—¶ç›¸åŒçš„é…ç½®
        self.config = self._create_config()
        
        # åŠ è½½æ¨¡å‹
        self.model = self._load_model()
        
        # è‚½æ®µç±»å‹æ˜ å°„
        self.peptide_type_map = {
            'antimicrobial': 0,
            'antifungal': 1,
            'antiviral': 2
        }
        
        # å›ºå®šåºåˆ—é•¿åº¦ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        self.fixed_length = 30  # ä¸åŒ…æ‹¬CLSå’ŒSEP token
        
        logger.info("âœ… SimplePeptideGeneratoråˆå§‹åŒ–å®Œæˆ")
    
    def _create_config(self):
        """åˆ›å»ºä¸è®­ç»ƒæ—¶ç›¸åŒçš„é…ç½®"""
        config = OmegaConf.create({
            'sequence_encoder': {
                'pretrained_model': 'facebook/esm2_t6_8M_UR50D',
                'freeze_encoder': False,
                'use_lora': True,
                'lora_rank': 16,
                'lora_alpha': 32,
                'lora_dropout': 0.1
            },
            'structure_encoder': {
                'hidden_dim': 256,
                'num_layers': 3,
                'use_esmfold': False,
                'use_coordinates': False,
                'use_distances': False,
                'use_angles': False,
                'use_secondary_structure': True
            },
            'denoiser': {
                'hidden_dim': 320,
                'num_layers': 6,
                'num_heads': 8,
                'dropout': 0.1,
                'use_cross_attention': True,
                'use_cfg': True,
                'cfg_dropout': 0.1
            },
            'data': {
                'max_length': 512
            }
        })
        return config
    
    def _load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        logger.info(f"ğŸ”„ ä» {self.model_path} åŠ è½½æ¨¡å‹...")
        
        # åˆ›å»ºæ¨¡å‹
        model = StructDiff(self.config)
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        if os.path.exists(self.model_path):
            checkpoint = torch.load(self.model_path, map_location=self.device, weights_only=False)
            
            # å¤„ç†ä¸åŒçš„æ£€æŸ¥ç‚¹æ ¼å¼
            if 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            else:
                state_dict = checkpoint
            
            # åŠ è½½çŠ¶æ€å­—å…¸
            model.load_state_dict(state_dict, strict=False)
            logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        else:
            logger.warning(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}")
            logger.warning("å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
        
        model.to(self.device)
        model.eval()
        
        return model
    
    def generate_sequences(
        self,
        peptide_type: str = 'antimicrobial',
        num_samples: int = 10,
        num_inference_steps: int = 20,
        guidance_scale: float = 1.5
    ):
        """ç”Ÿæˆè‚½æ®µåºåˆ—ï¼ˆå›ºå®šé•¿åº¦ï¼‰"""
        logger.info(f"ğŸ§¬ ç”Ÿæˆ {num_samples} ä¸ª {peptide_type} è‚½æ®µ (é•¿åº¦: {self.fixed_length})...")
        
        # å‡†å¤‡æ¡ä»¶
        peptide_type_id = self.peptide_type_map.get(peptide_type, 0)
        
        generated_sequences = []
        
        for i in range(num_samples):
            try:
                # ç”Ÿæˆå•ä¸ªåºåˆ—
                sequence = self._generate_single_sequence(
                    peptide_type_id=peptide_type_id,
                    guidance_scale=guidance_scale,
                    num_inference_steps=num_inference_steps
                )
                
                if sequence:
                    generated_sequences.append(sequence)
                    
                if (i + 1) % 5 == 0:
                    logger.info(f"  è¿›åº¦: {i + 1}/{num_samples}")
                    
            except Exception as e:
                logger.warning(f"ç”Ÿæˆç¬¬ {i+1} ä¸ªåºåˆ—æ—¶å‡ºé”™: {e}")
                continue
        
        logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(generated_sequences)} ä¸ªåºåˆ—")
        return generated_sequences
    
    def _generate_single_sequence(
        self,
        peptide_type_id: int,
        guidance_scale: float = 1.5,
        num_inference_steps: int = 20
    ):
        """ç”Ÿæˆå•ä¸ªåºåˆ—"""
        batch_size = 1
        device = self.device
        
        # å›ºå®šé•¿åº¦ï¼ˆåŒ…æ‹¬CLSå’ŒSEP tokenï¼‰
        total_length = self.fixed_length + 2
        
        # åˆ›å»ºéšæœºå™ªå£°ä½œä¸ºèµ·ç‚¹
        noise_shape = (batch_size, total_length, self.model.seq_hidden_dim)
        x_t = torch.randn(noise_shape, device=device)
        
        # åˆ›å»ºæ³¨æ„åŠ›æ©ç 
        attention_mask = torch.ones(batch_size, total_length, device=device)
        
        # åˆ›å»ºæ¡ä»¶
        conditions = {
            'peptide_type': torch.tensor([peptide_type_id], device=device, dtype=torch.long)
        }
        
        # ç®€åŒ–çš„å»å™ªè¿‡ç¨‹
        num_timesteps = 1000
        timesteps = torch.linspace(num_timesteps - 1, 0, num_inference_steps, dtype=torch.long, device=device)
        
        x = x_t
        for step, t in enumerate(timesteps):
            # åˆ›å»ºæ—¶é—´æ­¥å¼ é‡
            t_batch = torch.full((batch_size,), t, device=device, dtype=torch.long)
            
            # æ¨¡å‹å‰å‘ä¼ æ’­
            with torch.no_grad():
                # ä½¿ç”¨classifier-free guidance
                if guidance_scale > 1.0:
                    # æ— æ¡ä»¶é¢„æµ‹
                    uncond_output = self.model(
                        sequences=torch.zeros(batch_size, total_length, device=device, dtype=torch.long),
                        attention_mask=attention_mask,
                        timesteps=t_batch,
                        conditions=None,
                        return_loss=False
                    )
                    uncond_pred = uncond_output['denoised_embeddings']
                    
                    # æœ‰æ¡ä»¶é¢„æµ‹
                    cond_output = self.model(
                        sequences=torch.zeros(batch_size, total_length, device=device, dtype=torch.long),
                        attention_mask=attention_mask,
                        timesteps=t_batch,
                        conditions=conditions,
                        return_loss=False
                    )
                    cond_pred = cond_output['denoised_embeddings']
                    
                    # åº”ç”¨guidance
                    denoised = uncond_pred + guidance_scale * (cond_pred - uncond_pred)
                else:
                    # ç›´æ¥é¢„æµ‹
                    output = self.model(
                        sequences=torch.zeros(batch_size, total_length, device=device, dtype=torch.long),
                        attention_mask=attention_mask,
                        timesteps=t_batch,
                        conditions=conditions,
                        return_loss=False
                    )
                    denoised = output['denoised_embeddings']
                
                # ç®€åŒ–çš„æ›´æ–°è§„åˆ™
                alpha = 0.98  # ç®€åŒ–çš„å»å™ªæ¯”ä¾‹
                x = alpha * denoised + (1 - alpha) * x
        
        # è§£ç ä¸ºåºåˆ—
        sequences = self.model._decode_embeddings(x, attention_mask)
        
        return sequences[0] if sequences else None
    
    def analyze_sequences(self, sequences, peptide_type):
        """åˆ†æç”Ÿæˆçš„åºåˆ—"""
        if not sequences:
            return {}
        
        analysis = {
            'total_sequences': len(sequences),
            'peptide_type': peptide_type,
            'unique_sequences': len(set(sequences)),
            'average_length': np.mean([len(seq) for seq in sequences]),
            'length_std': np.std([len(seq) for seq in sequences]),
            'min_length': min(len(seq) for seq in sequences),
            'max_length': max(len(seq) for seq in sequences)
        }
        
        # æ°¨åŸºé…¸é¢‘ç‡åˆ†æ
        all_chars = ''.join(sequences)
        char_counts = Counter(all_chars)
        total_chars = len(all_chars)
        
        aa_freq = {}
        for aa in 'ACDEFGHIKLMNPQRSTVWY':
            aa_freq[f'freq_{aa}'] = char_counts.get(aa, 0) / total_chars if total_chars > 0 else 0
        
        analysis.update(aa_freq)
        
        # è®¡ç®—å¤šæ ·æ€§æŒ‡æ ‡
        analysis['uniqueness_ratio'] = len(set(sequences)) / len(sequences)
        
        # è®¡ç®—åºåˆ—å¤æ‚åº¦ï¼ˆä¿¡æ¯ç†µï¼‰
        if total_chars > 0:
            entropy = 0
            for count in char_counts.values():
                prob = count / total_chars
                if prob > 0:
                    entropy -= prob * np.log2(prob)
            analysis['sequence_entropy'] = entropy
        else:
            analysis['sequence_entropy'] = 0
        
        return analysis
    
    def save_sequences(self, sequences, output_file, peptide_type):
        """ä¿å­˜åºåˆ—åˆ°FASTAæ–‡ä»¶"""
        if not sequences:
            logger.warning("æ²¡æœ‰åºåˆ—å¯ä¿å­˜")
            return
        
        records = []
        for i, seq in enumerate(sequences):
            record = SeqRecord(
                Seq(seq),
                id=f"generated_{peptide_type}_{i+1:03d}",
                description=f"Generated {peptide_type} peptide (length: {len(seq)})"
            )
            records.append(record)
        
        SeqIO.write(records, output_file, "fasta")
        logger.info(f"ğŸ’¾ åºåˆ—å·²ä¿å­˜åˆ°: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    setup_logger()
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ¨¡å‹è·¯å¾„
    model_path = "./outputs/structdiff_fixed/best_model.pt"
    
    if not os.path.exists(model_path):
        logger.error(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        logger.info("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ç”Ÿæˆæ¨¡å‹")
        return
    
    # åˆ›å»ºç”Ÿæˆå™¨
    try:
        generator = SimplePeptideGenerator(model_path, device)
    except Exception as e:
        logger.error(f"âŒ åˆ›å»ºç”Ÿæˆå™¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("./test_generation_results")
    output_dir.mkdir(exist_ok=True)
    
    # ç”Ÿæˆå‚æ•°
    generation_params = {
        'num_samples': 10,
        'num_inference_steps': 20,
        'guidance_scale': 1.5
    }
    
    # å¯¹æ¯ç§è‚½ç±»å‹è¿›è¡Œç”Ÿæˆ
    peptide_types = ['antimicrobial', 'antifungal', 'antiviral']
    all_results = {}
    
    for peptide_type in peptide_types:
        logger.info(f"\nğŸ§¬ å¼€å§‹ç”Ÿæˆ {peptide_type} è‚½æ®µ...")
        
        try:
            # ç”Ÿæˆåºåˆ—
            sequences = generator.generate_sequences(
                peptide_type=peptide_type,
                **generation_params
            )
            
            if sequences:
                # åˆ†æåºåˆ—
                analysis = generator.analyze_sequences(sequences, peptide_type)
                all_results[peptide_type] = analysis
                
                # ä¿å­˜åºåˆ—
                fasta_file = output_dir / f"generated_{peptide_type}_simple.fasta"
                generator.save_sequences(sequences, fasta_file, peptide_type)
                
                # æ‰“å°ç¤ºä¾‹åºåˆ—
                logger.info(f"âœ… {peptide_type} ç”Ÿæˆå®Œæˆï¼Œå…± {len(sequences)} ä¸ªåºåˆ—")
                logger.info("ç¤ºä¾‹åºåˆ—:")
                for i, seq in enumerate(sequences):
                    logger.info(f"  {i+1}. {seq} (é•¿åº¦: {len(seq)})")
                
            else:
                logger.warning(f"âš ï¸ {peptide_type} æ²¡æœ‰ç”Ÿæˆä»»ä½•åºåˆ—")
                
        except Exception as e:
            logger.error(f"âŒ {peptide_type} ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # ä¿å­˜åˆ†æç»“æœ
    if all_results:
        results_file = output_dir / "simple_generation_analysis.json"
        with open(results_file, 'w') as f:
            json.dump(all_results, f, indent=2)
        
        # æ‰“å°ç»“æœæ‘˜è¦
        logger.info("\n" + "="*60)
        logger.info("ğŸ¯ ç”Ÿæˆç»“æœæ‘˜è¦")
        logger.info("="*60)
        
        for peptide_type, analysis in all_results.items():
            logger.info(f"\nğŸ“Š {peptide_type.upper()} è‚½æ®µ:")
            logger.info(f"  æ€»åºåˆ—æ•°: {analysis['total_sequences']}")
            logger.info(f"  å”¯ä¸€åºåˆ—: {analysis['unique_sequences']}")
            logger.info(f"  å¹³å‡é•¿åº¦: {analysis['average_length']:.1f} Â± {analysis['length_std']:.1f}")
            logger.info(f"  é•¿åº¦èŒƒå›´: {analysis['min_length']}-{analysis['max_length']}")
            logger.info(f"  å”¯ä¸€æ€§æ¯”ç‡: {analysis['uniqueness_ratio']:.3f}")
            logger.info(f"  åºåˆ—ç†µ: {analysis['sequence_entropy']:.3f}")
        
        logger.info(f"\nğŸ“ è¯¦ç»†ç»“æœä¿å­˜åˆ°: {results_file}")
    
    logger.info("\nğŸ‰ ç®€åŒ–è‚½æ®µç”Ÿæˆæµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main() 