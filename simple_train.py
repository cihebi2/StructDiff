# simple_train.py
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from omegaconf import OmegaConf
from tqdm import tqdm
import os
import sys
import gc

# å¯¼å…¥ ESMFold è¡¥ä¸ä»¥ä¿®å¤å…¼å®¹æ€§é—®é¢˜
from fix_esmfold_patch import apply_esmfold_patch

from structdiff.models.structdiff import StructDiff
from structdiff.data.dataset import PeptideStructureDataset
from structdiff.data.collator import PeptideStructureCollator

def setup_esmfold_patch():
    """è®¾ç½® ESMFold è¡¥ä¸"""
    print("æ­£åœ¨åº”ç”¨ ESMFold å…¼å®¹æ€§è¡¥ä¸...")
    apply_esmfold_patch()
    print("âœ“ ESMFold è¡¥ä¸åº”ç”¨æˆåŠŸ")

def clear_memory():
    """æ¸…ç† GPU å†…å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()

def move_to_device(obj, device):
    """é€’å½’åœ°å°†å¯¹è±¡ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
    if isinstance(obj, torch.Tensor):
        return obj.to(device)
    elif isinstance(obj, dict):
        return {k: move_to_device(v, device) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [move_to_device(item, device) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(move_to_device(item, device) for item in obj)
    else:
        return obj

def train_epoch(model, train_loader, optimizer, device, config):
    model.train()
    total_loss = 0
    total_diffusion_loss = 0
    total_structure_loss = 0
    successful_batches = 0
    
    pbar = tqdm(train_loader, desc="Training")
    for batch_idx, batch_raw in enumerate(pbar):
        try:
            # Move to device - ç¡®ä¿æ‰€æœ‰å¼ é‡éƒ½æ­£ç¡®ç§»åŠ¨åˆ°è®¾å¤‡
            batch = move_to_device(batch_raw, device)
            
            # æ£€æŸ¥batchçš„åŸºæœ¬å­—æ®µ
            if 'sequences' not in batch or 'attention_mask' not in batch:
                print(f"Batch {batch_idx} missing required fields")
                continue
            
            # æ‰“å°è°ƒè¯•ä¿¡æ¯
            print(f"\nBatch {batch_idx} debug info:")
            print(f"  sequences shape: {batch['sequences'].shape}")
            print(f"  attention_mask shape: {batch['attention_mask'].shape}")
            if 'structures' in batch:
                print(f"  structures keys: {list(batch['structures'].keys())}")
                for k, v in batch['structures'].items():
                    if hasattr(v, 'shape'):
                        print(f"    {k} shape: {v.shape}")
            
            # Sample timesteps
            batch_size = batch['sequences'].shape[0]
            timesteps = torch.randint(
                0, config.diffusion.num_timesteps, 
                (batch_size,), device=device
            )
            
            # Prepare structures if using ESMFold
            structures = None
            if config.data.get('use_predicted_structures', False) and 'structures' in batch:
                structures = batch['structures']
            
            # Forward pass
            print(f"  Calling model forward...")
            outputs = model(
                sequences=batch['sequences'],
                attention_mask=batch['attention_mask'],
                timesteps=timesteps,
                structures=structures,
                return_loss=True
            )
            
            loss = outputs['total_loss']
            diffusion_loss = outputs.get('diffusion_loss', torch.tensor(0.0))
            structure_loss = outputs.get('structure_loss', torch.tensor(0.0))
            
            # Scale loss for gradient accumulation
            loss = loss / config.training.gradient_accumulation_steps
            
            # Backward pass
            loss.backward()
            
            # Update weights every gradient_accumulation_steps
            if (batch_idx + 1) % config.training.gradient_accumulation_steps == 0:
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                optimizer.zero_grad()
                
                # Clear memory periodically
                if (batch_idx + 1) % (config.training.gradient_accumulation_steps * 4) == 0:
                    clear_memory()
            
            # Update statistics
            total_loss += loss.item() * config.training.gradient_accumulation_steps
            total_diffusion_loss += diffusion_loss.item()
            total_structure_loss += structure_loss.item()
            successful_batches += 1
            
            # Update progress bar
            pbar.set_postfix({
                'total': f"{loss.item() * config.training.gradient_accumulation_steps:.4f}",
                'diff': f"{diffusion_loss.item():.4f}",
                'struct': f"{structure_loss.item():.4f}",
                'mem': f"{torch.cuda.memory_allocated() / 1e9:.1f}GB" if torch.cuda.is_available() else "CPU"
            })
            
        except Exception as e:
            print(f"Error in batch {batch_idx}: {e}")
            print(f"Error type: {type(e).__name__}")
            import traceback
            traceback.print_exc()
            
            # æ¸…ç†å†…å­˜åç»§ç»­
            optimizer.zero_grad()
            clear_memory()
            continue
    
    # Final gradient update if needed
    if len(train_loader) % config.training.gradient_accumulation_steps != 0:
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        optimizer.zero_grad()
    
    if successful_batches > 0:
        avg_total_loss = total_loss / successful_batches
        avg_diffusion_loss = total_diffusion_loss / successful_batches
        avg_structure_loss = total_structure_loss / successful_batches
    else:
        avg_total_loss = avg_diffusion_loss = avg_structure_loss = 0.0
    
    return {
        'total_loss': avg_total_loss,
        'diffusion_loss': avg_diffusion_loss,
        'structure_loss': avg_structure_loss,
        'successful_batches': successful_batches
    }

def validate_model(model, val_loader, device, config):
    """éªŒè¯æ¨¡å‹æ€§èƒ½"""
    model.eval()
    total_loss = 0
    total_diffusion_loss = 0
    total_structure_loss = 0
    successful_batches = 0
    
    with torch.no_grad():
        pbar = tqdm(val_loader, desc="Validation")
        for batch_raw in pbar:
            try:
                # Move to device - ç¡®ä¿æ‰€æœ‰å¼ é‡éƒ½æ­£ç¡®ç§»åŠ¨åˆ°è®¾å¤‡
                batch = move_to_device(batch_raw, device)
                
                # æ£€æŸ¥batchçš„åŸºæœ¬å­—æ®µ
                if 'sequences' not in batch or 'attention_mask' not in batch:
                    continue
                
                # Sample timesteps
                batch_size = batch['sequences'].shape[0]
                timesteps = torch.randint(
                    0, config.diffusion.num_timesteps, 
                    (batch_size,), device=device
                )
                
                # Prepare structures
                structures = None
                if config.data.get('use_predicted_structures', False) and 'structures' in batch:
                    structures = batch['structures']
                
                # Forward pass
                outputs = model(
                    sequences=batch['sequences'],
                    attention_mask=batch['attention_mask'],
                    timesteps=timesteps,
                    structures=structures,
                    return_loss=True
                )
                
                loss = outputs['total_loss']
                diffusion_loss = outputs.get('diffusion_loss', torch.tensor(0.0))
                structure_loss = outputs.get('structure_loss', torch.tensor(0.0))
                
                total_loss += loss.item()
                total_diffusion_loss += diffusion_loss.item()
                total_structure_loss += structure_loss.item()
                successful_batches += 1
                
                pbar.set_postfix({
                    'val_loss': f"{loss.item():.4f}",
                    'mem': f"{torch.cuda.memory_allocated() / 1e9:.1f}GB" if torch.cuda.is_available() else "CPU"
                })
                
            except Exception as e:
                print(f"Validation error: {e}")
                clear_memory()
                continue
    
    if successful_batches > 0:
        return {
            'total_loss': total_loss / successful_batches,
            'diffusion_loss': total_diffusion_loss / successful_batches,
            'structure_loss': total_structure_loss / successful_batches
        }
    else:
        return {'total_loss': 0.0, 'diffusion_loss': 0.0, 'structure_loss': 0.0}

def main():
    # Setup ESMFold patch first
    setup_esmfold_patch()
    
    # Load minimal config
    config = OmegaConf.load("configs/minimal_test.yaml")
    
    # Setup device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    if torch.cuda.is_available():
        print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        print(f"Available Memory: {torch.cuda.memory_reserved(0) / 1e9:.1f}GB")
    
    # Clear initial memory
    clear_memory()
    
    # Check if ESMFold is enabled
    use_esmfold = config.model.structure_encoder.get('use_esmfold', False)
    use_structures = config.data.get('use_predicted_structures', False)
    
    print(f"ESMFold enabled: {use_esmfold}")
    print(f"Using predicted structures: {use_structures}")
    
    # åˆ›å»ºå…±äº«çš„ ESMFold å®ä¾‹
    shared_esmfold = None
    if use_esmfold and use_structures:
        print("æ­£åœ¨åˆ›å»ºå…±äº«çš„ ESMFold å®ä¾‹...")
        try:
            from structdiff.models.esmfold_wrapper import ESMFoldWrapper
            shared_esmfold = ESMFoldWrapper(device=device)
            if shared_esmfold.available:
                print("âœ“ å…±äº« ESMFold å®ä¾‹åˆ›å»ºæˆåŠŸ")
                print(f"ESMFold åŠ è½½å GPU å†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1e9:.1f}GB" if torch.cuda.is_available() else "")
            else:
                print("âŒ å…±äº« ESMFold å®ä¾‹åˆ›å»ºå¤±è´¥")
                shared_esmfold = None
        except Exception as e:
            print(f"åˆ›å»ºå…±äº« ESMFold å®ä¾‹å¤±è´¥: {e}")
            shared_esmfold = None
    
    if use_esmfold and use_structures and shared_esmfold and shared_esmfold.available:
        print("âœ“ å°†ä½¿ç”¨å…±äº«çš„ ESMFold è¿›è¡Œç»“æ„é¢„æµ‹")
    
    # Create model - ä¸´æ—¶ç¦ç”¨ESMFoldä»¥é¿å…é‡å¤åŠ è½½
    print("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
    try:
        # å¤‡ä»½åŸå§‹é…ç½®
        original_use_esmfold = config.model.structure_encoder.get('use_esmfold', False)
        
        # å¦‚æœå·²æœ‰å…±äº«å®ä¾‹ï¼Œä¸´æ—¶ç¦ç”¨æ¨¡å‹å†…éƒ¨çš„ESMFoldåŠ è½½
        if shared_esmfold and shared_esmfold.available:
            print("ä¸´æ—¶ç¦ç”¨æ¨¡å‹å†…éƒ¨ESMFoldåŠ è½½ä»¥é¿å…å†…å­˜ä¸è¶³...")
            config.model.structure_encoder.use_esmfold = False
        
        model = StructDiff(config).to(device)
        
        # æ¢å¤é…ç½®å¹¶è®¾ç½®å…±äº«å®ä¾‹
        config.model.structure_encoder.use_esmfold = original_use_esmfold
        
        # å¦‚æœæœ‰å…±äº«çš„ESMFoldå®ä¾‹ï¼Œæ‰‹åŠ¨è®¾ç½®åˆ°æ¨¡å‹ä¸­
        if shared_esmfold and shared_esmfold.available:
            print("æ­£åœ¨å°†å…±äº« ESMFold å®ä¾‹è®¾ç½®åˆ°æ¨¡å‹ä¸­...")
            if hasattr(model.structure_encoder, 'esmfold') or hasattr(model.structure_encoder, '_esmfold'):
                # è®¾ç½®ESMFoldå®ä¾‹
                model.structure_encoder.esmfold = shared_esmfold
                model.structure_encoder._esmfold = shared_esmfold
                # ç¡®ä¿ESMFoldè¢«æ ‡è®°ä¸ºå¯ç”¨
                model.structure_encoder.use_esmfold = True
                print("âœ“ å…±äº« ESMFold å®ä¾‹å·²è®¾ç½®åˆ°æ¨¡å‹ä¸­")
            else:
                # å¦‚æœæ¨¡å‹ç»“æ„ä¸åŒï¼Œå°è¯•ç›´æ¥è®¾ç½®å±æ€§
                setattr(model.structure_encoder, 'esmfold', shared_esmfold)
                setattr(model.structure_encoder, 'use_esmfold', True)
                print("âœ“ å…±äº« ESMFold å®ä¾‹å·²å¼ºåˆ¶è®¾ç½®åˆ°æ¨¡å‹ä¸­")
        
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {model.count_parameters():,}")
        
        # Print memory usage after model creation
        if torch.cuda.is_available():
            print(f"æ¨¡å‹åŠ è½½å GPU å†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1e9:.1f}GB")
        
    except Exception as e:
        print(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•å®Œå…¨ç¦ç”¨ESMFold
        print("å°è¯•ç¦ç”¨ESMFoldé‡æ–°åˆå§‹åŒ–æ¨¡å‹...")
        try:
            config.model.structure_encoder.use_esmfold = False
            config.data.use_predicted_structures = False
            model = StructDiff(config).to(device)
            print("âœ“ æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼ˆæœªä½¿ç”¨ESMFoldï¼‰")
            shared_esmfold = None  # æ¸…é™¤å…±äº«å®ä¾‹
        except Exception as e2:
            print(f"ç¦ç”¨ESMFoldåä»ç„¶å¤±è´¥: {e2}")
            return
    
    # Create optimizer with reduced parameters
    trainable_params = filter(lambda p: p.requires_grad, model.parameters())
    optimizer = torch.optim.AdamW(
        trainable_params,
        lr=config.training.optimizer.lr,
        weight_decay=config.training.optimizer.weight_decay
    )
    
    # Create datasets with limited data and shared ESMFold
    print("æ­£åœ¨åŠ è½½æ•°æ®é›†...")
    try:
        train_dataset = PeptideStructureDataset(
            config.data.train_path, 
            config,  # ä½¿ç”¨åŸå§‹é…ç½®
            is_training=True,
            shared_esmfold=shared_esmfold  # ç›´æ¥ä¼ é€’å…±äº«å®ä¾‹
        )
        
        # Limit dataset size for testing
        max_samples = min(20, len(train_dataset))  # å‡å°‘åˆ°20ä¸ªæ ·æœ¬
        train_dataset.data = train_dataset.data.head(max_samples)
        
        # Create validation dataset if available
        val_dataset = None
        if os.path.exists(config.data.get('val_path', '')):
            val_dataset = PeptideStructureDataset(
                config.data.val_path,
                config,  # ä½¿ç”¨åŸå§‹é…ç½®
                is_training=False,
                shared_esmfold=shared_esmfold  # ç›´æ¥ä¼ é€’å…±äº«å®ä¾‹
            )
            
            # Limit validation dataset
            max_val_samples = min(10, len(val_dataset))
            val_dataset.data = val_dataset.data.head(max_val_samples)
        
        print(f"è®­ç»ƒæ•°æ®é›†å¤§å°: {len(train_dataset)}")
        if val_dataset:
            print(f"éªŒè¯æ•°æ®é›†å¤§å°: {len(val_dataset)}")
            
        # æœ€ç»ˆå†…å­˜ä½¿ç”¨æƒ…å†µ
        if torch.cuda.is_available():
            print(f"æ•°æ®é›†åŠ è½½å GPU å†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1e9:.1f}GB")
            
    except Exception as e:
        print(f"æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return
    
    # Create data loaders with smaller batch size
    collator = PeptideStructureCollator(config)
    train_loader = DataLoader(
        train_dataset,
        batch_size=1,  # ä½¿ç”¨batch_size=1æ¥é¿å…å †å é—®é¢˜
        shuffle=True,
        collate_fn=collator,
        num_workers=0,  # åœ¨ Windows ä¸Šä½¿ç”¨ 0 é¿å…é—®é¢˜
        pin_memory=False  # ç¦ç”¨ pin_memory èŠ‚çœå†…å­˜
    )
    
    val_loader = None
    if val_dataset:
        val_loader = DataLoader(
            val_dataset,
            batch_size=1,
            shuffle=False,
            collate_fn=collator,
            num_workers=0,
            pin_memory=False
        )
    
    # Create learning rate scheduler
    scheduler = None
    if config.training.get('scheduler', None):
        from torch.optim.lr_scheduler import CosineAnnealingLR
        scheduler = CosineAnnealingLR(
            optimizer, 
            T_max=config.training.num_epochs,
            eta_min=1e-6
        )
    
    # Training loop
    os.makedirs("checkpoints", exist_ok=True)
    best_val_loss = float('inf')
    
    print(f"\nå¼€å§‹è®­ç»ƒ {config.training.num_epochs} ä¸ª epochs...")
    print(f"æ‰¹é‡å¤§å°: 1, æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {config.training.gradient_accumulation_steps}")
    print(f"æœ‰æ•ˆæ‰¹é‡å¤§å°: {config.training.gradient_accumulation_steps}")
    
    for epoch in range(config.training.num_epochs):
        print(f"\nğŸš€ Epoch {epoch + 1}/{config.training.num_epochs}")
        
        try:
            # Train
            train_metrics = train_epoch(model, train_loader, optimizer, device, config)
            
            print(f"è®­ç»ƒæŸå¤± - æ€»è®¡: {train_metrics['total_loss']:.4f}, "
                  f"æ‰©æ•£: {train_metrics['diffusion_loss']:.4f}, "
                  f"ç»“æ„: {train_metrics['structure_loss']:.4f}, "
                  f"æˆåŠŸæ‰¹æ¬¡: {train_metrics['successful_batches']}/{len(train_loader)}")
            
            # Validate
            if val_loader and (epoch + 1) % config.training.get('validate_every', 5) == 0:
                val_metrics = validate_model(model, val_loader, device, config)
                print(f"éªŒè¯æŸå¤± - æ€»è®¡: {val_metrics['total_loss']:.4f}, "
                      f"æ‰©æ•£: {val_metrics['diffusion_loss']:.4f}, "
                      f"ç»“æ„: {val_metrics['structure_loss']:.4f}")
                
                # Save best model
                if val_metrics['total_loss'] < best_val_loss:
                    best_val_loss = val_metrics['total_loss']
                    checkpoint = {
                        'epoch': epoch,
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'config': config,
                        'val_loss': best_val_loss,
                        'train_metrics': train_metrics,
                        'val_metrics': val_metrics
                    }
                    torch.save(checkpoint, "checkpoints/best_model.pth")
                    print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯æŸå¤±: {best_val_loss:.4f})")
            
            # Update learning rate
            if scheduler:
                scheduler.step()
                current_lr = scheduler.get_last_lr()[0]
                print(f"å­¦ä¹ ç‡: {current_lr:.2e}")
            
            # Memory cleanup
            clear_memory()
            
        except Exception as e:
            print(f"Epoch {epoch + 1} è®­ç»ƒå¤±è´¥: {e}")
            clear_memory()
            continue
    
    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    
    # Try to generate a simple test sample
    print("\næ­£åœ¨æµ‹è¯•ç”ŸæˆåŠŸèƒ½...")
    model.eval()
    with torch.no_grad():
        try:
            samples = model.sample(
                batch_size=2,
                seq_length=10,
                sampling_method='ddpm',
                temperature=1.0,
                progress_bar=True
            )
            
            print("\nç”Ÿæˆçš„æµ‹è¯•åºåˆ—:")
            for i, seq in enumerate(samples['sequences']):
                quality = samples['scores'][i].item()
                print(f"{i+1}: {seq} (è´¨é‡: {quality:.3f})")
                
        except Exception as e:
            print(f"ç”Ÿæˆæµ‹è¯•æ ·æœ¬æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    main()