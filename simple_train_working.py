#!/usr/bin/env python3

import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
from pathlib import Path
import sys
import os
from tqdm import tqdm
import logging

# Add project root to path
sys.path.append('/home/qlyu/sequence/StructDiff-7.0.0')

from structdiff.data.dataset import PeptideStructureDataset
from structdiff.utils.config import load_config
from structdiff.utils.logger import setup_logger, get_logger
from structdiff.models.structdiff import StructDiff
from structdiff.diffusion.gaussian_diffusion import GaussianDiffusion
from torch.utils.data import DataLoader

def custom_collate_fn(batch):
    """è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°ï¼Œå¤„ç†å¯èƒ½çš„ç»“æ„ç‰¹å¾ä¸åŒ¹é…"""
    # æ”¶é›†æ‰€æœ‰é”®
    keys = batch[0].keys()
    result = {}
    
    for key in keys:
        if key == 'structures':
            # è·³è¿‡ç»“æ„ç‰¹å¾ï¼Œé¿å…å½¢çŠ¶ä¸åŒ¹é…é—®é¢˜
            continue
        elif key == 'sequence':
            # å­—ç¬¦ä¸²åˆ—è¡¨
            result[key] = [item[key] for item in batch]
        else:
            # å¼ é‡å †å 
            result[key] = torch.stack([item[key] for item in batch])
    
    return result

def create_model_and_diffusion(config):
    """åˆ›å»ºæ¨¡å‹å’Œæ‰©æ•£è¿‡ç¨‹"""
    # åˆ›å»ºæ¨¡å‹
    model = StructDiff(config.model)
    
    # åˆ›å»ºæ‰©æ•£è¿‡ç¨‹
    diffusion = GaussianDiffusion(
        num_timesteps=config.diffusion.num_timesteps,
        noise_schedule=config.diffusion.noise_schedule,
        beta_start=config.diffusion.beta_start,
        beta_end=config.diffusion.beta_end
    )
    
    return model, diffusion

def training_step(model, diffusion, batch, device):
    """æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤"""
    # Move batch to device
    for key in batch:
        if isinstance(batch[key], torch.Tensor):
            batch[key] = batch[key].to(device)
    
    # Get sequence embeddings
    seq_embeddings = model.sequence_encoder(
        batch['sequences'], 
        attention_mask=batch['attention_mask']
    ).last_hidden_state
    
    # Sample timesteps
    batch_size = seq_embeddings.shape[0]
    timesteps = torch.randint(0, diffusion.num_timesteps, (batch_size,), device=device)
    
    # Add noise
    noise = torch.randn_like(seq_embeddings)
    noisy_embeddings = diffusion.q_sample(seq_embeddings, timesteps, noise)
    
    # Create conditions
    conditions = {'peptide_type': batch['label']}
    
    # Forward pass through denoiser
    predicted_noise, _ = model.denoiser(
        noisy_embeddings,
        timesteps,
        batch['attention_mask'],
        structure_features=None,  # æ˜ç¡®è®¾ç½®ä¸ºNone
        conditions=conditions
    )
    
    # Compute loss
    loss = nn.functional.mse_loss(predicted_noise, noise)
    
    return loss

def simple_train():
    """ç®€åŒ–çš„è®­ç»ƒå‡½æ•°"""
    print("ğŸš€ å¼€å§‹ç®€åŒ–è®­ç»ƒ...")
    
    # Setup logging
    setup_logger(
        level=logging.INFO,
        log_file="/home/qlyu/sequence/StructDiff-7.0.0/outputs/separated_training/simple_training.log"
    )
    logger = get_logger(__name__)
    
    try:
        # Load config
        config_path = "/home/qlyu/sequence/StructDiff-7.0.0/configs/separated_training.yaml"
        config = load_config(config_path)
        
        # å¼ºåˆ¶ç¦ç”¨ç»“æ„é¢„æµ‹
        config.data.use_predicted_structures = False
        
        logger.info("âœ… é…ç½®åŠ è½½æˆåŠŸï¼Œå·²ç¦ç”¨ç»“æ„é¢„æµ‹")
        
        # Create dataset
        dataset = PeptideStructureDataset(
            data_path="/home/qlyu/sequence/StructDiff-7.0.0/data/processed/train.csv",
            config=config,
            is_training=True
        )
        
        logger.info(f"âœ… æ•°æ®é›†åˆ›å»ºæˆåŠŸï¼Œå¤§å°: {len(dataset)}")
        
        # Create dataloader with custom collate function
        dataloader = DataLoader(
            dataset, 
            batch_size=8,  # è¾ƒå°çš„æ‰¹æ¬¡å¤§å°
            shuffle=True,
            collate_fn=custom_collate_fn,
            num_workers=0  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
        )
        
        logger.info("âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        
        # Create model and diffusion
        model, diffusion = create_model_and_diffusion(config)
        logger.info(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
        
        # Move to GPU (when CUDA_VISIBLE_DEVICES=1, the visible device index is 0)
        device = torch.device('cuda:0')
        model = model.to(device)
        logger.info("âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°GPU")
        
        # Create optimizer
        optimizer = optim.AdamW(model.parameters(), lr=1e-4)
        
        # Training loop
        model.train()
        num_epochs = 5  # å°‘é‡epochç”¨äºæµ‹è¯•
        
        logger.info(f"ğŸ¯ å¼€å§‹è®­ç»ƒï¼Œå…± {num_epochs} ä¸ªepoch")
        
        for epoch in range(num_epochs):
            epoch_loss = 0.0
            num_batches = 0
            
            progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{num_epochs}")
            
            for batch_idx, batch in enumerate(progress_bar):
                optimizer.zero_grad()
                
                # Training step
                loss = training_step(model, diffusion, batch, device)
                
                # Backward pass
                loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                # Optimizer step
                optimizer.step()
                
                # Update metrics
                epoch_loss += loss.item()
                num_batches += 1
                
                # Update progress bar
                progress_bar.set_postfix({
                    'loss': f'{loss.item():.6f}',
                    'avg_loss': f'{epoch_loss/num_batches:.6f}'
                })
                
                # Log every 10 batches
                if batch_idx % 10 == 0:
                    logger.info(f"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.6f}")
                
                # Early break for testing (remove this for full training)
                if batch_idx >= 50:  # åªè®­ç»ƒ50ä¸ªæ‰¹æ¬¡ç”¨äºæµ‹è¯•
                    break
            
            avg_loss = epoch_loss / num_batches
            logger.info(f"Epoch {epoch+1} å®Œæˆï¼Œå¹³å‡æŸå¤±: {avg_loss:.6f}")
            
            # Save checkpoint
            if (epoch + 1) % 2 == 0:
                checkpoint_path = f"/home/qlyu/sequence/StructDiff-7.0.0/outputs/separated_training/checkpoint_epoch_{epoch+1}.pt"
                torch.save({
                    'epoch': epoch + 1,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'loss': avg_loss,
                }, checkpoint_path)
                logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
        
        logger.info("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        
        # Save final model
        final_model_path = "/home/qlyu/sequence/StructDiff-7.0.0/outputs/separated_training/final_model.pt"
        torch.save(model.state_dict(), final_model_path)
        logger.info(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise

if __name__ == "__main__":
    # Create output directory
    os.makedirs("/home/qlyu/sequence/StructDiff-7.0.0/outputs/separated_training", exist_ok=True)
    
    # Run training
    simple_train() 