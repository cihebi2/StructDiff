#!/usr/bin/env python3
"""
CPL-Diffå¯å‘çš„åˆ†ç¦»å¼è®­ç»ƒç­–ç•¥
åŸºäºCPL-Diffçš„ä¸¤é˜¶æ®µè®­ç»ƒè®¾è®¡ï¼Œå°†å»å™ªå™¨è®­ç»ƒå’Œåºåˆ—è§£ç åˆ†ç¦»
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from typing import Dict, Optional, Tuple, Any, List
import logging
import os
from pathlib import Path
import json
from dataclasses import dataclass
from tqdm import tqdm
import numpy as np
from omegaconf import DictConfig

from ..models.structdiff import StructDiff
from ..diffusion.gaussian_diffusion import GaussianDiffusion
from ..utils.logger import get_logger
from ..utils.checkpoint import CheckpointManager
from ..utils.ema import EMA

# ä¸´æ—¶è®¾ç½®è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


@dataclass
class SeparatedTrainingConfig:
    """åˆ†ç¦»å¼è®­ç»ƒé…ç½®"""
    # é˜¶æ®µ1: å»å™ªå™¨è®­ç»ƒé…ç½®
    stage1_epochs: int = 200
    stage1_lr: float = 1e-4
    stage1_batch_size: int = 32
    stage1_gradient_clip: float = 1.0
    stage1_warmup_steps: int = 1000
    
    # é˜¶æ®µ2: è§£ç å™¨è®­ç»ƒé…ç½®  
    stage2_epochs: int = 100
    stage2_lr: float = 5e-5
    stage2_batch_size: int = 64
    stage2_gradient_clip: float = 0.5
    stage2_warmup_steps: int = 500
    
    # å…±åŒé…ç½®
    use_amp: bool = True
    use_ema: bool = True
    ema_decay: float = 0.9999
    save_every: int = 1000
    validate_every: int = 500
    log_every: int = 100
    
    # é•¿åº¦æ§åˆ¶
    use_length_control: bool = True
    max_length: int = 50
    min_length: int = 5
    
    # æ¡ä»¶æ§åˆ¶
    use_cfg: bool = True
    cfg_dropout_prob: float = 0.1
    cfg_guidance_scale: float = 2.0
    
    # æ•°æ®è·¯å¾„
    data_dir: str = "./data/processed"
    output_dir: str = "./outputs/separated_training"
    checkpoint_dir: str = "./checkpoints/separated"
    
    # è¯„ä¼°é…ç½®
    enable_evaluation: bool = True
    evaluate_every: int = 5
    evaluation_metrics: Optional[List[str]] = None
    evaluation_num_samples: int = 1000
    evaluation_guidance_scale: float = 2.0
    auto_generate_after_training: bool = True
    
    def __post_init__(self):
        if self.evaluation_metrics is None:
            self.evaluation_metrics = [
                "pseudo_perplexity",
                "plddt_score", 
                "instability_index",
                "similarity_score",
                "activity_prediction"
            ]


class SeparatedTrainingManager:
    """åˆ†ç¦»å¼è®­ç»ƒç®¡ç†å™¨"""
    
    def __init__(self, 
                 config: SeparatedTrainingConfig,
                 model: StructDiff,
                 diffusion: GaussianDiffusion,
                 device: str = 'cuda',
                 tokenizer=None):
        self.config = config
        self.model = model
        self.diffusion = diffusion
        self.device = device
        self.tokenizer = tokenizer
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(config.output_dir).mkdir(parents=True, exist_ok=True)
        Path(config.checkpoint_dir).mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.checkpoint_manager = CheckpointManager(config.checkpoint_dir)
        self.training_stats = {
            'stage1': {'losses': [], 'val_losses': [], 'evaluations': []},
            'stage2': {'losses': [], 'val_losses': [], 'evaluations': []},
        }
        
        # EMA
        if config.use_ema:
            self.ema = EMA(model, decay=config.ema_decay)
        else:
            self.ema = None
            
        # è¯„ä¼°å™¨ (å»¶è¿Ÿåˆå§‹åŒ–)
        self.evaluator = None
        if config.enable_evaluation:
            self._init_evaluator()
            
        logger.info(f"åˆå§‹åŒ–åˆ†ç¦»å¼è®­ç»ƒç®¡ç†å™¨ï¼Œè®¾å¤‡: {device}")
        if config.enable_evaluation:
            logger.info(f"è¯„ä¼°æŒ‡æ ‡: {config.evaluation_metrics}")
    
    def _init_evaluator(self):
        """åˆå§‹åŒ–CPL-Diffè¯„ä¼°å™¨"""
        try:
            # åŠ¨æ€å¯¼å…¥è¯„ä¼°å™¨
            import sys
            from pathlib import Path
            project_root = Path(__file__).parent.parent.parent
            sys.path.insert(0, str(project_root))
            
            from scripts.cpldiff_standard_evaluation import CPLDiffStandardEvaluator
            
            eval_output_dir = Path(self.config.output_dir) / "evaluations"
            self.evaluator = CPLDiffStandardEvaluator(output_dir=str(eval_output_dir))
            logger.info("âœ“ CPL-Diffè¯„ä¼°å™¨åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.warning(f"è¯„ä¼°å™¨åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†è·³è¿‡è¯„ä¼°")
            self.evaluator = None
            self.config.enable_evaluation = False
    
    def prepare_stage1_components(self) -> Tuple[nn.Module, torch.optim.Optimizer, Any]:
        """å‡†å¤‡é˜¶æ®µ1è®­ç»ƒç»„ä»¶ï¼ˆå»å™ªå™¨è®­ç»ƒï¼‰"""
        # å†»ç»“åºåˆ—ç¼–ç å™¨
        for param in self.model.sequence_encoder.parameters():
            param.requires_grad = False
        
        # åªè®­ç»ƒå»å™ªå™¨å’Œç»“æ„ç¼–ç å™¨
        trainable_params = []
        for name, param in self.model.named_parameters():
            if 'sequence_encoder' not in name:
                param.requires_grad = True
                trainable_params.append(param)
            else:
                param.requires_grad = False
        
        logger.info(f"é˜¶æ®µ1å¯è®­ç»ƒå‚æ•°æ•°é‡: {sum(p.numel() for p in trainable_params)}")
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(
            trainable_params,
            lr=self.config.stage1_lr,
            weight_decay=0.01,
            betas=(0.9, 0.999)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=self.config.stage1_epochs,
            eta_min=1e-6
        )
        
        return self.model, optimizer, scheduler
    
    def prepare_stage2_components(self) -> Tuple[nn.Module, torch.optim.Optimizer, Any]:
        """å‡†å¤‡é˜¶æ®µ2è®­ç»ƒç»„ä»¶ï¼ˆè§£ç å™¨è®­ç»ƒï¼‰"""
        # å†»ç»“å»å™ªå™¨å’Œç»“æ„ç¼–ç å™¨ï¼Œåªè®­ç»ƒè§£ç å™¨ç›¸å…³å‚æ•°
        for name, param in self.model.named_parameters():
            # ç¬¬äºŒé˜¶æ®µè®­ç»ƒè§£ç å™¨ç›¸å…³çš„å‚æ•°
            if any(decoder_key in name for decoder_key in ['decode_projection', 'decoder_layers']):
                param.requires_grad = True
            else:
                param.requires_grad = False
        
        # æ”¶é›†åºåˆ—è§£ç å™¨å‚æ•°
        decoder_params = []
        for name, param in self.model.named_parameters():
            if any(decoder_key in name for decoder_key in ['decode_projection', 'decoder_layers']) and param.requires_grad:
                decoder_params.append(param)
                logger.info(f"é˜¶æ®µ2è®­ç»ƒå‚æ•°: {name} - {param.shape}")
        
        total_params = sum(p.numel() for p in decoder_params)
        logger.info(f"é˜¶æ®µ2å¯è®­ç»ƒå‚æ•°æ•°é‡: {total_params:,}")
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(
            decoder_params,
            lr=self.config.stage2_lr,
            weight_decay=0.01,
            betas=(0.9, 0.999)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=self.config.stage2_epochs,
            eta_min=1e-6
        )
        
        return self.model, optimizer, scheduler
    
    def stage1_training_step(self, 
                           batch: Dict[str, torch.Tensor],
                           model: nn.Module,
                           optimizer: torch.optim.Optimizer) -> Dict[str, float]:
        """é˜¶æ®µ1è®­ç»ƒæ­¥éª¤ï¼šå»å™ªå™¨è®­ç»ƒ"""
        model.train()
        
        # å‡†å¤‡æ•°æ®
        sequences = batch['sequences'].to(self.device)
        attention_mask = batch['attention_mask'].to(self.device)
        
        # Process structure features through the encoder
        structures = batch.get('structures', None)
        if structures is not None and hasattr(model, 'structure_encoder'):
            # Move all tensors in the structures dict to the correct device
            for k, v in structures.items():
                if isinstance(v, torch.Tensor):
                    structures[k] = v.to(self.device)
            processed_structures = model.structure_encoder(structures, attention_mask)
        else:
            processed_structures = None
        
        conditions = batch.get('conditions', None)
        
        # è·å–å›ºå®šçš„åºåˆ—åµŒå…¥ï¼ˆä¸è®¡ç®—æ¢¯åº¦ï¼‰
        with torch.no_grad():
            seq_embeddings = model.sequence_encoder(sequences, attention_mask)
            if hasattr(seq_embeddings, 'last_hidden_state'):
                seq_embeddings = seq_embeddings.last_hidden_state
        
        # éšæœºæ—¶é—´æ­¥
        batch_size = sequences.shape[0]
        timesteps = torch.randint(
            0, self.diffusion.num_timesteps, 
            (batch_size,), device=self.device
        )
        
        # æ·»åŠ å™ªå£°
        noise = torch.randn_like(seq_embeddings)
        noisy_embeddings = self.diffusion.q_sample(seq_embeddings, timesteps, noise)
        
        # å¤„ç†æ¡ä»¶ï¼ˆæ”¯æŒCFGï¼‰
        if self.config.use_cfg and conditions is not None:
            # éšæœºä¸¢å¼ƒéƒ¨åˆ†æ¡ä»¶
            dropout_mask = torch.rand(batch_size) < self.config.cfg_dropout_prob
            for key, value in conditions.items():
                if isinstance(value, torch.Tensor):
                    conditions[key] = value.clone()
                    conditions[key][dropout_mask] = -1  # æ— æ¡ä»¶æ ‡è®°
        
        # å»å™ªé¢„æµ‹
        if hasattr(model, 'denoiser'):
            predicted_noise_output = model.denoiser(
                noisy_embeddings, timesteps, attention_mask,
                structure_features=processed_structures, conditions=conditions
            )
            # å»å™ªå™¨è¿”å›tuple (predicted_noise, cross_attention_weights)
            if isinstance(predicted_noise_output, tuple):
                predicted_noise, cross_attention_weights = predicted_noise_output
            else:
                predicted_noise = predicted_noise_output
        else:
            predicted_noise_output = model(
                noisy_embeddings, timesteps, attention_mask,
                structure_features=processed_structures, conditions=conditions
            )
            # å¤„ç†å¯èƒ½çš„tupleè¿”å›å€¼
            if isinstance(predicted_noise_output, tuple):
                predicted_noise, _ = predicted_noise_output
            else:
                predicted_noise = predicted_noise_output
        
        # è®¡ç®—æŸå¤±ï¼ˆé¢„æµ‹å™ªå£°ï¼‰
        loss = F.mse_loss(predicted_noise, noise)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        if self.config.use_amp:
            from torch.cuda.amp import autocast, GradScaler
            scaler = GradScaler()
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(
                model.parameters(), self.config.stage1_gradient_clip
            )
            scaler.step(optimizer)
            scaler.update()
        else:
            loss.backward()
            torch.nn.utils.clip_grad_norm_(
                model.parameters(), self.config.stage1_gradient_clip
            )
            optimizer.step()
        
        # EMAæ›´æ–°
        if self.ema is not None:
            self.ema.update()
        
        return {
            'loss': loss.item(),
            'lr': optimizer.param_groups[0]['lr']
        }
    
    def stage2_training_step(self,
                           batch: Dict[str, torch.Tensor],
                           model: nn.Module,
                           optimizer: torch.optim.Optimizer) -> Dict[str, float]:
        """é˜¶æ®µ2è®­ç»ƒæ­¥éª¤ï¼šè§£ç å™¨è®­ç»ƒ"""
        model.train()
        
        # å‡†å¤‡æ•°æ®
        sequences = batch['sequences'].to(self.device)
        attention_mask = batch['attention_mask'].to(self.device)
        
        # è·å–å¹²å‡€çš„åºåˆ—åµŒå…¥ï¼ˆå›ºå®šç¼–ç å™¨ï¼‰
        with torch.no_grad():
            seq_embeddings = model.sequence_encoder(sequences, attention_mask)
            if hasattr(seq_embeddings, 'last_hidden_state'):
                seq_embeddings = seq_embeddings.last_hidden_state
        
        # åºåˆ—è§£ç ï¼šç›´æ¥ä½¿ç”¨è§£ç æŠ•å½±å±‚
        # ç§»é™¤CLSå’ŒSEP tokensä»¥åŒ¹é…åŸå§‹åºåˆ—é•¿åº¦
        if seq_embeddings.shape[1] == attention_mask.shape[1]:
            # å¦‚æœé•¿åº¦åŒ¹é…ï¼Œéœ€è¦ç§»é™¤CLSå’ŒSEP tokens
            seq_embeddings_trimmed = seq_embeddings[:, 1:-1, :]  # ç§»é™¤é¦–å°¾ç‰¹æ®Štokens
            attention_mask_trimmed = attention_mask[:, 1:-1]
        else:
            # å¦‚æœé•¿åº¦å·²ç»åŒ¹é…ï¼Œç›´æ¥ä½¿ç”¨
            seq_embeddings_trimmed = seq_embeddings
            attention_mask_trimmed = attention_mask
        
        # åº”ç”¨å¯é€‰çš„è§£ç å™¨å±‚
        if hasattr(model, 'decoder_layers') and model.decoder_layers is not None:
            memory_mask = ~attention_mask_trimmed.bool()
            decoded_embeddings = model.decoder_layers(
                seq_embeddings_trimmed,
                seq_embeddings_trimmed,
                tgt_key_padding_mask=memory_mask,
                memory_key_padding_mask=memory_mask
            )
        else:
            decoded_embeddings = seq_embeddings_trimmed
        
        # æŠ•å½±åˆ°è¯æ±‡è¡¨ç»´åº¦
        logits = model.decode_projection(decoded_embeddings)  # (B, L, V)
        
        # è®¡ç®—äº¤å‰ç†µæŸå¤±
        # åŒæ ·ç§»é™¤ç›®æ ‡åºåˆ—çš„CLSå’ŒSEP tokens
        if sequences.shape[1] == attention_mask.shape[1]:
            targets_trimmed = sequences[:, 1:-1]  # ç§»é™¤CLSå’ŒSEP tokens
        else:
            targets_trimmed = sequences
        
        vocab_size = logits.size(-1)
        flat_logits = logits.reshape(-1, vocab_size)
        flat_targets = targets_trimmed.reshape(-1)
        
        # å¿½ç•¥paddingä½ç½®
        if attention_mask_trimmed is not None:
            flat_mask = attention_mask_trimmed.reshape(-1).bool()
            flat_logits = flat_logits[flat_mask]
            flat_targets = flat_targets[flat_mask]
        
        loss = F.cross_entropy(flat_logits, flat_targets)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(
            model.parameters(), self.config.stage2_gradient_clip
        )
        optimizer.step()
        
        return {
            'loss': loss.item(),
            'lr': optimizer.param_groups[0]['lr']
        }
    
    def validate_stage1(self, val_loader: DataLoader, model: nn.Module) -> Dict[str, float]:
        """é˜¶æ®µ1éªŒè¯"""
        model.eval()
        total_loss = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                sequences = batch['sequences'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                
                # Process structure features through the encoder
                structures = batch.get('structures', None)
                if structures is not None and hasattr(model, 'structure_encoder'):
                    # Move all tensors in the structures dict to the correct device
                    for k, v in structures.items():
                        if isinstance(v, torch.Tensor):
                            structures[k] = v.to(self.device)
                    processed_structures = model.structure_encoder(structures, attention_mask)
                else:
                    processed_structures = None
                
                conditions = batch.get('conditions', None)
                
                # è·å–åºåˆ—åµŒå…¥
                seq_embeddings = model.sequence_encoder(sequences, attention_mask)
                if hasattr(seq_embeddings, 'last_hidden_state'):
                    seq_embeddings = seq_embeddings.last_hidden_state
                
                # éšæœºæ—¶é—´æ­¥å’Œå™ªå£°
                batch_size = sequences.shape[0]
                timesteps = torch.randint(
                    0, self.diffusion.num_timesteps,
                    (batch_size,), device=self.device
                )
                noise = torch.randn_like(seq_embeddings)
                noisy_embeddings = self.diffusion.q_sample(seq_embeddings, timesteps, noise)
                
                # é¢„æµ‹
                if hasattr(model, 'denoiser'):
                    predicted_noise_output = model.denoiser(
                        noisy_embeddings, timesteps, attention_mask,
                        structure_features=processed_structures, conditions=conditions
                    )
                    # å¤„ç†tupleè¿”å›å€¼
                    if isinstance(predicted_noise_output, tuple):
                        predicted_noise, _ = predicted_noise_output
                    else:
                        predicted_noise = predicted_noise_output
                else:
                    predicted_noise_output = model(
                        noisy_embeddings, timesteps, attention_mask,
                        structure_features=processed_structures, conditions=conditions
                    )
                    # å¤„ç†tupleè¿”å›å€¼
                    if isinstance(predicted_noise_output, tuple):
                        predicted_noise, _ = predicted_noise_output
                    else:
                        predicted_noise = predicted_noise_output
                
                loss = F.mse_loss(predicted_noise, noise)
                total_loss += loss.item()
                num_batches += 1
        
        return {'val_loss': total_loss / max(num_batches, 1)}
    
    def validate_stage2(self, val_loader: DataLoader, model: nn.Module) -> Dict[str, float]:
        """é˜¶æ®µ2éªŒè¯"""
        model.eval()
        total_loss = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                sequences = batch['sequences'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                
                # è·å–åºåˆ—åµŒå…¥
                seq_embeddings = model.sequence_encoder(sequences, attention_mask)
                if hasattr(seq_embeddings, 'last_hidden_state'):
                    seq_embeddings = seq_embeddings.last_hidden_state
                
                # åºåˆ—è§£ç ï¼šç›´æ¥ä½¿ç”¨è§£ç æŠ•å½±å±‚
                # ç§»é™¤CLSå’ŒSEP tokensä»¥åŒ¹é…åŸå§‹åºåˆ—é•¿åº¦
                if seq_embeddings.shape[1] == attention_mask.shape[1]:
                    # å¦‚æœé•¿åº¦åŒ¹é…ï¼Œéœ€è¦ç§»é™¤CLSå’ŒSEP tokens
                    seq_embeddings_trimmed = seq_embeddings[:, 1:-1, :]
                    attention_mask_trimmed = attention_mask[:, 1:-1]
                else:
                    # å¦‚æœé•¿åº¦å·²ç»åŒ¹é…ï¼Œç›´æ¥ä½¿ç”¨
                    seq_embeddings_trimmed = seq_embeddings
                    attention_mask_trimmed = attention_mask
                
                # åº”ç”¨å¯é€‰çš„è§£ç å™¨å±‚
                if hasattr(model, 'decoder_layers') and model.decoder_layers is not None:
                    memory_mask = ~attention_mask_trimmed.bool()
                    decoded_embeddings = model.decoder_layers(
                        seq_embeddings_trimmed,
                        seq_embeddings_trimmed,
                        tgt_key_padding_mask=memory_mask,
                        memory_key_padding_mask=memory_mask
                    )
                else:
                    decoded_embeddings = seq_embeddings_trimmed
                
                # æŠ•å½±åˆ°è¯æ±‡è¡¨ç»´åº¦
                logits = model.decode_projection(decoded_embeddings)
                
                # è®¡ç®—æŸå¤±
                # åŒæ ·ç§»é™¤ç›®æ ‡åºåˆ—çš„CLSå’ŒSEP tokens
                if sequences.shape[1] == attention_mask.shape[1]:
                    targets_trimmed = sequences[:, 1:-1]
                else:
                    targets_trimmed = sequences
                
                vocab_size = logits.size(-1)
                flat_logits = logits.reshape(-1, vocab_size)
                flat_targets = targets_trimmed.reshape(-1)
                
                if attention_mask_trimmed is not None:
                    flat_mask = attention_mask_trimmed.reshape(-1).bool()
                    flat_logits = flat_logits[flat_mask]
                    flat_targets = flat_targets[flat_mask]
                
                loss = F.cross_entropy(flat_logits, flat_targets)
                total_loss += loss.item()
                num_batches += 1
        
        return {'val_loss': total_loss / max(num_batches, 1)}
    
    def train_stage1(self, 
                    train_loader: DataLoader, 
                    val_loader: Optional[DataLoader] = None) -> Dict[str, List[float]]:
        """æ‰§è¡Œé˜¶æ®µ1è®­ç»ƒï¼šå»å™ªå™¨è®­ç»ƒ"""
        logger.info("ğŸš€ å¼€å§‹é˜¶æ®µ1è®­ç»ƒï¼šå»å™ªå™¨è®­ç»ƒï¼ˆå›ºå®šESMç¼–ç å™¨ï¼‰")
        
        model, optimizer, scheduler = self.prepare_stage1_components()

        stage1_stats = {'losses': [], 'val_losses': [], 'evaluations': []}

        for epoch in range(self.config.stage1_epochs):
            # è®­ç»ƒå¾ªç¯
            model.train()
            epoch_losses = []

            pbar = tqdm(train_loader, desc=f"é˜¶æ®µ1 Epoch {epoch+1}/{self.config.stage1_epochs}")
            for step, batch in enumerate(pbar):
                step_stats = self.stage1_training_step(batch, model, optimizer)
                epoch_losses.append(step_stats['loss'])

                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'loss': f"{step_stats['loss']:.4f}",
                    'lr': f"{step_stats['lr']:.2e}"
                })

                # æ—¥å¿—è®°å½•
                if step % self.config.log_every == 0:
                    logger.info(f"é˜¶æ®µ1 Epoch {epoch+1}, Step {step}: "
                              f"Loss = {step_stats['loss']:.4f}, "
                              f"LR = {step_stats['lr']:.2e}")

                # --- éªŒè¯å’Œè¯„ä¼° ---
                if val_loader and step > 0 and step % self.config.validate_every == 0:
                    # åº”ç”¨EMAè¿›è¡Œè¯„ä¼°
                    if self.ema:
                        self.ema.apply_shadow()

                    val_stats = self.validate_stage1(val_loader, model)
                    stage1_stats['val_losses'].append(val_stats['val_loss'])
                    logger.info(f"é˜¶æ®µ1éªŒè¯ - Val Loss: {val_stats['val_loss']:.4f}")

                    # æ¢å¤æ¨¡å‹å‚æ•°
                    if self.ema:
                        self.ema.restore()

                # --- å®šæœŸè¯„ä¼° ---
                if (self.config.enable_evaluation and self.evaluator is not None and
                        epoch > 0 and epoch % self.config.evaluate_every == 0 and
                        step == len(pbar) - 1):  # åœ¨epochæœ«å°¾è¯„ä¼°
                    logger.info(f"ğŸ”¬ æ‰§è¡Œé˜¶æ®µ1å®šæœŸè¯„ä¼° (Epoch {epoch+1})...")
                    if self.ema:
                        self.ema.apply_shadow()
                    
                    eval_results = self._run_evaluation(model, stage=f'stage1_epoch_{epoch+1}')
                    if eval_results:
                        self.training_stats['stage1']['evaluations'].append({
                            'epoch': epoch + 1,
                            'results': eval_results
                        })
                    
                    if self.ema:
                        self.ema.restore()

                # --- ä¿å­˜æ£€æŸ¥ç‚¹ ---
                if step > 0 and step % self.config.save_every == 0:
                    # ä¿å­˜EMAæ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if self.ema:
                        self.ema.apply_shadow()

                    checkpoint_state = {
                        'model': model.state_dict(),
                        'optimizer': optimizer.state_dict(),
                        'scheduler': scheduler.state_dict(),
                        'epoch': epoch,
                        'step': step,
                        'stage': 'stage1',
                        'stats': stage1_stats,
                        'config': self.config
                    }
                    self.checkpoint_manager.save_checkpoint(
                        state_dict=checkpoint_state,
                        epoch=epoch,
                        is_best=(step % 5000 == 0)  # æ¯5000æ­¥æ ‡è®°ä¸€æ¬¡best
                    )

                    # æ¢å¤æ¨¡å‹å‚æ•°
                    if self.ema:
                        self.ema.restore()

            # è®°å½•epochç»Ÿè®¡
            epoch_loss = sum(epoch_losses) / len(epoch_losses)
            self.training_stats['stage1']['losses'].append(epoch_loss)

            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step()

            logger.info(f"é˜¶æ®µ1 Epoch {epoch+1} å®Œæˆ - å¹³å‡Loss: {epoch_loss:.4f}")

        # --- æœ€ç»ˆæ¨¡å‹ä¿å­˜ ---
        logger.info("ğŸ’¾ ä¿å­˜æœ€ç»ˆé˜¶æ®µ1æ¨¡å‹...")
        if self.ema:
            self.ema.apply_shadow()

        # å‡†å¤‡æœ€ç»ˆæ£€æŸ¥ç‚¹çŠ¶æ€å­—å…¸
        final_checkpoint_state = {
            'model': model.state_dict(),
            'optimizer': optimizer.state_dict(),
            'scheduler': scheduler.state_dict(),
            'epoch': self.config.stage1_epochs,
            'stage': 'stage1_final',
            'stats': self.training_stats['stage1'],
            'config': self.config
        }

        self.checkpoint_manager.save_checkpoint(
            state_dict=final_checkpoint_state,
            epoch=self.config.stage1_epochs,
            is_best=True  # æ ‡è®°ä¸ºæœ€ä½³æ¨¡å‹
        )

        # --- æœ€ç»ˆè¯„ä¼° ---
        if self.config.enable_evaluation and self.evaluator is not None:
            logger.info("ğŸ”¬ æ‰§è¡Œé˜¶æ®µ1ç»“æŸè¯„ä¼°...")
            # EMAå·²ç»åº”ç”¨ï¼Œç›´æ¥ä½¿ç”¨model
            eval_results = self._run_evaluation(model, stage='stage1_final')
            if eval_results:
                self.training_stats['stage1']['final_evaluation'] = eval_results

        # æ¢å¤æ¨¡å‹å‚æ•°ä»¥å¤‡åç»­æ“ä½œ
        if self.ema:
            self.ema.restore()
        
        logger.info("âœ… é˜¶æ®µ1è®­ç»ƒå®Œæˆ")
        self.training_stats['stage1'] = stage1_stats
        return stage1_stats
    
    def train_stage2(self,
                    train_loader: DataLoader,
                    val_loader: Optional[DataLoader] = None) -> Dict[str, List[float]]:
        """æ‰§è¡Œé˜¶æ®µ2è®­ç»ƒï¼šè§£ç å™¨è®­ç»ƒ"""
        logger.info("ğŸš€ å¼€å§‹é˜¶æ®µ2è®­ç»ƒï¼šè§£ç å™¨è®­ç»ƒï¼ˆå›ºå®šå»å™ªå™¨ï¼‰")
        
        model, optimizer, scheduler = self.prepare_stage2_components()
        
        stage2_stats = {'losses': [], 'val_losses': [], 'evaluations': []}
        
        for epoch in range(self.config.stage2_epochs):
            # è®­ç»ƒå¾ªç¯
            model.train()
            epoch_losses = []
            
            pbar = tqdm(train_loader, desc=f"é˜¶æ®µ2 Epoch {epoch+1}/{self.config.stage2_epochs}")
            for step, batch in enumerate(pbar):
                step_stats = self.stage2_training_step(batch, model, optimizer)
                epoch_losses.append(step_stats['loss'])
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'loss': f"{step_stats['loss']:.4f}",
                    'lr': f"{step_stats['lr']:.2e}"
                })
                
                # æ—¥å¿—è®°å½•
                if step % self.config.log_every == 0:
                    logger.info(f"é˜¶æ®µ2 Epoch {epoch+1}, Step {step}: "
                              f"Loss = {step_stats['loss']:.4f}, "
                              f"LR = {step_stats['lr']:.2e}")
                
                # éªŒè¯
                if val_loader and step % self.config.validate_every == 0:
                    val_stats = self.validate_stage2(val_loader, model)
                    stage2_stats['val_losses'].append(val_stats['val_loss'])
                    logger.info(f"é˜¶æ®µ2éªŒè¯ - Val Loss: {val_stats['val_loss']:.4f}")
                    
                    # å®šæœŸè¯„ä¼°
                    if (self.config.enable_evaluation and self.evaluator is not None and 
                        epoch > 0 and epoch % self.config.evaluate_every == 0):
                        logger.info(f"ğŸ”¬ æ‰§è¡Œé˜¶æ®µ2å®šæœŸè¯„ä¼° (Epoch {epoch+1})...")
                        eval_results = self._run_evaluation(model, stage=f'stage2_epoch_{epoch+1}')
                        if eval_results:
                            stage2_stats['evaluations'].append({
                                'epoch': epoch + 1,  
                                'step': step,
                                'results': eval_results
                            })
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if step % self.config.save_every == 0:
                    checkpoint_state = {
                        'model': model.state_dict(),
                        'optimizer': optimizer.state_dict(),
                        'scheduler': scheduler.state_dict(),
                        'epoch': epoch,
                        'step': step,
                        'stage': 'stage2',
                        'stats': stage2_stats
                    }
                    self.checkpoint_manager.save_checkpoint(
                        state_dict=checkpoint_state,
                        epoch=epoch,
                        is_best=False
                    )
            
            # è®°å½•epochç»Ÿè®¡
            epoch_loss = sum(epoch_losses) / len(epoch_losses)
            stage2_stats['losses'].append(epoch_loss)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step()
            
            logger.info(f"é˜¶æ®µ2 Epoch {epoch+1} å®Œæˆ - å¹³å‡Loss: {epoch_loss:.4f}")
        
        # ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹
        final_checkpoint_state = {
            'model': model.state_dict(),
            'optimizer': optimizer.state_dict(),
            'scheduler': scheduler.state_dict(),
            'epoch': self.config.stage2_epochs,
            'step': 0,
            'stage': 'stage2_final',
            'stats': stage2_stats
        }
        self.checkpoint_manager.save_checkpoint(
            final_checkpoint_state,
            epoch=self.config.stage2_epochs,
            is_best=True
        )
        
        # é˜¶æ®µ2ç»“æŸåçš„è¯„ä¼°
        if self.config.enable_evaluation and self.evaluator is not None:
            logger.info("ğŸ”¬ æ‰§è¡Œé˜¶æ®µ2ç»“æŸè¯„ä¼°...")
            eval_results = self._run_evaluation(model, stage='stage2_final')
            if eval_results:
                stage2_stats['final_evaluation'] = eval_results
        
        logger.info("âœ… é˜¶æ®µ2è®­ç»ƒå®Œæˆ")
        self.training_stats['stage2'] = stage2_stats
        return stage2_stats
    
    def run_complete_training(self,
                            train_loader: DataLoader,
                            val_loader: Optional[DataLoader] = None) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„åˆ†ç¦»å¼è®­ç»ƒ"""
        logger.info("ğŸ¯ å¼€å§‹CPL-Diffå¯å‘çš„åˆ†ç¦»å¼è®­ç»ƒç­–ç•¥")
        
        # ä¿å­˜é…ç½®
        config_path = Path(self.config.output_dir) / "training_config.json"
        with open(config_path, 'w') as f:
            # å°†dataclassè½¬æ¢ä¸ºdictï¼Œç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯JSONå¯åºåˆ—åŒ–çš„
            config_dict = {}
            for field in self.config.__dataclass_fields__.values():
                value = getattr(self.config, field.name)
                # å¤„ç†å¯èƒ½çš„OmegaConfå¯¹è±¡
                if hasattr(value, '__iter__') and not isinstance(value, (str, bytes)):
                    try:
                        # å°è¯•è½¬æ¢ä¸ºåˆ—è¡¨
                        config_dict[field.name] = list(value)
                    except:
                        config_dict[field.name] = str(value)
                else:
                    config_dict[field.name] = value
            json.dump(config_dict, f, indent=2)
        
        # é˜¶æ®µ1ï¼šå»å™ªå™¨è®­ç»ƒ
        stage1_stats = self.train_stage1(train_loader, val_loader)
        
        # é˜¶æ®µ2ï¼šè§£ç å™¨è®­ç»ƒ
        stage2_stats = self.train_stage2(train_loader, val_loader)
        
        # ä¿å­˜å®Œæ•´è®­ç»ƒç»Ÿè®¡
        final_stats = {
            'stage1': stage1_stats,
            'stage2': stage2_stats,
            'config': config_dict
        }
        
        stats_path = Path(self.config.output_dir) / "training_stats.json"
        with open(stats_path, 'w') as f:
            json.dump(final_stats, f, indent=2)
        
        # è®­ç»ƒå®Œæˆåçš„è‡ªåŠ¨ç”Ÿæˆå’Œè¯„ä¼°
        if self.config.auto_generate_after_training:
            logger.info("ğŸ¯ å¼€å§‹è®­ç»ƒåè‡ªåŠ¨ç”Ÿæˆå’Œè¯„ä¼°...")
            generation_results = self._run_post_training_generation_and_evaluation()
            if generation_results:
                final_stats['post_training_evaluation'] = generation_results
        
        logger.info("ğŸ‰ åˆ†ç¦»å¼è®­ç»ƒå®Œæˆï¼")
        logger.info(f"ç»Ÿè®¡æ•°æ®ä¿å­˜åˆ°: {stats_path}")
        logger.info(f"æ£€æŸ¥ç‚¹ä¿å­˜åˆ°: {self.config.checkpoint_dir}")
        
        return final_stats
    
    def load_stage1_checkpoint(self, checkpoint_path: str):
        """åŠ è½½é˜¶æ®µ1æ£€æŸ¥ç‚¹ï¼ˆå…¼å®¹æ€§åŠ è½½ï¼‰"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
        
        # è·å–æ£€æŸ¥ç‚¹ä¸­çš„æ¨¡å‹çŠ¶æ€å’Œå½“å‰æ¨¡å‹çŠ¶æ€
        checkpoint_state = checkpoint['model']
        model_state = self.model.state_dict()
        
        # è¿‡æ»¤ä¸åŒ¹é…çš„å‚æ•°ï¼ˆå¦‚ESMFoldå‚æ•°ï¼‰
        compatible_state = {}
        skipped_params = []
        
        for key, value in checkpoint_state.items():
            if key in model_state:
                # æ£€æŸ¥å‚æ•°å½¢çŠ¶æ˜¯å¦åŒ¹é…
                if value.shape == model_state[key].shape:
                    compatible_state[key] = value
                else:
                    skipped_params.append(f"{key} (å½¢çŠ¶ä¸åŒ¹é…: {value.shape} vs {model_state[key].shape})")
            else:
                skipped_params.append(f"{key} (å‚æ•°ä¸å­˜åœ¨)")
        
        # åŠ è½½å…¼å®¹çš„å‚æ•°
        self.model.load_state_dict(compatible_state, strict=False)
        
        # è®°å½•åŠ è½½ä¿¡æ¯
        logger.info(f"å·²åŠ è½½é˜¶æ®µ1æ£€æŸ¥ç‚¹: {checkpoint_path}")
        logger.info(f"æˆåŠŸåŠ è½½å‚æ•°æ•°é‡: {len(compatible_state)}")
        logger.info(f"è·³è¿‡å‚æ•°æ•°é‡: {len(skipped_params)}")
        
        if skipped_params:
            logger.warning(f"è·³è¿‡çš„å‚æ•°ç¤ºä¾‹ (å‰10ä¸ª): {skipped_params[:10]}")
        
        # åŠ è½½å…¶ä»–çŠ¶æ€ä¿¡æ¯
        if 'epoch' in checkpoint:
            logger.info(f"æ£€æŸ¥ç‚¹æ¥è‡ªepoch: {checkpoint['epoch']}")
        if 'stage' in checkpoint:
            logger.info(f"æ£€æŸ¥ç‚¹æ¥è‡ªé˜¶æ®µ: {checkpoint['stage']}")
        if 'stats' in checkpoint:
            logger.info(f"é˜¶æ®µ1è®­ç»ƒç»Ÿè®¡å¯ç”¨")
    
    def _run_evaluation(self, model, stage: str) -> Optional[Dict]:
        """è¿è¡ŒCPL-Diffè¯„ä¼°"""
        # åœ¨é˜¶æ®µ1ï¼Œè§£ç å™¨æœªè®­ç»ƒï¼ŒåŸºäºç”Ÿæˆçš„è¯„ä¼°æ— æ„ä¹‰
        if stage.startswith('stage1'):
            logger.info(f"è·³è¿‡é˜¶æ®µ1çš„ç”Ÿæˆè¯„ä¼° (é˜¶æ®µ: {stage})ï¼Œå› ä¸ºè§£ç å™¨å°šæœªè®­ç»ƒã€‚")
            return None

        if not self.evaluator or not self.tokenizer:
            return None
            
        try:
            logger.info(f"ç”Ÿæˆæ ·æœ¬ç”¨äºè¯„ä¼° (é˜¶æ®µ: {stage})...")
            
            # ç”Ÿæˆè¯„ä¼°æ ·æœ¬
            generated_sequences = self._generate_evaluation_samples(
                model, 
                num_samples=min(100, self.config.evaluation_num_samples)
            )
            
            if not generated_sequences:
                logger.warning("ç”Ÿæˆæ ·æœ¬å¤±è´¥ï¼Œè·³è¿‡è¯„ä¼°")
                return None
            
            logger.info(f"å¯¹ {len(generated_sequences)} ä¸ªæ ·æœ¬è¿›è¡ŒCPL-Diffè¯„ä¼°...")
            
            # è¿è¡Œè¯„ä¼°
            eval_results = self.evaluator.comprehensive_cpldiff_evaluation(
                generated_sequences=generated_sequences,
                reference_sequences=[],  # ä½¿ç”¨å†…ç½®å‚è€ƒåºåˆ—
                peptide_type='antimicrobial'
            )
            
            # ç”ŸæˆæŠ¥å‘Š
            report_name = f"{stage}_evaluation"
            self.evaluator.generate_cpldiff_report(eval_results, report_name)
            
            logger.info(f"âœ“ {stage} è¯„ä¼°å®Œæˆ")
            return eval_results
            
        except Exception as e:
            logger.error(f"è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {e}")
            return None
    
    def _generate_evaluation_samples(self, model, num_samples: int = 100) -> List[str]:
        """ç”Ÿæˆç”¨äºè¯„ä¼°çš„æ ·æœ¬åºåˆ—ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        try:
            sequences = []
            model.eval()
            
            with torch.no_grad():
                # è·å–æ¨¡å‹çš„å®é™…éšè—ç»´åº¦
                try:
                    if hasattr(model, 'sequence_encoder') and hasattr(model.sequence_encoder, 'config'):
                        hidden_size = getattr(model.sequence_encoder.config, 'hidden_size', 320)
                    else:
                        hidden_size = 320  # é»˜è®¤ä½¿ç”¨ESM2_t6_8Mçš„ç»´åº¦
                    hidden_size = int(hidden_size)
                except:
                    hidden_size = 320
                
                for i in range(num_samples):
                    try:
                        # éšæœºé•¿åº¦
                        length = torch.randint(
                            int(self.config.min_length),
                            int(self.config.max_length) + 1,
                            (1,)
                        ).item()
                        
                        # ç¡®ä¿lengthæ˜¯æœ‰æ•ˆçš„æ•´æ•°
                        length = max(int(length), 5)
                        length = min(length, int(self.config.max_length))
                        
                        # ç”ŸæˆéšæœºåµŒå…¥
                        seq_embeddings = torch.randn(1, length, hidden_size, device=self.device)
                        
                        # ä½¿ç”¨è§£ç å™¨ç›´æ¥ç”Ÿæˆåºåˆ—ï¼ˆè·³è¿‡å¤æ‚çš„å»å™ªè¿‡ç¨‹ï¼‰
                        if hasattr(model, 'decode_projection') and model.decode_projection is not None:
                            # ç›´æ¥ä½¿ç”¨è§£ç å™¨
                            logits = model.decode_projection(seq_embeddings)
                            token_ids = torch.argmax(logits, dim=-1).squeeze(0)
                            
                            if self.tokenizer:
                                sequence = self.tokenizer.decode(token_ids, skip_special_tokens=True)
                                amino_acids = set('ACDEFGHIKLMNPQRSTVWY')
                                clean_sequence = ''.join([c for c in sequence.upper() if c in amino_acids])
                                
                                # ç¡®ä¿åºåˆ—é•¿åº¦åˆé€‚
                                if clean_sequence and len(clean_sequence) >= self.config.min_length:
                                    # æˆªæ–­åˆ°ç›®æ ‡é•¿åº¦
                                    final_sequence = clean_sequence[:length]
                                    sequences.append(final_sequence)
                                else:
                                    # ç”Ÿæˆéšæœºæœ‰æ•ˆåºåˆ—
                                    import random
                                    amino_acid_list = list('ACDEFGHIKLMNPQRSTVWY')
                                    fallback_seq = ''.join(random.choices(amino_acid_list, k=length))
                                    sequences.append(fallback_seq)
                            else:
                                # æ— tokenizeræ—¶çš„å›é€€æ–¹æ¡ˆ
                                import random
                                amino_acid_list = list('ACDEFGHIKLMNPQRSTVWY')
                                fallback_seq = ''.join(random.choices(amino_acid_list, k=length))
                                sequences.append(fallback_seq)
                        else:
                            # æ— è§£ç å™¨æ—¶çš„å›é€€æ–¹æ¡ˆ
                            import random
                            amino_acid_list = list('ACDEFGHIKLMNPQRSTVWY')
                            fallback_seq = ''.join(random.choices(amino_acid_list, k=length))
                            sequences.append(fallback_seq)
                            
                    except Exception as e:
                        logger.error(f"ç”Ÿæˆå•ä¸ªåºåˆ—å¤±è´¥: {e}")
                        # ç”Ÿæˆéšæœºå›é€€åºåˆ—
                        import random
                        fallback_length = max(10, min(30, int(self.config.min_length) + 5))
                        amino_acid_list = list('ACDEFGHIKLMNPQRSTVWY')
                        fallback_seq = ''.join(random.choices(amino_acid_list, k=fallback_length))
                        sequences.append(fallback_seq)
                        continue
            
            return sequences[:num_samples]
            
        except Exception as e:
            logger.error(f"æ ·æœ¬ç”Ÿæˆå¤±è´¥: {e}")
            # å®Œå…¨å›é€€æ–¹æ¡ˆ
            import random
            amino_acid_list = list('ACDEFGHIKLMNPQRSTVWY')
            fallback_sequences = [''.join(random.choices(amino_acid_list, k=20)) for _ in range(min(10, num_samples))]
            return fallback_sequences
    
    def _decode_for_evaluation(self, embeddings: torch.Tensor, attention_mask: torch.Tensor, target_length) -> str:
        """ä¸ºè¯„ä¼°è§£ç åºåˆ—ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        try:
            # å¼ºåˆ¶è½¬æ¢target_lengthä¸ºæ•´æ•°
            if target_length is None:
                target_length = 15  # é»˜è®¤é•¿åº¦
            elif isinstance(target_length, torch.Tensor):
                target_length = int(target_length.item())
            elif isinstance(target_length, (float, int)):
                target_length = int(target_length)
            else:
                try:
                    target_length = int(target_length)
                except (ValueError, TypeError):
                    target_length = 15
            
            # ç¡®ä¿é•¿åº¦åœ¨åˆç†èŒƒå›´å†…
            target_length = max(int(self.config.min_length), min(target_length, int(self.config.max_length)))
            
        except Exception:
            target_length = 15
        
        try:
            # ç¡®ä¿è¾“å…¥æ˜¯æœ‰æ•ˆçš„tensor
            if not isinstance(embeddings, torch.Tensor):
                logger.warning("embeddingsä¸æ˜¯tensorï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ")
                raise ValueError("Invalid embeddings type")
            
            if not isinstance(attention_mask, torch.Tensor):
                logger.warning("attention_maskä¸æ˜¯tensorï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ")
                raise ValueError("Invalid attention_mask type")
            
            # ç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            embeddings = embeddings.to(self.device)
            attention_mask = attention_mask.to(self.device)
            
            # ç¡®ä¿å½¢çŠ¶æ­£ç¡®
            if embeddings.dim() == 1:
                embeddings = embeddings.unsqueeze(0)
            if embeddings.dim() == 2:
                embeddings = embeddings.unsqueeze(0)
            
            if attention_mask.dim() == 0:
                attention_mask = attention_mask.unsqueeze(0)
            if attention_mask.dim() == 1:
                attention_mask = attention_mask.unsqueeze(0)
            
            # ä½¿ç”¨è§£ç å™¨
            if hasattr(self.model, 'decode_projection') and self.model.decode_projection is not None:
                # è·å–logits
                logits = self.model.decode_projection(embeddings)  # [batch, seq_len, vocab_size]
                
                # è·å–token ids
                token_ids = torch.argmax(logits, dim=-1).squeeze()  # [seq_len] or [batch, seq_len]
                
                # å¤„ç†å¯èƒ½çš„æ‰¹æ¬¡ç»´åº¦
                if token_ids.dim() > 1:
                    token_ids = token_ids[0]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
                
                # è½¬æ¢ä¸ºåºåˆ—
                if self.tokenizer:
                    sequence = self.tokenizer.decode(token_ids, skip_special_tokens=True)
                    
                    # æ¸…ç†åºåˆ—ï¼Œåªä¿ç•™æœ‰æ•ˆæ°¨åŸºé…¸
                    amino_acids = set('ACDEFGHIKLMNPQRSTVWY')
                    clean_sequence = ''.join([c for c in sequence.upper() if c in amino_acids])
                    
                    if clean_sequence:
                        # ç¡®ä¿é•¿åº¦åˆé€‚
                        final_length = min(len(clean_sequence), target_length)
                        return clean_sequence[:final_length]
            
            # å›é€€æ–¹æ¡ˆ
            import random
            amino_acid_list = list('ACDEFGHIKLMNPQRSTVWY')
            return ''.join(random.choices(amino_acid_list, k=target_length))
            
        except Exception as e:
            logger.error(f"è§£ç å¤±è´¥: {e}")
            # å›é€€åˆ°éšæœºåºåˆ—
            import random
            amino_acid_list = list('ACDEFGHIKLMNPQRSTVWY')
            return ''.join(random.choices(amino_acid_list, k=target_length))
    
    def _run_post_training_generation_and_evaluation(self) -> Optional[Dict]:
        """è®­ç»ƒå®Œæˆåè¿è¡Œç”Ÿæˆå’Œè¯„ä¼°"""
        try:
            logger.info("ğŸ¯ ç”Ÿæˆæœ€ç»ˆè¯„ä¼°æ ·æœ¬...")
            
            # ä½¿ç”¨æœ€ä½³æ¨¡å‹
            eval_model = self.ema.ema_model if self.ema else self.model
            
            # ç”Ÿæˆæ›´å¤šæ ·æœ¬
            generated_sequences = self._generate_evaluation_samples(
                eval_model, 
                num_samples=self.config.evaluation_num_samples
            )
            
            if not generated_sequences:
                logger.warning("æœ€ç»ˆç”Ÿæˆå¤±è´¥")
                return None
            
            logger.info(f"ç”Ÿæˆ {len(generated_sequences)} ä¸ªæœ€ç»ˆæ ·æœ¬")
            
            # ä¿å­˜ç”Ÿæˆçš„åºåˆ—
            output_path = Path(self.config.output_dir) / "final_generated_sequences.fasta"
            with open(output_path, 'w') as f:
                for i, seq in enumerate(generated_sequences):
                    f.write(f">Generated_{i}\n{seq}\n")
            logger.info(f"åºåˆ—ä¿å­˜åˆ°: {output_path}")
            
            # è¿è¡Œæœ€ç»ˆè¯„ä¼°
            if self.evaluator:
                logger.info("ğŸ”¬ è¿è¡Œæœ€ç»ˆCPL-Diffè¯„ä¼°...")
                eval_results = self.evaluator.comprehensive_cpldiff_evaluation(
                    generated_sequences=generated_sequences,
                    reference_sequences=[],
                    peptide_type='antimicrobial'
                )
                
                # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
                self.evaluator.generate_cpldiff_report(eval_results, "final_post_training")
                
                logger.info("âœ… æœ€ç»ˆè¯„ä¼°å®Œæˆ")
                return {
                    'generated_sequences_count': len(generated_sequences),
                    'sequences_file': str(output_path),
                    'evaluation_results': eval_results
                }
            
            return {
                'generated_sequences_count': len(generated_sequences),
                'sequences_file': str(output_path)
            }
            
        except Exception as e:
            logger.error(f"æœ€ç»ˆç”Ÿæˆå’Œè¯„ä¼°å¤±è´¥: {e}")
            return None
    
    def get_training_summary(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒæ‘˜è¦"""
        summary = {
            'stage1': {
                'final_loss': self.training_stats['stage1']['losses'][-1] if self.training_stats['stage1']['losses'] else None,
                'best_val_loss': min(self.training_stats['stage1']['val_losses']) if self.training_stats['stage1']['val_losses'] else None,
                'total_epochs': len(self.training_stats['stage1']['losses']),
                'evaluations_count': len(self.training_stats['stage1'].get('evaluations', []))
            },
            'stage2': {
                'final_loss': self.training_stats['stage2']['losses'][-1] if self.training_stats['stage2']['losses'] else None,
                'best_val_loss': min(self.training_stats['stage2']['val_losses']) if self.training_stats['stage2']['val_losses'] else None,
                'total_epochs': len(self.training_stats['stage2']['losses']),
                'evaluations_count': len(self.training_stats['stage2'].get('evaluations', []))
            }
        }
        return summary