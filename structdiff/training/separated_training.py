#!/usr/bin/env python3
"""
CPL-Diffå¯å‘çš„åˆ†ç¦»å¼è®­ç»ƒç­–ç•¥
åŸºäºCPL-Diffçš„ä¸¤é˜¶æ®µè®­ç»ƒè®¾è®¡ï¼Œå°†å»å™ªå™¨è®­ç»ƒå’Œåºåˆ—è§£ç åˆ†ç¦»
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from typing import Dict, Optional, Tuple, Any, List
import logging
from pathlib import Path
import json
from dataclasses import dataclass
from tqdm import tqdm

from ..models.structdiff import StructDiff
from ..diffusion.gaussian_diffusion import GaussianDiffusion
from ..utils.logger import get_logger
from ..utils.checkpoint import CheckpointManager
from ..utils.ema import EMA

logger = get_logger(__name__)


@dataclass
class SeparatedTrainingConfig:
    """åˆ†ç¦»å¼è®­ç»ƒé…ç½®"""
    # é˜¶æ®µ1: å»å™ªå™¨è®­ç»ƒé…ç½®
    stage1_epochs: int = 200
    stage1_lr: float = 1e-4
    stage1_batch_size: int = 32
    stage1_gradient_clip: float = 1.0
    stage1_warmup_steps: int = 1000
    
    # é˜¶æ®µ2: è§£ç å™¨è®­ç»ƒé…ç½®  
    stage2_epochs: int = 100
    stage2_lr: float = 5e-5
    stage2_batch_size: int = 64
    stage2_gradient_clip: float = 0.5
    stage2_warmup_steps: int = 500
    
    # å…±åŒé…ç½®
    use_amp: bool = True
    use_ema: bool = True
    ema_decay: float = 0.9999
    save_every: int = 1000
    validate_every: int = 500
    log_every: int = 100
    
    # é•¿åº¦æ§åˆ¶
    use_length_control: bool = True
    max_length: int = 50
    min_length: int = 5
    
    # æ¡ä»¶æ§åˆ¶
    use_cfg: bool = True
    cfg_dropout_prob: float = 0.1
    cfg_guidance_scale: float = 2.0
    
    # æ•°æ®è·¯å¾„
    data_dir: str = "./data/processed"
    output_dir: str = "./outputs/separated_training"
    checkpoint_dir: str = "./checkpoints/separated"


class SeparatedTrainingManager:
    """åˆ†ç¦»å¼è®­ç»ƒç®¡ç†å™¨"""
    
    def __init__(self, 
                 config: SeparatedTrainingConfig,
                 model: StructDiff,
                 diffusion: GaussianDiffusion,
                 device: str = 'cuda'):
        self.config = config
        self.model = model
        self.diffusion = diffusion
        self.device = device
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(config.output_dir).mkdir(parents=True, exist_ok=True)
        Path(config.checkpoint_dir).mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.checkpoint_manager = CheckpointManager(config.checkpoint_dir)
        self.training_stats = {
            'stage1': {'losses': [], 'val_losses': []},
            'stage2': {'losses': [], 'val_losses': []},
        }
        
        # EMA
        if config.use_ema:
            self.ema = EMA(model, decay=config.ema_decay)
        else:
            self.ema = None
            
        logger.info(f"åˆå§‹åŒ–åˆ†ç¦»å¼è®­ç»ƒç®¡ç†å™¨ï¼Œè®¾å¤‡: {device}")
    
    def prepare_stage1_components(self) -> Tuple[nn.Module, torch.optim.Optimizer, Any]:
        """å‡†å¤‡é˜¶æ®µ1è®­ç»ƒç»„ä»¶ï¼ˆå»å™ªå™¨è®­ç»ƒï¼‰"""
        # å†»ç»“åºåˆ—ç¼–ç å™¨
        for param in self.model.sequence_encoder.parameters():
            param.requires_grad = False
        
        # åªè®­ç»ƒå»å™ªå™¨å’Œç»“æ„ç¼–ç å™¨
        trainable_params = []
        for name, param in self.model.named_parameters():
            if 'sequence_encoder' not in name:
                param.requires_grad = True
                trainable_params.append(param)
            else:
                param.requires_grad = False
        
        logger.info(f"é˜¶æ®µ1å¯è®­ç»ƒå‚æ•°æ•°é‡: {sum(p.numel() for p in trainable_params)}")
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(
            trainable_params,
            lr=self.config.stage1_lr,
            weight_decay=0.01,
            betas=(0.9, 0.999)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=self.config.stage1_epochs,
            eta_min=1e-6
        )
        
        return self.model, optimizer, scheduler
    
    def prepare_stage2_components(self) -> Tuple[nn.Module, torch.optim.Optimizer, Any]:
        """å‡†å¤‡é˜¶æ®µ2è®­ç»ƒç»„ä»¶ï¼ˆè§£ç å™¨è®­ç»ƒï¼‰"""
        # å†»ç»“å»å™ªå™¨å’Œç»“æ„ç¼–ç å™¨
        for name, param in self.model.named_parameters():
            if 'sequence_decoder' in name:
                param.requires_grad = True
            else:
                param.requires_grad = False
        
        # åªè®­ç»ƒåºåˆ—è§£ç å™¨
        decoder_params = [p for n, p in self.model.named_parameters() 
                         if 'sequence_decoder' in n and p.requires_grad]
        
        logger.info(f"é˜¶æ®µ2å¯è®­ç»ƒå‚æ•°æ•°é‡: {sum(p.numel() for p in decoder_params)}")
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(
            decoder_params,
            lr=self.config.stage2_lr,
            weight_decay=0.01,
            betas=(0.9, 0.999)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=self.config.stage2_epochs,
            eta_min=1e-6
        )
        
        return self.model, optimizer, scheduler
    
    def stage1_training_step(self, 
                           batch: Dict[str, torch.Tensor],
                           model: nn.Module,
                           optimizer: torch.optim.Optimizer) -> Dict[str, float]:
        """é˜¶æ®µ1è®­ç»ƒæ­¥éª¤ï¼šå»å™ªå™¨è®­ç»ƒ"""
        model.train()
        
        # å‡†å¤‡æ•°æ®
        sequences = batch['sequences'].to(self.device)
        attention_mask = batch['attention_mask'].to(self.device)
        structures = batch.get('structures', None)
        conditions = batch.get('conditions', None)
        
        # è·å–å›ºå®šçš„åºåˆ—åµŒå…¥ï¼ˆä¸è®¡ç®—æ¢¯åº¦ï¼‰
        with torch.no_grad():
            seq_embeddings = model.sequence_encoder(sequences, attention_mask)
            if hasattr(seq_embeddings, 'last_hidden_state'):
                seq_embeddings = seq_embeddings.last_hidden_state
        
        # éšæœºæ—¶é—´æ­¥
        batch_size = sequences.shape[0]
        timesteps = torch.randint(
            0, self.diffusion.num_timesteps, 
            (batch_size,), device=self.device
        )
        
        # æ·»åŠ å™ªå£°
        noise = torch.randn_like(seq_embeddings)
        noisy_embeddings = self.diffusion.q_sample(seq_embeddings, timesteps, noise)
        
        # å¤„ç†æ¡ä»¶ï¼ˆæ”¯æŒCFGï¼‰
        if self.config.use_cfg and conditions is not None:
            # éšæœºä¸¢å¼ƒéƒ¨åˆ†æ¡ä»¶
            dropout_mask = torch.rand(batch_size) < self.config.cfg_dropout_prob
            for key, value in conditions.items():
                if isinstance(value, torch.Tensor):
                    conditions[key] = value.clone()
                    conditions[key][dropout_mask] = -1  # æ— æ¡ä»¶æ ‡è®°
        
        # å»å™ªé¢„æµ‹
        if hasattr(model, 'denoiser'):
            predicted_noise = model.denoiser(
                noisy_embeddings, timesteps, attention_mask,
                structure_features=structures, conditions=conditions
            )
        else:
            predicted_noise = model(
                noisy_embeddings, timesteps, attention_mask,
                structure_features=structures, conditions=conditions
            )
        
        # è®¡ç®—æŸå¤±ï¼ˆé¢„æµ‹å™ªå£°ï¼‰
        loss = F.mse_loss(predicted_noise, noise)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        if self.config.use_amp:
            from torch.cuda.amp import autocast, GradScaler
            scaler = GradScaler()
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(
                model.parameters(), self.config.stage1_gradient_clip
            )
            scaler.step(optimizer)
            scaler.update()
        else:
            loss.backward()
            torch.nn.utils.clip_grad_norm_(
                model.parameters(), self.config.stage1_gradient_clip
            )
            optimizer.step()
        
        # EMAæ›´æ–°
        if self.ema is not None:
            self.ema.update()
        
        return {
            'loss': loss.item(),
            'lr': optimizer.param_groups[0]['lr']
        }
    
    def stage2_training_step(self,
                           batch: Dict[str, torch.Tensor],
                           model: nn.Module,
                           optimizer: torch.optim.Optimizer) -> Dict[str, float]:
        """é˜¶æ®µ2è®­ç»ƒæ­¥éª¤ï¼šè§£ç å™¨è®­ç»ƒ"""
        model.train()
        
        # å‡†å¤‡æ•°æ®
        sequences = batch['sequences'].to(self.device)
        attention_mask = batch['attention_mask'].to(self.device)
        
        # è·å–å¹²å‡€çš„åºåˆ—åµŒå…¥ï¼ˆå›ºå®šç¼–ç å™¨ï¼‰
        with torch.no_grad():
            seq_embeddings = model.sequence_encoder(sequences, attention_mask)
            if hasattr(seq_embeddings, 'last_hidden_state'):
                seq_embeddings = seq_embeddings.last_hidden_state
        
        # åºåˆ—è§£ç 
        if hasattr(model, 'sequence_decoder'):
            logits = model.sequence_decoder(seq_embeddings, attention_mask)
        else:
            # å¦‚æœæ²¡æœ‰ç‹¬ç«‹è§£ç å™¨ï¼Œä½¿ç”¨åŸå§‹å‰å‘ä¼ æ’­
            logits = model.decode_sequences(seq_embeddings, attention_mask)
        
        # è®¡ç®—äº¤å‰ç†µæŸå¤±
        vocab_size = logits.size(-1)
        flat_logits = logits.view(-1, vocab_size)
        flat_targets = sequences.view(-1)
        
        # å¿½ç•¥paddingä½ç½®
        if attention_mask is not None:
            flat_mask = attention_mask.view(-1).bool()
            flat_logits = flat_logits[flat_mask]
            flat_targets = flat_targets[flat_mask]
        
        loss = F.cross_entropy(flat_logits, flat_targets)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(
            model.parameters(), self.config.stage2_gradient_clip
        )
        optimizer.step()
        
        return {
            'loss': loss.item(),
            'lr': optimizer.param_groups[0]['lr']
        }
    
    def validate_stage1(self, val_loader: DataLoader, model: nn.Module) -> Dict[str, float]:
        """é˜¶æ®µ1éªŒè¯"""
        model.eval()
        total_loss = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                sequences = batch['sequences'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                structures = batch.get('structures', None)
                conditions = batch.get('conditions', None)
                
                # è·å–åºåˆ—åµŒå…¥
                seq_embeddings = model.sequence_encoder(sequences, attention_mask)
                if hasattr(seq_embeddings, 'last_hidden_state'):
                    seq_embeddings = seq_embeddings.last_hidden_state
                
                # éšæœºæ—¶é—´æ­¥å’Œå™ªå£°
                batch_size = sequences.shape[0]
                timesteps = torch.randint(
                    0, self.diffusion.num_timesteps,
                    (batch_size,), device=self.device
                )
                noise = torch.randn_like(seq_embeddings)
                noisy_embeddings = self.diffusion.q_sample(seq_embeddings, timesteps, noise)
                
                # é¢„æµ‹
                if hasattr(model, 'denoiser'):
                    predicted_noise = model.denoiser(
                        noisy_embeddings, timesteps, attention_mask,
                        structure_features=structures, conditions=conditions
                    )
                else:
                    predicted_noise = model(
                        noisy_embeddings, timesteps, attention_mask,
                        structure_features=structures, conditions=conditions
                    )
                
                loss = F.mse_loss(predicted_noise, noise)
                total_loss += loss.item()
                num_batches += 1
        
        return {'val_loss': total_loss / max(num_batches, 1)}
    
    def validate_stage2(self, val_loader: DataLoader, model: nn.Module) -> Dict[str, float]:
        """é˜¶æ®µ2éªŒè¯"""
        model.eval()
        total_loss = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                sequences = batch['sequences'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                
                # è·å–åºåˆ—åµŒå…¥
                seq_embeddings = model.sequence_encoder(sequences, attention_mask)
                if hasattr(seq_embeddings, 'last_hidden_state'):
                    seq_embeddings = seq_embeddings.last_hidden_state
                
                # è§£ç 
                if hasattr(model, 'sequence_decoder'):
                    logits = model.sequence_decoder(seq_embeddings, attention_mask)
                else:
                    logits = model.decode_sequences(seq_embeddings, attention_mask)
                
                # è®¡ç®—æŸå¤±
                vocab_size = logits.size(-1)
                flat_logits = logits.view(-1, vocab_size)
                flat_targets = sequences.view(-1)
                
                if attention_mask is not None:
                    flat_mask = attention_mask.view(-1).bool()
                    flat_logits = flat_logits[flat_mask]
                    flat_targets = flat_targets[flat_mask]
                
                loss = F.cross_entropy(flat_logits, flat_targets)
                total_loss += loss.item()
                num_batches += 1
        
        return {'val_loss': total_loss / max(num_batches, 1)}
    
    def train_stage1(self, 
                    train_loader: DataLoader, 
                    val_loader: Optional[DataLoader] = None) -> Dict[str, List[float]]:
        """æ‰§è¡Œé˜¶æ®µ1è®­ç»ƒï¼šå»å™ªå™¨è®­ç»ƒ"""
        logger.info("ğŸš€ å¼€å§‹é˜¶æ®µ1è®­ç»ƒï¼šå»å™ªå™¨è®­ç»ƒï¼ˆå›ºå®šESMç¼–ç å™¨ï¼‰")
        
        model, optimizer, scheduler = self.prepare_stage1_components()
        
        # ä½¿ç”¨EMAæ¨¡å‹è¿›è¡ŒéªŒè¯
        eval_model = self.ema.ema_model if self.ema else model
        
        stage1_stats = {'losses': [], 'val_losses': []}
        
        for epoch in range(self.config.stage1_epochs):
            # è®­ç»ƒå¾ªç¯
            model.train()
            epoch_losses = []
            
            pbar = tqdm(train_loader, desc=f"é˜¶æ®µ1 Epoch {epoch+1}/{self.config.stage1_epochs}")
            for step, batch in enumerate(pbar):
                step_stats = self.stage1_training_step(batch, model, optimizer)
                epoch_losses.append(step_stats['loss'])
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'loss': f"{step_stats['loss']:.4f}",
                    'lr': f"{step_stats['lr']:.2e}"
                })
                
                # æ—¥å¿—è®°å½•
                if step % self.config.log_every == 0:
                    logger.info(f"é˜¶æ®µ1 Epoch {epoch+1}, Step {step}: "
                              f"Loss = {step_stats['loss']:.4f}, "
                              f"LR = {step_stats['lr']:.2e}")
                
                # éªŒè¯
                if val_loader and step % self.config.validate_every == 0:
                    val_stats = self.validate_stage1(val_loader, eval_model)
                    stage1_stats['val_losses'].append(val_stats['val_loss'])
                    logger.info(f"é˜¶æ®µ1éªŒè¯ - Val Loss: {val_stats['val_loss']:.4f}")
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if step % self.config.save_every == 0:
                    self.checkpoint_manager.save_checkpoint(
                        model=eval_model,
                        optimizer=optimizer,
                        scheduler=scheduler,
                        epoch=epoch,
                        step=step,
                        stage='stage1',
                        stats=stage1_stats
                    )
            
            # è®°å½•epochç»Ÿè®¡
            epoch_loss = sum(epoch_losses) / len(epoch_losses)
            stage1_stats['losses'].append(epoch_loss)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step()
            
            logger.info(f"é˜¶æ®µ1 Epoch {epoch+1} å®Œæˆ - å¹³å‡Loss: {epoch_loss:.4f}")
        
        # ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹
        self.checkpoint_manager.save_checkpoint(
            model=eval_model,
            optimizer=optimizer,
            scheduler=scheduler,
            epoch=self.config.stage1_epochs,
            step=0,
            stage='stage1_final',
            stats=stage1_stats
        )
        
        logger.info("âœ… é˜¶æ®µ1è®­ç»ƒå®Œæˆ")
        self.training_stats['stage1'] = stage1_stats
        return stage1_stats
    
    def train_stage2(self,
                    train_loader: DataLoader,
                    val_loader: Optional[DataLoader] = None) -> Dict[str, List[float]]:
        """æ‰§è¡Œé˜¶æ®µ2è®­ç»ƒï¼šè§£ç å™¨è®­ç»ƒ"""
        logger.info("ğŸš€ å¼€å§‹é˜¶æ®µ2è®­ç»ƒï¼šè§£ç å™¨è®­ç»ƒï¼ˆå›ºå®šå»å™ªå™¨ï¼‰")
        
        model, optimizer, scheduler = self.prepare_stage2_components()
        
        stage2_stats = {'losses': [], 'val_losses': []}
        
        for epoch in range(self.config.stage2_epochs):
            # è®­ç»ƒå¾ªç¯
            model.train()
            epoch_losses = []
            
            pbar = tqdm(train_loader, desc=f"é˜¶æ®µ2 Epoch {epoch+1}/{self.config.stage2_epochs}")
            for step, batch in enumerate(pbar):
                step_stats = self.stage2_training_step(batch, model, optimizer)
                epoch_losses.append(step_stats['loss'])
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'loss': f"{step_stats['loss']:.4f}",
                    'lr': f"{step_stats['lr']:.2e}"
                })
                
                # æ—¥å¿—è®°å½•
                if step % self.config.log_every == 0:
                    logger.info(f"é˜¶æ®µ2 Epoch {epoch+1}, Step {step}: "
                              f"Loss = {step_stats['loss']:.4f}, "
                              f"LR = {step_stats['lr']:.2e}")
                
                # éªŒè¯
                if val_loader and step % self.config.validate_every == 0:
                    val_stats = self.validate_stage2(val_loader, model)
                    stage2_stats['val_losses'].append(val_stats['val_loss'])
                    logger.info(f"é˜¶æ®µ2éªŒè¯ - Val Loss: {val_stats['val_loss']:.4f}")
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if step % self.config.save_every == 0:
                    self.checkpoint_manager.save_checkpoint(
                        model=model,
                        optimizer=optimizer,
                        scheduler=scheduler,
                        epoch=epoch,
                        step=step,
                        stage='stage2',
                        stats=stage2_stats
                    )
            
            # è®°å½•epochç»Ÿè®¡
            epoch_loss = sum(epoch_losses) / len(epoch_losses)
            stage2_stats['losses'].append(epoch_loss)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step()
            
            logger.info(f"é˜¶æ®µ2 Epoch {epoch+1} å®Œæˆ - å¹³å‡Loss: {epoch_loss:.4f}")
        
        # ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹
        self.checkpoint_manager.save_checkpoint(
            model=model,
            optimizer=optimizer,
            scheduler=scheduler,
            epoch=self.config.stage2_epochs,
            step=0,
            stage='stage2_final',
            stats=stage2_stats
        )
        
        logger.info("âœ… é˜¶æ®µ2è®­ç»ƒå®Œæˆ")
        self.training_stats['stage2'] = stage2_stats
        return stage2_stats
    
    def run_complete_training(self,
                            train_loader: DataLoader,
                            val_loader: Optional[DataLoader] = None) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„åˆ†ç¦»å¼è®­ç»ƒ"""
        logger.info("ğŸ¯ å¼€å§‹CPL-Diffå¯å‘çš„åˆ†ç¦»å¼è®­ç»ƒç­–ç•¥")
        
        # ä¿å­˜é…ç½®
        config_path = Path(self.config.output_dir) / "training_config.json"
        with open(config_path, 'w') as f:
            # å°†dataclassè½¬æ¢ä¸ºdict
            config_dict = {
                field.name: getattr(self.config, field.name)
                for field in self.config.__dataclass_fields__.values()
            }
            json.dump(config_dict, f, indent=2)
        
        # é˜¶æ®µ1ï¼šå»å™ªå™¨è®­ç»ƒ
        stage1_stats = self.train_stage1(train_loader, val_loader)
        
        # é˜¶æ®µ2ï¼šè§£ç å™¨è®­ç»ƒ
        stage2_stats = self.train_stage2(train_loader, val_loader)
        
        # ä¿å­˜å®Œæ•´è®­ç»ƒç»Ÿè®¡
        final_stats = {
            'stage1': stage1_stats,
            'stage2': stage2_stats,
            'config': config_dict
        }
        
        stats_path = Path(self.config.output_dir) / "training_stats.json"
        with open(stats_path, 'w') as f:
            json.dump(final_stats, f, indent=2)
        
        logger.info("ğŸ‰ åˆ†ç¦»å¼è®­ç»ƒå®Œæˆï¼")
        logger.info(f"ç»Ÿè®¡æ•°æ®ä¿å­˜åˆ°: {stats_path}")
        logger.info(f"æ£€æŸ¥ç‚¹ä¿å­˜åˆ°: {self.config.checkpoint_dir}")
        
        return final_stats
    
    def load_stage1_checkpoint(self, checkpoint_path: str):
        """åŠ è½½é˜¶æ®µ1æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        logger.info(f"å·²åŠ è½½é˜¶æ®µ1æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    def get_training_summary(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒæ‘˜è¦"""
        summary = {
            'stage1': {
                'final_loss': self.training_stats['stage1']['losses'][-1] if self.training_stats['stage1']['losses'] else None,
                'best_val_loss': min(self.training_stats['stage1']['val_losses']) if self.training_stats['stage1']['val_losses'] else None,
                'total_epochs': len(self.training_stats['stage1']['losses'])
            },
            'stage2': {
                'final_loss': self.training_stats['stage2']['losses'][-1] if self.training_stats['stage2']['losses'] else None,
                'best_val_loss': min(self.training_stats['stage2']['val_losses']) if self.training_stats['stage2']['val_losses'] else None,
                'total_epochs': len(self.training_stats['stage2']['losses'])
            }
        }
        return summary