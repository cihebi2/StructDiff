#!/usr/bin/env python3
"""
CPL-Diffå¯å‘çš„åˆ†ç¦»å¼è®­ç»ƒç­–ç•¥
åŸºäºCPL-Diffçš„ä¸¤é˜¶æ®µè®­ç»ƒè®¾è®¡ï¼Œå°†å»å™ªå™¨è®­ç»ƒå’Œåºåˆ—è§£ç åˆ†ç¦»
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from typing import Dict, Optional, Tuple, Any, List
import logging
from pathlib import Path
import json
from dataclasses import dataclass
from tqdm import tqdm

from ..models.structdiff import StructDiff
from ..diffusion.gaussian_diffusion import GaussianDiffusion
from ..utils.logger import get_logger
from ..utils.checkpoint import CheckpointManager
from ..utils.ema import EMA

logger = get_logger(__name__)


@dataclass
class SeparatedTrainingConfig:
    """åˆ†ç¦»å¼è®­ç»ƒé…ç½®"""
    # é˜¶æ®µ1: å»å™ªå™¨è®­ç»ƒé…ç½®
    stage1_epochs: int = 200
    stage1_lr: float = 1e-4
    stage1_batch_size: int = 32
    stage1_gradient_clip: float = 1.0
    stage1_warmup_steps: int = 1000
    
    # é˜¶æ®µ2: è§£ç å™¨è®­ç»ƒé…ç½®  
    stage2_epochs: int = 100
    stage2_lr: float = 5e-5
    stage2_batch_size: int = 64
    stage2_gradient_clip: float = 0.5
    stage2_warmup_steps: int = 500
    
    # å…±åŒé…ç½®
    use_amp: bool = True
    use_ema: bool = True
    ema_decay: float = 0.9999
    save_every: int = 1000
    validate_every: int = 500
    log_every: int = 100
    
    # é•¿åº¦æ§åˆ¶
    use_length_control: bool = True
    max_length: int = 50
    min_length: int = 5
    
    # æ¡ä»¶æ§åˆ¶
    use_cfg: bool = True
    cfg_dropout_prob: float = 0.1
    cfg_guidance_scale: float = 2.0
    
    # æ•°æ®è·¯å¾„
    data_dir: str = "./data/processed"
    output_dir: str = "./outputs/separated_training"
    checkpoint_dir: str = "./checkpoints/separated"
    
    # è¯„ä¼°é…ç½®
    enable_evaluation: bool = True
    evaluate_every: int = 5
    evaluation_metrics: Optional[List[str]] = None
    evaluation_num_samples: int = 1000
    evaluation_guidance_scale: float = 2.0
    auto_generate_after_training: bool = True
    
    def __post_init__(self):
        if self.evaluation_metrics is None:
            self.evaluation_metrics = [
                "pseudo_perplexity",
                "plddt_score", 
                "instability_index",
                "similarity_score",
                "activity_prediction"
            ]


class SeparatedTrainingManager:
    """åˆ†ç¦»å¼è®­ç»ƒç®¡ç†å™¨"""
    
    def __init__(self, 
                 config: SeparatedTrainingConfig,
                 model: StructDiff,
                 diffusion: GaussianDiffusion,
                 device: str = 'cuda',
                 tokenizer=None):
        self.config = config
        self.model = model
        self.diffusion = diffusion
        self.device = device
        self.tokenizer = tokenizer
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(config.output_dir).mkdir(parents=True, exist_ok=True)
        Path(config.checkpoint_dir).mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.checkpoint_manager = CheckpointManager(config.checkpoint_dir)
        self.training_stats = {
            'stage1': {'losses': [], 'val_losses': [], 'evaluations': []},
            'stage2': {'losses': [], 'val_losses': [], 'evaluations': []},
        }
        
        # EMA
        if config.use_ema:
            self.ema = EMA(model, decay=config.ema_decay)
        else:
            self.ema = None
            
        # è¯„ä¼°å™¨ (å»¶è¿Ÿåˆå§‹åŒ–)
        self.evaluator = None
        if config.enable_evaluation:
            self._init_evaluator()
            
        logger.info(f"åˆå§‹åŒ–åˆ†ç¦»å¼è®­ç»ƒç®¡ç†å™¨ï¼Œè®¾å¤‡: {device}")
        if config.enable_evaluation:
            logger.info(f"è¯„ä¼°æŒ‡æ ‡: {config.evaluation_metrics}")
    
    def _init_evaluator(self):
        """åˆå§‹åŒ–CPL-Diffè¯„ä¼°å™¨"""
        try:
            # åŠ¨æ€å¯¼å…¥è¯„ä¼°å™¨
            import sys
            from pathlib import Path
            project_root = Path(__file__).parent.parent.parent
            sys.path.insert(0, str(project_root))
            
            from scripts.cpldiff_standard_evaluation import CPLDiffStandardEvaluator
            
            eval_output_dir = Path(self.config.output_dir) / "evaluations"
            self.evaluator = CPLDiffStandardEvaluator(output_dir=str(eval_output_dir))
            logger.info("âœ“ CPL-Diffè¯„ä¼°å™¨åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.warning(f"è¯„ä¼°å™¨åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†è·³è¿‡è¯„ä¼°")
            self.evaluator = None
            self.config.enable_evaluation = False
    
    def prepare_stage1_components(self) -> Tuple[nn.Module, torch.optim.Optimizer, Any]:
        """å‡†å¤‡é˜¶æ®µ1è®­ç»ƒç»„ä»¶ï¼ˆå»å™ªå™¨è®­ç»ƒï¼‰"""
        # å†»ç»“åºåˆ—ç¼–ç å™¨
        for param in self.model.sequence_encoder.parameters():
            param.requires_grad = False
        
        # åªè®­ç»ƒå»å™ªå™¨å’Œç»“æ„ç¼–ç å™¨
        trainable_params = []
        for name, param in self.model.named_parameters():
            if 'sequence_encoder' not in name:
                param.requires_grad = True
                trainable_params.append(param)
            else:
                param.requires_grad = False
        
        logger.info(f"é˜¶æ®µ1å¯è®­ç»ƒå‚æ•°æ•°é‡: {sum(p.numel() for p in trainable_params)}")
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(
            trainable_params,
            lr=self.config.stage1_lr,
            weight_decay=0.01,
            betas=(0.9, 0.999)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=self.config.stage1_epochs,
            eta_min=1e-6
        )
        
        return self.model, optimizer, scheduler
    
    def prepare_stage2_components(self) -> Tuple[nn.Module, torch.optim.Optimizer, Any]:
        """å‡†å¤‡é˜¶æ®µ2è®­ç»ƒç»„ä»¶ï¼ˆè§£ç å™¨è®­ç»ƒï¼‰"""
        # å†»ç»“å»å™ªå™¨å’Œç»“æ„ç¼–ç å™¨
        for name, param in self.model.named_parameters():
            if 'sequence_decoder' in name:
                param.requires_grad = True
            else:
                param.requires_grad = False
        
        # åªè®­ç»ƒåºåˆ—è§£ç å™¨
        decoder_params = [p for n, p in self.model.named_parameters() 
                         if 'sequence_decoder' in n and p.requires_grad]
        
        logger.info(f"é˜¶æ®µ2å¯è®­ç»ƒå‚æ•°æ•°é‡: {sum(p.numel() for p in decoder_params)}")
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(
            decoder_params,
            lr=self.config.stage2_lr,
            weight_decay=0.01,
            betas=(0.9, 0.999)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=self.config.stage2_epochs,
            eta_min=1e-6
        )
        
        return self.model, optimizer, scheduler
    
    def stage1_training_step(self, 
                           batch: Dict[str, torch.Tensor],
                           model: nn.Module,
                           optimizer: torch.optim.Optimizer) -> Dict[str, float]:
        """é˜¶æ®µ1è®­ç»ƒæ­¥éª¤ï¼šå»å™ªå™¨è®­ç»ƒ"""
        model.train()
        
        # å‡†å¤‡æ•°æ®
        sequences = batch['sequences'].to(self.device)
        attention_mask = batch['attention_mask'].to(self.device)
        structures = batch.get('structures', None)
        conditions = batch.get('conditions', None)
        
        # è·å–å›ºå®šçš„åºåˆ—åµŒå…¥ï¼ˆä¸è®¡ç®—æ¢¯åº¦ï¼‰
        with torch.no_grad():
            seq_embeddings = model.sequence_encoder(sequences, attention_mask)
            if hasattr(seq_embeddings, 'last_hidden_state'):
                seq_embeddings = seq_embeddings.last_hidden_state
        
        # éšæœºæ—¶é—´æ­¥
        batch_size = sequences.shape[0]
        timesteps = torch.randint(
            0, self.diffusion.num_timesteps, 
            (batch_size,), device=self.device
        )
        
        # æ·»åŠ å™ªå£°
        noise = torch.randn_like(seq_embeddings)
        noisy_embeddings = self.diffusion.q_sample(seq_embeddings, timesteps, noise)
        
        # å¤„ç†æ¡ä»¶ï¼ˆæ”¯æŒCFGï¼‰
        if self.config.use_cfg and conditions is not None:
            # éšæœºä¸¢å¼ƒéƒ¨åˆ†æ¡ä»¶
            dropout_mask = torch.rand(batch_size) < self.config.cfg_dropout_prob
            for key, value in conditions.items():
                if isinstance(value, torch.Tensor):
                    conditions[key] = value.clone()
                    conditions[key][dropout_mask] = -1  # æ— æ¡ä»¶æ ‡è®°
        
        # å»å™ªé¢„æµ‹
        if hasattr(model, 'denoiser'):
            predicted_noise_output = model.denoiser(
                noisy_embeddings, timesteps, attention_mask,
                structure_features=structures, conditions=conditions
            )
            # å»å™ªå™¨è¿”å›tuple (predicted_noise, cross_attention_weights)
            if isinstance(predicted_noise_output, tuple):
                predicted_noise, cross_attention_weights = predicted_noise_output
            else:
                predicted_noise = predicted_noise_output
        else:
            predicted_noise_output = model(
                noisy_embeddings, timesteps, attention_mask,
                structure_features=structures, conditions=conditions
            )
            # å¤„ç†å¯èƒ½çš„tupleè¿”å›å€¼
            if isinstance(predicted_noise_output, tuple):
                predicted_noise, _ = predicted_noise_output
            else:
                predicted_noise = predicted_noise_output
        
        # è®¡ç®—æŸå¤±ï¼ˆé¢„æµ‹å™ªå£°ï¼‰
        loss = F.mse_loss(predicted_noise, noise)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        if self.config.use_amp:
            from torch.cuda.amp import autocast, GradScaler
            scaler = GradScaler()
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(
                model.parameters(), self.config.stage1_gradient_clip
            )
            scaler.step(optimizer)
            scaler.update()
        else:
            loss.backward()
            torch.nn.utils.clip_grad_norm_(
                model.parameters(), self.config.stage1_gradient_clip
            )
            optimizer.step()
        
        # EMAæ›´æ–°
        if self.ema is not None:
            self.ema.update()
        
        return {
            'loss': loss.item(),
            'lr': optimizer.param_groups[0]['lr']
        }
    
    def stage2_training_step(self,
                           batch: Dict[str, torch.Tensor],
                           model: nn.Module,
                           optimizer: torch.optim.Optimizer) -> Dict[str, float]:
        """é˜¶æ®µ2è®­ç»ƒæ­¥éª¤ï¼šè§£ç å™¨è®­ç»ƒ"""
        model.train()
        
        # å‡†å¤‡æ•°æ®
        sequences = batch['sequences'].to(self.device)
        attention_mask = batch['attention_mask'].to(self.device)
        
        # è·å–å¹²å‡€çš„åºåˆ—åµŒå…¥ï¼ˆå›ºå®šç¼–ç å™¨ï¼‰
        with torch.no_grad():
            seq_embeddings = model.sequence_encoder(sequences, attention_mask)
            if hasattr(seq_embeddings, 'last_hidden_state'):
                seq_embeddings = seq_embeddings.last_hidden_state
        
        # åºåˆ—è§£ç 
        if hasattr(model, 'sequence_decoder'):
            logits = model.sequence_decoder(seq_embeddings, attention_mask)
        else:
            # å¦‚æœæ²¡æœ‰ç‹¬ç«‹è§£ç å™¨ï¼Œä½¿ç”¨åŸå§‹å‰å‘ä¼ æ’­
            logits = model.decode_sequences(seq_embeddings, attention_mask)
        
        # è®¡ç®—äº¤å‰ç†µæŸå¤±
        vocab_size = logits.size(-1)
        flat_logits = logits.view(-1, vocab_size)
        flat_targets = sequences.view(-1)
        
        # å¿½ç•¥paddingä½ç½®
        if attention_mask is not None:
            flat_mask = attention_mask.view(-1).bool()
            flat_logits = flat_logits[flat_mask]
            flat_targets = flat_targets[flat_mask]
        
        loss = F.cross_entropy(flat_logits, flat_targets)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(
            model.parameters(), self.config.stage2_gradient_clip
        )
        optimizer.step()
        
        return {
            'loss': loss.item(),
            'lr': optimizer.param_groups[0]['lr']
        }
    
    def validate_stage1(self, val_loader: DataLoader, model: nn.Module) -> Dict[str, float]:
        """é˜¶æ®µ1éªŒè¯"""
        model.eval()
        total_loss = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                sequences = batch['sequences'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                structures = batch.get('structures', None)
                conditions = batch.get('conditions', None)
                
                # è·å–åºåˆ—åµŒå…¥
                seq_embeddings = model.sequence_encoder(sequences, attention_mask)
                if hasattr(seq_embeddings, 'last_hidden_state'):
                    seq_embeddings = seq_embeddings.last_hidden_state
                
                # éšæœºæ—¶é—´æ­¥å’Œå™ªå£°
                batch_size = sequences.shape[0]
                timesteps = torch.randint(
                    0, self.diffusion.num_timesteps,
                    (batch_size,), device=self.device
                )
                noise = torch.randn_like(seq_embeddings)
                noisy_embeddings = self.diffusion.q_sample(seq_embeddings, timesteps, noise)
                
                # é¢„æµ‹
                if hasattr(model, 'denoiser'):
                    predicted_noise_output = model.denoiser(
                        noisy_embeddings, timesteps, attention_mask,
                        structure_features=structures, conditions=conditions
                    )
                    # å¤„ç†tupleè¿”å›å€¼
                    if isinstance(predicted_noise_output, tuple):
                        predicted_noise, _ = predicted_noise_output
                    else:
                        predicted_noise = predicted_noise_output
                else:
                    predicted_noise_output = model(
                        noisy_embeddings, timesteps, attention_mask,
                        structure_features=structures, conditions=conditions
                    )
                    # å¤„ç†tupleè¿”å›å€¼
                    if isinstance(predicted_noise_output, tuple):
                        predicted_noise, _ = predicted_noise_output
                    else:
                        predicted_noise = predicted_noise_output
                
                loss = F.mse_loss(predicted_noise, noise)
                total_loss += loss.item()
                num_batches += 1
        
        return {'val_loss': total_loss / max(num_batches, 1)}
    
    def validate_stage2(self, val_loader: DataLoader, model: nn.Module) -> Dict[str, float]:
        """é˜¶æ®µ2éªŒè¯"""
        model.eval()
        total_loss = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                sequences = batch['sequences'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                
                # è·å–åºåˆ—åµŒå…¥
                seq_embeddings = model.sequence_encoder(sequences, attention_mask)
                if hasattr(seq_embeddings, 'last_hidden_state'):
                    seq_embeddings = seq_embeddings.last_hidden_state
                
                # è§£ç 
                if hasattr(model, 'sequence_decoder'):
                    logits = model.sequence_decoder(seq_embeddings, attention_mask)
                else:
                    logits = model.decode_sequences(seq_embeddings, attention_mask)
                
                # è®¡ç®—æŸå¤±
                vocab_size = logits.size(-1)
                flat_logits = logits.view(-1, vocab_size)
                flat_targets = sequences.view(-1)
                
                if attention_mask is not None:
                    flat_mask = attention_mask.view(-1).bool()
                    flat_logits = flat_logits[flat_mask]
                    flat_targets = flat_targets[flat_mask]
                
                loss = F.cross_entropy(flat_logits, flat_targets)
                total_loss += loss.item()
                num_batches += 1
        
        return {'val_loss': total_loss / max(num_batches, 1)}
    
    def train_stage1(self, 
                    train_loader: DataLoader, 
                    val_loader: Optional[DataLoader] = None) -> Dict[str, List[float]]:
        """æ‰§è¡Œé˜¶æ®µ1è®­ç»ƒï¼šå»å™ªå™¨è®­ç»ƒ"""
        logger.info("ğŸš€ å¼€å§‹é˜¶æ®µ1è®­ç»ƒï¼šå»å™ªå™¨è®­ç»ƒï¼ˆå›ºå®šESMç¼–ç å™¨ï¼‰")
        
        model, optimizer, scheduler = self.prepare_stage1_components()
        
        # ä½¿ç”¨EMAæ¨¡å‹è¿›è¡ŒéªŒè¯
        eval_model = self.ema.ema_model if self.ema else model
        
        stage1_stats = {'losses': [], 'val_losses': []}
        
        for epoch in range(self.config.stage1_epochs):
            # è®­ç»ƒå¾ªç¯
            model.train()
            epoch_losses = []
            
            pbar = tqdm(train_loader, desc=f"é˜¶æ®µ1 Epoch {epoch+1}/{self.config.stage1_epochs}")
            for step, batch in enumerate(pbar):
                step_stats = self.stage1_training_step(batch, model, optimizer)
                epoch_losses.append(step_stats['loss'])
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'loss': f"{step_stats['loss']:.4f}",
                    'lr': f"{step_stats['lr']:.2e}"
                })
                
                # æ—¥å¿—è®°å½•
                if step % self.config.log_every == 0:
                    logger.info(f"é˜¶æ®µ1 Epoch {epoch+1}, Step {step}: "
                              f"Loss = {step_stats['loss']:.4f}, "
                              f"LR = {step_stats['lr']:.2e}")
                
                # éªŒè¯
                if val_loader and step % self.config.validate_every == 0:
                    val_stats = self.validate_stage1(val_loader, eval_model)
                    stage1_stats['val_losses'].append(val_stats['val_loss'])
                    logger.info(f"é˜¶æ®µ1éªŒè¯ - Val Loss: {val_stats['val_loss']:.4f}")
                    
                    # å®šæœŸè¯„ä¼°
                    if (self.config.enable_evaluation and self.evaluator is not None and 
                        epoch > 0 and epoch % self.config.evaluate_every == 0):
                        logger.info(f"ğŸ”¬ æ‰§è¡Œé˜¶æ®µ1å®šæœŸè¯„ä¼° (Epoch {epoch+1})...")
                        eval_results = self._run_evaluation(eval_model, stage=f'stage1_epoch_{epoch+1}')
                        if eval_results:
                            stage1_stats['evaluations'].append({
                                'epoch': epoch + 1,
                                'step': step,
                                'results': eval_results
                            })
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if step % self.config.save_every == 0:
                    checkpoint_state = {
                        'model': eval_model.state_dict(),
                        'optimizer': optimizer.state_dict(),
                        'scheduler': scheduler.state_dict(),
                        'epoch': epoch,
                        'step': step,
                        'stage': 'stage1',
                        'stats': stage1_stats,
                        'config': self.config
                    }
                    self.checkpoint_manager.save_checkpoint(
                        state_dict=checkpoint_state,
                        epoch=epoch,
                        is_best=(step % 5000 == 0)  # æ¯5000æ­¥æ ‡è®°ä¸€æ¬¡best
                    )
            
            # è®°å½•epochç»Ÿè®¡
            epoch_loss = sum(epoch_losses) / len(epoch_losses)
            stage1_stats['losses'].append(epoch_loss)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step()
            
            logger.info(f"é˜¶æ®µ1 Epoch {epoch+1} å®Œæˆ - å¹³å‡Loss: {epoch_loss:.4f}")
        
        # ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹
        self.checkpoint_manager.save_checkpoint(
            model=eval_model,
            optimizer=optimizer,
            scheduler=scheduler,
            epoch=self.config.stage1_epochs,
            step=0,
            stage='stage1_final',
            stats=stage1_stats
        )
        
        # é˜¶æ®µ1ç»“æŸåçš„è¯„ä¼°
        if self.config.enable_evaluation and self.evaluator is not None:
            logger.info("ğŸ”¬ æ‰§è¡Œé˜¶æ®µ1ç»“æŸè¯„ä¼°...")
            eval_results = self._run_evaluation(eval_model, stage='stage1_final')
            if eval_results:
                stage1_stats['final_evaluation'] = eval_results
        
        logger.info("âœ… é˜¶æ®µ1è®­ç»ƒå®Œæˆ")
        self.training_stats['stage1'] = stage1_stats
        return stage1_stats
    
    def train_stage2(self,
                    train_loader: DataLoader,
                    val_loader: Optional[DataLoader] = None) -> Dict[str, List[float]]:
        """æ‰§è¡Œé˜¶æ®µ2è®­ç»ƒï¼šè§£ç å™¨è®­ç»ƒ"""
        logger.info("ğŸš€ å¼€å§‹é˜¶æ®µ2è®­ç»ƒï¼šè§£ç å™¨è®­ç»ƒï¼ˆå›ºå®šå»å™ªå™¨ï¼‰")
        
        model, optimizer, scheduler = self.prepare_stage2_components()
        
        stage2_stats = {'losses': [], 'val_losses': []}
        
        for epoch in range(self.config.stage2_epochs):
            # è®­ç»ƒå¾ªç¯
            model.train()
            epoch_losses = []
            
            pbar = tqdm(train_loader, desc=f"é˜¶æ®µ2 Epoch {epoch+1}/{self.config.stage2_epochs}")
            for step, batch in enumerate(pbar):
                step_stats = self.stage2_training_step(batch, model, optimizer)
                epoch_losses.append(step_stats['loss'])
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'loss': f"{step_stats['loss']:.4f}",
                    'lr': f"{step_stats['lr']:.2e}"
                })
                
                # æ—¥å¿—è®°å½•
                if step % self.config.log_every == 0:
                    logger.info(f"é˜¶æ®µ2 Epoch {epoch+1}, Step {step}: "
                              f"Loss = {step_stats['loss']:.4f}, "
                              f"LR = {step_stats['lr']:.2e}")
                
                # éªŒè¯
                if val_loader and step % self.config.validate_every == 0:
                    val_stats = self.validate_stage2(val_loader, model)
                    stage2_stats['val_losses'].append(val_stats['val_loss'])
                    logger.info(f"é˜¶æ®µ2éªŒè¯ - Val Loss: {val_stats['val_loss']:.4f}")
                    
                    # å®šæœŸè¯„ä¼°
                    if (self.config.enable_evaluation and self.evaluator is not None and 
                        epoch > 0 and epoch % self.config.evaluate_every == 0):
                        logger.info(f"ğŸ”¬ æ‰§è¡Œé˜¶æ®µ2å®šæœŸè¯„ä¼° (Epoch {epoch+1})...")
                        eval_results = self._run_evaluation(model, stage=f'stage2_epoch_{epoch+1}')
                        if eval_results:
                            stage2_stats['evaluations'].append({
                                'epoch': epoch + 1,  
                                'step': step,
                                'results': eval_results
                            })
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if step % self.config.save_every == 0:
                    self.checkpoint_manager.save_checkpoint(
                        model=model,
                        optimizer=optimizer,
                        scheduler=scheduler,
                        epoch=epoch,
                        step=step,
                        stage='stage2',
                        stats=stage2_stats
                    )
            
            # è®°å½•epochç»Ÿè®¡
            epoch_loss = sum(epoch_losses) / len(epoch_losses)
            stage2_stats['losses'].append(epoch_loss)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step()
            
            logger.info(f"é˜¶æ®µ2 Epoch {epoch+1} å®Œæˆ - å¹³å‡Loss: {epoch_loss:.4f}")
        
        # ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹
        self.checkpoint_manager.save_checkpoint(
            model=model,
            optimizer=optimizer,
            scheduler=scheduler,
            epoch=self.config.stage2_epochs,
            step=0,
            stage='stage2_final',
            stats=stage2_stats
        )
        
        # é˜¶æ®µ2ç»“æŸåçš„è¯„ä¼°
        if self.config.enable_evaluation and self.evaluator is not None:
            logger.info("ğŸ”¬ æ‰§è¡Œé˜¶æ®µ2ç»“æŸè¯„ä¼°...")
            eval_results = self._run_evaluation(model, stage='stage2_final')
            if eval_results:
                stage2_stats['final_evaluation'] = eval_results
        
        logger.info("âœ… é˜¶æ®µ2è®­ç»ƒå®Œæˆ")
        self.training_stats['stage2'] = stage2_stats
        return stage2_stats
    
    def run_complete_training(self,
                            train_loader: DataLoader,
                            val_loader: Optional[DataLoader] = None) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„åˆ†ç¦»å¼è®­ç»ƒ"""
        logger.info("ğŸ¯ å¼€å§‹CPL-Diffå¯å‘çš„åˆ†ç¦»å¼è®­ç»ƒç­–ç•¥")
        
        # ä¿å­˜é…ç½®
        config_path = Path(self.config.output_dir) / "training_config.json"
        with open(config_path, 'w') as f:
            # å°†dataclassè½¬æ¢ä¸ºdictï¼Œç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯JSONå¯åºåˆ—åŒ–çš„
            config_dict = {}
            for field in self.config.__dataclass_fields__.values():
                value = getattr(self.config, field.name)
                # å¤„ç†å¯èƒ½çš„OmegaConfå¯¹è±¡
                if hasattr(value, '__iter__') and not isinstance(value, (str, bytes)):
                    try:
                        # å°è¯•è½¬æ¢ä¸ºåˆ—è¡¨
                        config_dict[field.name] = list(value)
                    except:
                        config_dict[field.name] = str(value)
                else:
                    config_dict[field.name] = value
            json.dump(config_dict, f, indent=2)
        
        # é˜¶æ®µ1ï¼šå»å™ªå™¨è®­ç»ƒ
        stage1_stats = self.train_stage1(train_loader, val_loader)
        
        # é˜¶æ®µ2ï¼šè§£ç å™¨è®­ç»ƒ
        stage2_stats = self.train_stage2(train_loader, val_loader)
        
        # ä¿å­˜å®Œæ•´è®­ç»ƒç»Ÿè®¡
        final_stats = {
            'stage1': stage1_stats,
            'stage2': stage2_stats,
            'config': config_dict
        }
        
        stats_path = Path(self.config.output_dir) / "training_stats.json"
        with open(stats_path, 'w') as f:
            json.dump(final_stats, f, indent=2)
        
        # è®­ç»ƒå®Œæˆåçš„è‡ªåŠ¨ç”Ÿæˆå’Œè¯„ä¼°
        if self.config.auto_generate_after_training:
            logger.info("ğŸ¯ å¼€å§‹è®­ç»ƒåè‡ªåŠ¨ç”Ÿæˆå’Œè¯„ä¼°...")
            generation_results = self._run_post_training_generation_and_evaluation()
            if generation_results:
                final_stats['post_training_evaluation'] = generation_results
        
        logger.info("ğŸ‰ åˆ†ç¦»å¼è®­ç»ƒå®Œæˆï¼")
        logger.info(f"ç»Ÿè®¡æ•°æ®ä¿å­˜åˆ°: {stats_path}")
        logger.info(f"æ£€æŸ¥ç‚¹ä¿å­˜åˆ°: {self.config.checkpoint_dir}")
        
        return final_stats
    
    def load_stage1_checkpoint(self, checkpoint_path: str):
        """åŠ è½½é˜¶æ®µ1æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        logger.info(f"å·²åŠ è½½é˜¶æ®µ1æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    def _run_evaluation(self, model, stage: str) -> Optional[Dict]:
        """è¿è¡ŒCPL-Diffè¯„ä¼°"""
        if not self.evaluator or not self.tokenizer:
            return None
            
        try:
            logger.info(f"ç”Ÿæˆæ ·æœ¬ç”¨äºè¯„ä¼° (é˜¶æ®µ: {stage})...")
            
            # ç”Ÿæˆè¯„ä¼°æ ·æœ¬
            generated_sequences = self._generate_evaluation_samples(
                model, 
                num_samples=min(100, self.config.evaluation_num_samples)
            )
            
            if not generated_sequences:
                logger.warning("ç”Ÿæˆæ ·æœ¬å¤±è´¥ï¼Œè·³è¿‡è¯„ä¼°")
                return None
            
            logger.info(f"å¯¹ {len(generated_sequences)} ä¸ªæ ·æœ¬è¿›è¡ŒCPL-Diffè¯„ä¼°...")
            
            # è¿è¡Œè¯„ä¼°
            eval_results = self.evaluator.comprehensive_cpldiff_evaluation(
                generated_sequences=generated_sequences,
                reference_sequences=[],  # ä½¿ç”¨å†…ç½®å‚è€ƒåºåˆ—
                peptide_type='antimicrobial'
            )
            
            # ç”ŸæˆæŠ¥å‘Š
            report_name = f"{stage}_evaluation"
            self.evaluator.generate_cpldiff_report(eval_results, report_name)
            
            logger.info(f"âœ“ {stage} è¯„ä¼°å®Œæˆ")
            return eval_results
            
        except Exception as e:
            logger.error(f"è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {e}")
            return None
    
    def _generate_evaluation_samples(self, model, num_samples: int = 100) -> List[str]:
        """ç”Ÿæˆç”¨äºè¯„ä¼°çš„æ ·æœ¬åºåˆ—"""
        try:
            sequences = []
            model.eval()
            
            with torch.no_grad():
                for i in range(0, num_samples, 10):  # æ‰¹æ¬¡ç”Ÿæˆ
                    batch_size = min(10, num_samples - i)
                    
                    # éšæœºé•¿åº¦
                    lengths = torch.randint(
                        self.config.min_length,
                        self.config.max_length + 1,
                        (batch_size,)
                    ).tolist()
                    
                    for length in lengths:
                        try:
                            # ç”Ÿæˆå™ªå£°åµŒå…¥
                            seq_embeddings = torch.randn(
                                1, length, 
                                getattr(model.sequence_encoder.config, 'hidden_size', 768),
                                device=self.device
                            )
                            attention_mask = torch.ones(1, length, device=self.device)
                            
                            # ç®€å•å»å™ª
                            if hasattr(model, 'denoiser'):
                                for t in reversed(range(0, 1000, 100)):
                                    timesteps = torch.tensor([t], device=self.device)
                                    noise_pred = model.denoiser(
                                        seq_embeddings, timesteps, attention_mask
                                    )
                                    seq_embeddings = seq_embeddings - 0.1 * noise_pred
                            
                            # è§£ç åºåˆ—
                            sequence = self._decode_for_evaluation(
                                seq_embeddings[0], attention_mask[0], length
                            )
                            
                            if sequence and len(sequence) >= self.config.min_length:
                                sequences.append(sequence)
                                
                        except Exception as e:
                            logger.debug(f"ç”Ÿæˆå•ä¸ªåºåˆ—å¤±è´¥: {e}")
                            continue
            
            return sequences[:num_samples]
            
        except Exception as e:
            logger.error(f"æ ·æœ¬ç”Ÿæˆå¤±è´¥: {e}")
            return []
    
    def _decode_for_evaluation(self, embeddings: torch.Tensor, attention_mask: torch.Tensor, target_length: int) -> str:
        """ä¸ºè¯„ä¼°è§£ç åºåˆ—"""
        try:
            if hasattr(self.model, 'sequence_decoder') and self.model.sequence_decoder is not None:
                logits = self.model.sequence_decoder(embeddings.unsqueeze(0), attention_mask.unsqueeze(0))
                token_ids = torch.argmax(logits, dim=-1).squeeze(0)
                
                # è½¬æ¢ä¸ºåºåˆ—
                if self.tokenizer:
                    sequence = self.tokenizer.decode(token_ids, skip_special_tokens=True)
                    amino_acids = set('ACDEFGHIKLMNPQRSTVWY')
                    clean_sequence = ''.join([c for c in sequence.upper() if c in amino_acids])
                    return clean_sequence[:target_length] if clean_sequence else None
            
            # å›é€€æ–¹æ¡ˆ
            import random
            amino_acids = 'ACDEFGHIKLMNPQRSTVWY'
            return ''.join(random.choices(amino_acids, k=target_length))
            
        except Exception as e:
            logger.debug(f"è§£ç å¤±è´¥: {e}")
            return None
    
    def _run_post_training_generation_and_evaluation(self) -> Optional[Dict]:
        """è®­ç»ƒå®Œæˆåè¿è¡Œç”Ÿæˆå’Œè¯„ä¼°"""
        try:
            logger.info("ğŸ¯ ç”Ÿæˆæœ€ç»ˆè¯„ä¼°æ ·æœ¬...")
            
            # ä½¿ç”¨æœ€ä½³æ¨¡å‹
            eval_model = self.ema.ema_model if self.ema else self.model
            
            # ç”Ÿæˆæ›´å¤šæ ·æœ¬
            generated_sequences = self._generate_evaluation_samples(
                eval_model, 
                num_samples=self.config.evaluation_num_samples
            )
            
            if not generated_sequences:
                logger.warning("æœ€ç»ˆç”Ÿæˆå¤±è´¥")
                return None
            
            logger.info(f"ç”Ÿæˆ {len(generated_sequences)} ä¸ªæœ€ç»ˆæ ·æœ¬")
            
            # ä¿å­˜ç”Ÿæˆçš„åºåˆ—
            output_path = Path(self.config.output_dir) / "final_generated_sequences.fasta"
            with open(output_path, 'w') as f:
                for i, seq in enumerate(generated_sequences):
                    f.write(f">Generated_{i}\n{seq}\n")
            logger.info(f"åºåˆ—ä¿å­˜åˆ°: {output_path}")
            
            # è¿è¡Œæœ€ç»ˆè¯„ä¼°
            if self.evaluator:
                logger.info("ğŸ”¬ è¿è¡Œæœ€ç»ˆCPL-Diffè¯„ä¼°...")
                eval_results = self.evaluator.comprehensive_cpldiff_evaluation(
                    generated_sequences=generated_sequences,
                    reference_sequences=[],
                    peptide_type='antimicrobial'
                )
                
                # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
                self.evaluator.generate_cpldiff_report(eval_results, "final_post_training")
                
                logger.info("âœ… æœ€ç»ˆè¯„ä¼°å®Œæˆ")
                return {
                    'generated_sequences_count': len(generated_sequences),
                    'sequences_file': str(output_path),
                    'evaluation_results': eval_results
                }
            
            return {
                'generated_sequences_count': len(generated_sequences),
                'sequences_file': str(output_path)
            }
            
        except Exception as e:
            logger.error(f"æœ€ç»ˆç”Ÿæˆå’Œè¯„ä¼°å¤±è´¥: {e}")
            return None
    
    def get_training_summary(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒæ‘˜è¦"""
        summary = {
            'stage1': {
                'final_loss': self.training_stats['stage1']['losses'][-1] if self.training_stats['stage1']['losses'] else None,
                'best_val_loss': min(self.training_stats['stage1']['val_losses']) if self.training_stats['stage1']['val_losses'] else None,
                'total_epochs': len(self.training_stats['stage1']['losses']),
                'evaluations_count': len(self.training_stats['stage1'].get('evaluations', []))
            },
            'stage2': {
                'final_loss': self.training_stats['stage2']['losses'][-1] if self.training_stats['stage2']['losses'] else None,
                'best_val_loss': min(self.training_stats['stage2']['val_losses']) if self.training_stats['stage2']['val_losses'] else None,
                'total_epochs': len(self.training_stats['stage2']['losses']),
                'evaluations_count': len(self.training_stats['stage2'].get('evaluations', []))
            }
        }
        return summary