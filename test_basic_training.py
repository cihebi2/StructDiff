#!/usr/bin/env python3
"""
åŸºç¡€StructDiffè®­ç»ƒæµ‹è¯•è„šæœ¬
æµ‹è¯•æœ€ç®€åŒ–çš„åºåˆ—æ‰©æ•£æ¨¡åž‹è®­ç»ƒï¼Œå®Œå…¨ç¦ç”¨ç»“æž„ç‰¹å¾
"""

import os
import sys
import torch
import numpy as np
from pathlib import Path
from omegaconf import OmegaConf
from torch.nn.utils.rnn import pad_sequence

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from structdiff.models.structdiff import StructDiff
from structdiff.diffusion.gaussian_diffusion import GaussianDiffusion
from structdiff.data.dataset import PeptideStructureDataset
from torch.utils.data import DataLoader
from transformers import AutoTokenizer

def custom_collate_fn(batch):
    """è‡ªå®šä¹‰collateå‡½æ•°ï¼Œå¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—"""
    # èŽ·å–æœ€å¤§é•¿åº¦
    max_len = max(item['sequences'].shape[0] for item in batch)
    
    # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
    sequences = []
    attention_masks = []
    labels = []
    
    for item in batch:
        seq = item['sequences']
        # æˆªæ–­æˆ–å¡«å……åˆ°å›ºå®šé•¿åº¦
        if seq.shape[0] > max_len:
            seq = seq[:max_len]
        elif seq.shape[0] < max_len:
            # ä½¿ç”¨pad_token_idå¡«å……ï¼ˆé€šå¸¸æ˜¯0ï¼‰
            pad_length = max_len - seq.shape[0]
            seq = torch.cat([seq, torch.zeros(pad_length, dtype=seq.dtype)], dim=0)
        
        sequences.append(seq)
        
        # åˆ›å»ºattention mask
        attention_mask = torch.ones(max_len, dtype=torch.bool)
        if item['sequences'].shape[0] < max_len:
            attention_mask[item['sequences'].shape[0]:] = False
        attention_masks.append(attention_mask)
        
        labels.append(item['label'])
    
    return {
        'sequences': torch.stack(sequences),
        'attention_mask': torch.stack(attention_masks),
        'peptide_type': torch.stack(labels)  # ä¸ºäº†ä¸Žæ¨¡åž‹æŽ¥å£ä¸€è‡´ï¼Œå‘½åä¸ºpeptide_type
    }

def test_basic_model():
    """æµ‹è¯•åŸºç¡€æ¨¡åž‹åˆå§‹åŒ–å’Œå‰å‘ä¼ æ’­"""
    print("ðŸ§ª æµ‹è¯•åŸºç¡€æ¨¡åž‹åˆå§‹åŒ–...")
    
    # è®¾å¤‡ - å…ˆç”¨CPUæµ‹è¯•
    device = torch.device('cpu')  # æš‚æ—¶ä½¿ç”¨CPU
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æœ€ç®€åŒ–çš„é…ç½®
    config = OmegaConf.create({
        "model": {
            "type": "StructDiff",
            "sequence_encoder": {
                "pretrained_model": "facebook/esm2_t6_8M_UR50D",
                "freeze_encoder": True,  # å›ºå®šç¼–ç å™¨
                "use_lora": False
            },
            "structure_encoder": {
                "type": "multi_scale",
                "hidden_dim": 256,
                "use_esmfold": False,  # å®Œå…¨ç¦ç”¨
                "local": {
                    "hidden_dim": 256,
                    "num_layers": 2,
                    "kernel_sizes": [3, 5],
                    "dropout": 0.1
                },
                "global": {
                    "hidden_dim": 256,
                    "num_attention_heads": 4,
                    "num_layers": 2,
                    "dropout": 0.1
                },
                "fusion": {
                    "method": "attention",
                    "hidden_dim": 256
                }
            },
            "denoiser": {
                "hidden_dim": 320,  # åŒ¹é…ESM2_t6_8M
                "num_layers": 4,    # å‡å°‘å±‚æ•°
                "num_heads": 4,     # å‡å°‘æ³¨æ„åŠ›å¤´
                "dropout": 0.1,
                "use_cross_attention": False  # ç¦ç”¨ç»“æž„äº¤å‰æ³¨æ„åŠ›
            }
        },
        "diffusion": {
            "num_timesteps": 100,  # å‡å°‘æ—¶é—´æ­¥
            "noise_schedule": "linear",
            "beta_start": 0.0001,
            "beta_end": 0.02
        },
        "data": {
            "max_length": 50,
            "use_predicted_structures": False  # å®Œå…¨ç¦ç”¨ç»“æž„ç‰¹å¾
        }
    })
    
    try:
        # åˆ›å»ºæ¨¡åž‹
        model = StructDiff(config.model).to(device)
        print(f"âœ… æ¨¡åž‹åˆå§‹åŒ–æˆåŠŸï¼Œå‚æ•°æ•°é‡: {model.count_parameters():,}")
        
        # åˆ›å»ºæ‰©æ•£è¿‡ç¨‹
        diffusion = GaussianDiffusion(
            num_timesteps=config.diffusion.num_timesteps,
            noise_schedule=config.diffusion.noise_schedule,
            beta_start=config.diffusion.beta_start,
            beta_end=config.diffusion.beta_end
        )
        print("âœ… æ‰©æ•£è¿‡ç¨‹åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        batch_size = 2
        seq_length = 20
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_sequences = torch.randint(0, 32, (batch_size, seq_length + 2), device=device)  # +2 for CLS/SEP
        attention_mask = torch.ones(batch_size, seq_length + 2, device=device)
        timesteps = torch.randint(0, config.diffusion.num_timesteps, (batch_size,), device=device)
        conditions = {'peptide_type': torch.randint(0, 3, (batch_size,), device=device)}
        
        print("ðŸ”„ æµ‹è¯•å‰å‘ä¼ æ’­...")
        with torch.no_grad():
            outputs = model(
                sequences=test_sequences,
                attention_mask=attention_mask,
                timesteps=timesteps,
                structures=None,  # æ— ç»“æž„ç‰¹å¾
                conditions=conditions,
                return_loss=False
            )
        
        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"   è¾“å‡ºåµŒå…¥å½¢çŠ¶: {outputs['denoised_embeddings'].shape}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½"""
    print("\nðŸ“Š æµ‹è¯•æ•°æ®åŠ è½½...")
    
    try:
        # å®Œæ•´é…ç½®ç»“æž„ - ä¿®å¤é…ç½®è·¯å¾„é—®é¢˜
        config = OmegaConf.create({
            "data": {
                "max_length": 50,
                "use_predicted_structures": False
            },
            "model": {
                "sequence_encoder": {
                    "pretrained_model": "facebook/esm2_t6_8M_UR50D"
                },
                "structure_encoder": {
                    "use_esmfold": False
                }
            }
        })
        
        # åˆ›å»ºæ•°æ®é›†ï¼ˆç¦ç”¨ç»“æž„é¢„æµ‹ï¼‰
        dataset = PeptideStructureDataset(
            data_path="data/processed/train.csv",
            config=config,
            is_training=True
        )
        
        print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸï¼Œæ ·æœ¬æ•°: {len(dataset)}")
        
        # æµ‹è¯•æ•°æ®åŠ è½½å™¨ - ä½¿ç”¨è‡ªå®šä¹‰collateå‡½æ•°
        dataloader = DataLoader(
            dataset,
            batch_size=2,
            shuffle=False,
            num_workers=0,  # ç¦ç”¨å¤šè¿›ç¨‹
            pin_memory=False,
            collate_fn=custom_collate_fn  # æ·»åŠ è‡ªå®šä¹‰collateå‡½æ•°
        )
        
        # æµ‹è¯•èŽ·å–ä¸€ä¸ªæ‰¹æ¬¡
        for batch in dataloader:
            print("âœ… æ•°æ®æ‰¹æ¬¡èŽ·å–æˆåŠŸ")
            print(f"   åºåˆ—å½¢çŠ¶: {batch['sequences'].shape}")
            print(f"   æ³¨æ„åŠ›æŽ©ç å½¢çŠ¶: {batch['attention_mask'].shape}")
            print(f"   æ ‡ç­¾: {batch['peptide_type']}")
            print(f"   æ ‡ç­¾èŒƒå›´: {batch['peptide_type'].min()}-{batch['peptide_type'].max()}")
            break
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_single_training_step():
    """æµ‹è¯•å•ä¸ªè®­ç»ƒæ­¥éª¤"""
    print("\nðŸ‹ï¸ æµ‹è¯•å•ä¸ªè®­ç»ƒæ­¥éª¤...")
    
    device = torch.device('cpu')  # å…ˆç”¨CPUæµ‹è¯•
    
    try:
        # ä½¿ç”¨æ›´ç®€å•çš„é…ç½®
        config = OmegaConf.create({
            "model": {
                "sequence_encoder": {
                    "pretrained_model": "facebook/esm2_t6_8M_UR50D",
                    "freeze_encoder": True,
                    "use_lora": False
                },
                "structure_encoder": {
                    "use_esmfold": False,
                    "hidden_dim": 256
                },
                "denoiser": {
                    "hidden_dim": 320,
                    "num_layers": 2,
                    "num_heads": 4,
                    "dropout": 0.1,
                    "use_cross_attention": False
                }
            },
            "diffusion": {
                "num_timesteps": 100,
                "noise_schedule": "linear",
                "beta_start": 0.0001,
                "beta_end": 0.02
            }
        })
        
        # åˆ›å»ºæ¨¡åž‹å’Œä¼˜åŒ–å™¨
        model = StructDiff(config.model).to(device)
        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
        
        # åˆ›å»ºæ‰©æ•£è¿‡ç¨‹
        diffusion = GaussianDiffusion(
            num_timesteps=config.diffusion.num_timesteps,
            noise_schedule=config.diffusion.noise_schedule,
            beta_start=config.diffusion.beta_start,
            beta_end=config.diffusion.beta_end
        )
        
        # æ¨¡æ‹Ÿæ‰¹æ¬¡æ•°æ®
        batch_size = 2
        seq_length = 20
        
        # åˆ›å»ºåºåˆ—åµŒå…¥ï¼ˆæ¨¡æ‹ŸESMè¾“å‡ºï¼‰
        seq_embeddings = torch.randn(batch_size, seq_length, 320, device=device)
        attention_mask = torch.ones(batch_size, seq_length, device=device)
        conditions = {'peptide_type': torch.tensor([0, 1], device=device)}
        
        # æ‰©æ•£å‰å‘è¿‡ç¨‹
        timesteps = torch.randint(0, config.diffusion.num_timesteps, (batch_size,), device=device)
        noise = torch.randn_like(seq_embeddings)
        noisy_embeddings = diffusion.q_sample(seq_embeddings, timesteps, noise)
        
        # è®­ç»ƒæ­¥éª¤
        model.train()
        optimizer.zero_grad()
        
        # é¢„æµ‹å™ªå£°
        predicted_noise = model.denoiser(
            noisy_embeddings=noisy_embeddings,
            timesteps=timesteps,
            attention_mask=attention_mask,
            structure_features=None,
            conditions=conditions
        )[0]  # åªå–é¢„æµ‹ç»“æžœï¼Œå¿½ç•¥æ³¨æ„åŠ›æƒé‡
        
        # è®¡ç®—æŸå¤±
        loss = torch.nn.functional.mse_loss(predicted_noise, noise)
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        print(f"âœ… è®­ç»ƒæ­¥éª¤æˆåŠŸï¼ŒæŸå¤±: {loss.item():.6f}")
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒæ­¥éª¤æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ðŸ§ª å¼€å§‹StructDiffåŸºç¡€è®­ç»ƒæµ‹è¯•")
    print("=" * 50)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    tests = [
        ("æ¨¡åž‹åˆå§‹åŒ–", test_basic_model),
        ("æ•°æ®åŠ è½½", test_data_loading),
        ("è®­ç»ƒæ­¥éª¤", test_single_training_step),
    ]
    
    results = []
    for test_name, test_func in tests:
        print(f"\nðŸ” æ‰§è¡Œæµ‹è¯•: {test_name}")
        success = test_func()
        results.append((test_name, success))
    
    # æ€»ç»“
    print("\n" + "=" * 50)
    print("ðŸŽ¯ æµ‹è¯•ç»“æžœæ€»ç»“:")
    for test_name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"  {test_name}: {status}")
    
    all_passed = all(success for _, success in results)
    if all_passed:
        print("\nðŸŽ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¯ä»¥å¼€å§‹æ­£å¼è®­ç»ƒã€‚")
    else:
        print("\nâš ï¸ å­˜åœ¨å¤±è´¥çš„æµ‹è¯•ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¿®å¤ã€‚")
    
    return all_passed

if __name__ == "__main__":
    main() 