#!/usr/bin/env python3
"""
ESMFoldç”Ÿäº§ç¯å¢ƒæµ‹è¯•è„šæœ¬
éªŒè¯ç»“æ„é¢„æµ‹åŠŸèƒ½åœ¨å½“å‰ç¡¬ä»¶ç¯å¢ƒä¸‹çš„å¯ç”¨æ€§
"""

import os
import sys
import torch
import numpy as np
import time
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['CUDA_VISIBLE_DEVICES'] = '2'  # ä½¿ç”¨GPU 2è¿›è¡Œæµ‹è¯•

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from structdiff.models.esmfold_wrapper import ESMFoldWrapper
from structdiff.utils.logger import setup_logger, get_logger

setup_logger(level="INFO")
logger = get_logger(__name__)

def test_esmfold_basic():
    """åŸºç¡€ESMFoldåŠŸèƒ½æµ‹è¯•"""
    logger.info("ğŸ§ª å¼€å§‹ESMFoldåŸºç¡€åŠŸèƒ½æµ‹è¯•")
    
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æµ‹è¯•åºåˆ—
    test_sequences = [
        "MKTFFGREDLG",  # çŸ­åºåˆ— (11 aa)
        "MKTFFGREDLGKYKLLACYRGFQDLFETKGFFEDEPKLLNQGYRKQVKMLPGFDPFLFRRWCNMSCF",  # ä¸­ç­‰é•¿åº¦ (65 aa)
    ]
    
    try:
        # åˆå§‹åŒ–ESMFold
        logger.info("åˆå§‹åŒ–ESMFoldåŒ…è£…å™¨...")
        start_time = time.time()
        esmfold = ESMFoldWrapper(device=device)
        init_time = time.time() - start_time
        
        logger.info(f"ESMFoldåˆå§‹åŒ–è€—æ—¶: {init_time:.2f}s")
        logger.info(f"ESMFoldå¯ç”¨æ€§: {esmfold.available}")
        
        if not esmfold.available:
            logger.error("âŒ ESMFoldä¸å¯ç”¨ï¼Œæµ‹è¯•å¤±è´¥")
            return False
        
        # æµ‹è¯•ä¸åŒé•¿åº¦çš„åºåˆ—
        for i, sequence in enumerate(test_sequences):
            logger.info(f"\nğŸ§ª æµ‹è¯•åºåˆ— {i+1}: é•¿åº¦ {len(sequence)}")
            
            # é¢„æµ‹ç»“æ„
            start_time = time.time()
            features = esmfold.predict_structure(sequence)
            pred_time = time.time() - start_time
            
            logger.info(f"é¢„æµ‹è€—æ—¶: {pred_time:.2f}s")
            
            # éªŒè¯è¾“å‡º
            assert 'positions' in features, "ç¼ºå°‘positionsç‰¹å¾"
            assert 'plddt' in features, "ç¼ºå°‘plddtç‰¹å¾"
            assert 'distance_matrix' in features, "ç¼ºå°‘distance_matrixç‰¹å¾"
            assert 'angles' in features, "ç¼ºå°‘anglesç‰¹å¾"
            
            # æ£€æŸ¥å½¢çŠ¶
            seq_len = len(sequence)
            assert features['positions'].shape[0] == seq_len, f"positionså½¢çŠ¶é”™è¯¯: {features['positions'].shape}"
            assert features['plddt'].shape[0] == seq_len, f"plddtå½¢çŠ¶é”™è¯¯: {features['plddt'].shape}"
            assert features['distance_matrix'].shape == (seq_len, seq_len), f"distance_matrixå½¢çŠ¶é”™è¯¯: {features['distance_matrix'].shape}"
            assert features['angles'].shape == (seq_len, 10), f"angleså½¢çŠ¶é”™è¯¯: {features['angles'].shape}"
            
            # æ£€æŸ¥æ•°å€¼èŒƒå›´
            assert torch.all(features['plddt'] >= 0) and torch.all(features['plddt'] <= 100), "plddtå€¼è¶…å‡ºèŒƒå›´"
            assert torch.all(features['distance_matrix'] >= 0), "distance_matrixåŒ…å«è´Ÿå€¼"
            
            logger.info(f"âœ“ åºåˆ— {i+1} æµ‹è¯•é€šè¿‡")
            logger.info(f"  - å¹³å‡pLDDT: {features['plddt'].mean().item():.2f}")
            logger.info(f"  - ä½ç½®èŒƒå›´: {features['positions'].min().item():.2f} - {features['positions'].max().item():.2f}")
        
        logger.info("âœ… ESMFoldåŸºç¡€åŠŸèƒ½æµ‹è¯•å…¨éƒ¨é€šè¿‡")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ESMFoldæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def test_memory_usage():
    """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    logger.info("\nğŸ§ª å¼€å§‹å†…å­˜ä½¿ç”¨æµ‹è¯•")
    
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    
    if device.type == 'cuda':
        torch.cuda.empty_cache()
        initial_memory = torch.cuda.memory_allocated(device)
        logger.info(f"åˆå§‹GPUå†…å­˜: {initial_memory / 1024**3:.2f} GB")
    
    try:
        # åˆå§‹åŒ–ESMFold
        esmfold = ESMFoldWrapper(device=device)
        
        if device.type == 'cuda':
            after_init_memory = torch.cuda.memory_allocated(device)
            init_memory_usage = (after_init_memory - initial_memory) / 1024**3
            logger.info(f"ESMFoldåŠ è½½åGPUå†…å­˜: {after_init_memory / 1024**3:.2f} GB")
            logger.info(f"ESMFoldå†…å­˜å ç”¨: {init_memory_usage:.2f} GB")
        
        if not esmfold.available:
            logger.warning("ESMFoldä¸å¯ç”¨ï¼Œè·³è¿‡å†…å­˜æµ‹è¯•")
            return True
        
        # æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°
        test_sequence = "MKTFFGREDLGKYKLLACYRGFQDLFETKGFFEDEPKLLNQGYRKQV"
        
        for batch_size in [1, 2, 4]:
            logger.info(f"\næµ‹è¯•æ‰¹æ¬¡å¤§å°: {batch_size}")
            
            if device.type == 'cuda':
                torch.cuda.empty_cache()
                before_memory = torch.cuda.memory_allocated(device)
            
            # æ¨¡æ‹Ÿæ‰¹æ¬¡å¤„ç†
            for i in range(batch_size):
                features = esmfold.predict_structure(test_sequence)
            
            if device.type == 'cuda':
                after_memory = torch.cuda.memory_allocated(device)
                batch_memory = (after_memory - before_memory) / 1024**3
                logger.info(f"æ‰¹æ¬¡å¤„ç†åGPUå†…å­˜: {after_memory / 1024**3:.2f} GB")
                logger.info(f"æ‰¹æ¬¡å†…å­˜å¢é‡: {batch_memory:.2f} GB")
        
        logger.info("âœ… å†…å­˜ä½¿ç”¨æµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ å†…å­˜æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_cache_strategy():
    """æµ‹è¯•ç¼“å­˜ç­–ç•¥"""
    logger.info("\nğŸ§ª å¼€å§‹ç¼“å­˜ç­–ç•¥æµ‹è¯•")
    
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    
    try:
        esmfold = ESMFoldWrapper(device=device)
        
        if not esmfold.available:
            logger.warning("ESMFoldä¸å¯ç”¨ï¼Œè·³è¿‡ç¼“å­˜æµ‹è¯•")
            return True
        
        test_sequence = "MKTFFGREDLGKYKLL"
        
        # ç¬¬ä¸€æ¬¡é¢„æµ‹ï¼ˆæ— ç¼“å­˜ï¼‰
        start_time = time.time()
        features1 = esmfold.predict_structure(test_sequence)
        first_time = time.time() - start_time
        
        # ç¬¬äºŒæ¬¡é¢„æµ‹ï¼ˆå¯èƒ½æœ‰ç¼“å­˜ï¼‰
        start_time = time.time()
        features2 = esmfold.predict_structure(test_sequence)
        second_time = time.time() - start_time
        
        logger.info(f"ç¬¬ä¸€æ¬¡é¢„æµ‹è€—æ—¶: {first_time:.2f}s")
        logger.info(f"ç¬¬äºŒæ¬¡é¢„æµ‹è€—æ—¶: {second_time:.2f}s")
        
        # éªŒè¯ç»“æœä¸€è‡´æ€§
        assert torch.allclose(features1['plddt'], features2['plddt'], atol=1e-5), "ç¼“å­˜ç»“æœä¸ä¸€è‡´"
        
        logger.info("âœ… ç¼“å­˜ç­–ç•¥æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ç¼“å­˜æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†"""
    logger.info("\nğŸ§ª å¼€å§‹é”™è¯¯å¤„ç†æµ‹è¯•")
    
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    
    try:
        esmfold = ESMFoldWrapper(device=device)
        
        # æµ‹è¯•ç©ºåºåˆ—
        empty_features = esmfold.predict_structure("")
        assert empty_features is not None, "ç©ºåºåˆ—å¤„ç†å¤±è´¥"
        
        # æµ‹è¯•æ— æ•ˆå­—ç¬¦
        invalid_sequence = "MKTFFGREDLGXYZ"  # åŒ…å«æ— æ•ˆæ°¨åŸºé…¸
        invalid_features = esmfold.predict_structure(invalid_sequence)
        assert invalid_features is not None, "æ— æ•ˆåºåˆ—å¤„ç†å¤±è´¥"
        
        # æµ‹è¯•è¶…é•¿åºåˆ—
        long_sequence = "M" * 1000  # 1000ä¸ªæ°¨åŸºé…¸
        long_features = esmfold.predict_structure(long_sequence)
        assert long_features is not None, "è¶…é•¿åºåˆ—å¤„ç†å¤±è´¥"
        
        logger.info("âœ… é”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        logger.error(f"âŒ é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹ESMFoldç”Ÿäº§ç¯å¢ƒæµ‹è¯•")
    
    # GPUä¿¡æ¯
    if torch.cuda.is_available():
        logger.info(f"GPUè®¾å¤‡: {torch.cuda.get_device_name()}")
        logger.info(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        logger.info("ä½¿ç”¨CPUæ¨¡å¼")
    
    tests = [
        ("åŸºç¡€åŠŸèƒ½æµ‹è¯•", test_esmfold_basic),
        ("å†…å­˜ä½¿ç”¨æµ‹è¯•", test_memory_usage),
        ("ç¼“å­˜ç­–ç•¥æµ‹è¯•", test_cache_strategy),
        ("é”™è¯¯å¤„ç†æµ‹è¯•", test_error_handling),
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        logger.info(f"\n{'='*50}")
        logger.info(f"å¼€å§‹: {test_name}")
        logger.info(f"{'='*50}")
        
        try:
            result = test_func()
            results[test_name] = result
            
            if result:
                logger.info(f"âœ… {test_name} é€šè¿‡")
            else:
                logger.error(f"âŒ {test_name} å¤±è´¥")
                
        except Exception as e:
            logger.error(f"âŒ {test_name} å¼‚å¸¸: {e}")
            results[test_name] = False
    
    # æ€»ç»“
    logger.info(f"\n{'='*50}")
    logger.info("æµ‹è¯•æ€»ç»“")
    logger.info(f"{'='*50}")
    
    passed = sum(results.values())
    total = len(results)
    
    for test_name, result in results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        logger.info(f"{test_name}: {status}")
    
    logger.info(f"\næ€»ä½“ç»“æœ: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ESMFoldç”Ÿäº§ç¯å¢ƒå°±ç»ª")
        return True
    else:
        logger.error("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 