#!/usr/bin/env python3
"""
ç®€åŒ–çš„è¯„ä¼°æŒ‡æ ‡æµ‹è¯•è„šæœ¬
åªæµ‹è¯•è¯„ä¼°åŠŸèƒ½ï¼Œä¸éœ€è¦å®Œæ•´çš„StructDiffæ¨¡å‹
"""

import os
import sys
import torch
import logging
import math
from pathlib import Path
from collections import Counter
from statistics import mean, stdev
import tempfile

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å°è¯•å¯¼å…¥æ‰€éœ€çš„åº“
try:
    from transformers import EsmTokenizer, EsmModel
    ESM_AVAILABLE = True
except ImportError:
    ESM_AVAILABLE = False
    print("âš ï¸ transformers not available, pseudo-perplexity will be skipped")

try:
    from Bio import Align
    BIO_AVAILABLE = True
except ImportError:
    BIO_AVAILABLE = False
    print("âš ï¸ Biopython not available, BLOSUM62 similarity will be skipped")

try:
    from modlamp.descriptors import GlobalDescriptor
    MODLAMP_AVAILABLE = True
except ImportError:
    MODLAMP_AVAILABLE = False
    print("âš ï¸ modlamp not available, instability index will be skipped")

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class SimpleEvaluator:
    """ç®€åŒ–çš„è¯„ä¼°å™¨ï¼ŒåªåŒ…å«è¯„ä¼°åŠŸèƒ½"""
    
    def __init__(self, device='cpu'):
        self.device = device
        self.amino_acids = 'ACDEFGHIKLMNPQRSTVWY'
        
        # åˆå§‹åŒ–ESM2æ¨¡å‹
        self.esm_tokenizer = None
        self.esm_model = None
        if ESM_AVAILABLE:
            self._init_esm_model()
        
        # åˆå§‹åŒ–BLOSUM62æ¯”å¯¹å™¨
        self.aligner = None
        if BIO_AVAILABLE:
            self._init_aligner()
    
    def _init_esm_model(self):
        """åˆå§‹åŒ–ESM2æ¨¡å‹"""
        try:
            logger.info("ğŸ”¬ åˆå§‹åŒ–ESM2æ¨¡å‹...")
            self.esm_tokenizer = EsmTokenizer.from_pretrained('facebook/esm2_t6_8M_UR50D')
            self.esm_model = EsmModel.from_pretrained('facebook/esm2_t6_8M_UR50D').to(self.device)
            self.esm_model.eval()
            logger.info("âœ… ESM2æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ ESM2æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.esm_tokenizer = None
            self.esm_model = None
    
    def _init_aligner(self):
        """åˆå§‹åŒ–BLOSUM62æ¯”å¯¹å™¨"""
        try:
            self.aligner = Align.PairwiseAligner()
            self.aligner.substitution_matrix = Align.substitution_matrices.load("BLOSUM62")
            self.aligner.open_gap_score = -10
            self.aligner.extend_gap_score = -0.5
            logger.info("âœ… BLOSUM62æ¯”å¯¹å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ BLOSUM62æ¯”å¯¹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.aligner = None
    
    def evaluate_pseudo_perplexity(self, sequences):
        """è®¡ç®—ä¼ªå›°æƒ‘åº¦"""
        if not ESM_AVAILABLE or self.esm_tokenizer is None or self.esm_model is None:
            logger.warning("âš ï¸ ESM2æ¨¡å‹æœªå¯ç”¨ï¼Œè·³è¿‡ä¼ªå›°æƒ‘åº¦è®¡ç®—")
            return {'mean_pseudo_perplexity': 0.0, 'std_pseudo_perplexity': 0.0}
        
        logger.info("ğŸ§® è®¡ç®—ä¼ªå›°æƒ‘åº¦...")
        pseudo_perplexities = []
        
        with torch.no_grad():
            for seq in sequences:
                try:
                    # å¯¹åºåˆ—è¿›è¡Œç¼–ç 
                    inputs = self.esm_tokenizer(seq, return_tensors='pt', padding=True, truncation=True)
                    input_ids = inputs['input_ids'].to(self.device)
                    attention_mask = inputs['attention_mask'].to(self.device)
                    
                    seq_len = input_ids.size(1)
                    total_loss = 0.0
                    valid_positions = 0
                    
                    # é€ä¸ªä½ç½®è¿›è¡Œæ©ç é¢„æµ‹
                    for pos in range(1, seq_len - 1):  # è·³è¿‡CLSå’ŒSEP token
                        if attention_mask[0, pos] == 1:  # åªå¤„ç†æœ‰æ•ˆä½ç½®
                            # åˆ›å»ºæ©ç ç‰ˆæœ¬
                            masked_input = input_ids.clone()
                            original_token = masked_input[0, pos].item()
                            masked_input[0, pos] = self.esm_tokenizer.mask_token_id
                            
                            # é¢„æµ‹
                            outputs = self.esm_model(masked_input, attention_mask=attention_mask)
                            logits = outputs.last_hidden_state[0, pos]
                            
                            # è®¡ç®—äº¤å‰ç†µæŸå¤±
                            loss = torch.nn.functional.cross_entropy(
                                logits.unsqueeze(0), 
                                torch.tensor([original_token], device=self.device)
                            )
                            total_loss += loss.item()
                            valid_positions += 1
                    
                    if valid_positions > 0:
                        avg_loss = total_loss / valid_positions
                        pseudo_perplexity = math.exp(avg_loss)
                        pseudo_perplexities.append(pseudo_perplexity)
                
                except Exception as e:
                    logger.warning(f"è®¡ç®—åºåˆ—ä¼ªå›°æƒ‘åº¦å¤±è´¥: {e}")
                    continue
        
        if pseudo_perplexities:
            return {
                'mean_pseudo_perplexity': mean(pseudo_perplexities),
                'std_pseudo_perplexity': stdev(pseudo_perplexities) if len(pseudo_perplexities) > 1 else 0.0,
                'valid_sequences': len(pseudo_perplexities)
            }
        else:
            return {'mean_pseudo_perplexity': 0.0, 'std_pseudo_perplexity': 0.0, 'valid_sequences': 0}
    
    def evaluate_shannon_entropy(self, sequences):
        """è®¡ç®—Shannonä¿¡æ¯ç†µ"""
        logger.info("ğŸ“Š è®¡ç®—Shannonä¿¡æ¯ç†µ...")
        
        # è®¡ç®—æ¯ä¸ªåºåˆ—çš„ç†µ
        sequence_entropies = []
        for seq in sequences:
            aa_counts = Counter(seq)
            total_aa = len(seq)
            
            entropy = 0.0
            for count in aa_counts.values():
                prob = count / total_aa
                if prob > 0:
                    entropy -= prob * math.log2(prob)
            
            sequence_entropies.append(entropy)
        
        # è®¡ç®—æ•´ä½“æ°¨åŸºé…¸åˆ†å¸ƒçš„ç†µ
        all_aa = ''.join(sequences)
        overall_aa_counts = Counter(all_aa)
        total_aa = len(all_aa)
        
        overall_entropy = 0.0
        for count in overall_aa_counts.values():
            prob = count / total_aa
            if prob > 0:
                overall_entropy -= prob * math.log2(prob)
        
        return {
            'mean_sequence_entropy': mean(sequence_entropies) if sequence_entropies else 0.0,
            'std_sequence_entropy': stdev(sequence_entropies) if len(sequence_entropies) > 1 else 0.0,
            'overall_entropy': overall_entropy,
            'max_possible_entropy': math.log2(20)
        }
    
    def evaluate_instability_index(self, sequences):
        """è®¡ç®—ä¸ç¨³å®šæ€§æŒ‡æ•°"""
        if not MODLAMP_AVAILABLE:
            logger.warning("âš ï¸ modlampæœªå®‰è£…ï¼Œè·³è¿‡ä¸ç¨³å®šæ€§æŒ‡æ•°è®¡ç®—")
            return {'mean_instability_index': 0.0, 'std_instability_index': 0.0}
        
        logger.info("ğŸ§ª è®¡ç®—ä¸ç¨³å®šæ€§æŒ‡æ•°...")
        
        # åˆ›å»ºä¸´æ—¶FASTAæ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode='w', suffix='.fasta', delete=False) as tmp_file:
            for i, seq in enumerate(sequences):
                tmp_file.write(f">seq_{i}\n{seq}\n")
            tmp_file_path = tmp_file.name
        
        try:
            # ä½¿ç”¨modlampè®¡ç®—ä¸ç¨³å®šæ€§æŒ‡æ•°
            desc = GlobalDescriptor(tmp_file_path)
            desc.instability_index()
            instability_scores = desc.descriptor.flatten()
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(tmp_file_path)
            
            return {
                'mean_instability_index': mean(instability_scores),
                'std_instability_index': stdev(instability_scores) if len(instability_scores) > 1 else 0.0,
                'stable_peptides': sum(1 for score in instability_scores if score <= 40),
                'unstable_peptides': sum(1 for score in instability_scores if score > 40)
            }
        
        except Exception as e:
            logger.warning(f"è®¡ç®—ä¸ç¨³å®šæ€§æŒ‡æ•°å¤±è´¥: {e}")
            if os.path.exists(tmp_file_path):
                os.unlink(tmp_file_path)
            return {'mean_instability_index': 0.0, 'std_instability_index': 0.0}
    
    def evaluate_similarity_to_training(self, sequences, reference_sequences):
        """è®¡ç®—BLOSUM62ç›¸ä¼¼æ€§å¾—åˆ†"""
        if not BIO_AVAILABLE or self.aligner is None:
            logger.warning("âš ï¸ BLOSUM62æ¯”å¯¹å™¨æœªå¯ç”¨ï¼Œè·³è¿‡ç›¸ä¼¼æ€§è®¡ç®—")
            return {'mean_similarity_score': 0.0, 'std_similarity_score': 0.0}
        
        if not reference_sequences:
            logger.warning("âš ï¸ æœªæä¾›å‚è€ƒåºåˆ—ï¼Œè·³è¿‡ç›¸ä¼¼æ€§è®¡ç®—")
            return {'mean_similarity_score': 0.0, 'std_similarity_score': 0.0}
        
        logger.info("ğŸ” è®¡ç®—BLOSUM62ç›¸ä¼¼æ€§å¾—åˆ†...")
        
        similarity_scores = []
        
        for gen_seq in sequences:
            seq_scores = []
            
            # ä¸å‚è€ƒåºåˆ—é›†åˆä¸­çš„æ¯ä¸ªåºåˆ—è¿›è¡Œæ¯”å¯¹
            for ref_seq in reference_sequences[:10]:  # é™åˆ¶å‚è€ƒåºåˆ—æ•°é‡
                try:
                    alignments = self.aligner.align(gen_seq, ref_seq)
                    if alignments:
                        score = alignments.score
                        # æ ‡å‡†åŒ–å¾—åˆ†
                        normalized_score = score / max(len(gen_seq), len(ref_seq))
                        seq_scores.append(normalized_score)
                except Exception as e:
                    continue
            
            if seq_scores:
                max_similarity = max(seq_scores)
                similarity_scores.append(max_similarity)
        
        if similarity_scores:
            return {
                'mean_similarity_score': mean(similarity_scores),
                'std_similarity_score': stdev(similarity_scores) if len(similarity_scores) > 1 else 0.0,
                'max_similarity_score': max(similarity_scores),
                'min_similarity_score': min(similarity_scores)
            }
        else:
            return {'mean_similarity_score': 0.0, 'std_similarity_score': 0.0}
    
    def evaluate_diversity_metrics(self, sequences):
        """å¤šæ ·æ€§è¯„ä¼°"""
        logger.info("ğŸ“ˆ è®¡ç®—å¤šæ ·æ€§æŒ‡æ ‡...")
        
        # å»é‡æ¯”ä¾‹
        unique_sequences = set(sequences)
        uniqueness_ratio = len(unique_sequences) / len(sequences) if sequences else 0.0
        
        # é•¿åº¦åˆ†å¸ƒ
        lengths = [len(seq) for seq in sequences]
        length_stats = {
            'mean_length': mean(lengths) if lengths else 0.0,
            'std_length': stdev(lengths) if len(lengths) > 1 else 0.0,
            'min_length': min(lengths) if lengths else 0,
            'max_length': max(lengths) if lengths else 0,
            'length_range': max(lengths) - min(lengths) if lengths else 0
        }
        
        # æ°¨åŸºé…¸é¢‘ç‡åˆ†æ
        all_aa = ''.join(sequences)
        aa_counts = Counter(all_aa)
        total_aa = len(all_aa)
        
        aa_frequencies = {}
        for aa in self.amino_acids:
            aa_frequencies[f'freq_{aa}'] = aa_counts.get(aa, 0) / total_aa if total_aa > 0 else 0.0
        
        # è®¡ç®—æ°¨åŸºé…¸ä½¿ç”¨çš„å‡åŒ€æ€§ï¼ˆåŸºå°¼ç³»æ•°ï¼‰
        frequencies = [aa_counts.get(aa, 0) / total_aa for aa in self.amino_acids if total_aa > 0]
        if frequencies:
            frequencies.sort()
            n = len(frequencies)
            gini = sum((2 * i - n - 1) * freq for i, freq in enumerate(frequencies, 1)) / (n * sum(frequencies))
        else:
            gini = 0.0
        
        return {
            'uniqueness_ratio': uniqueness_ratio,
            'total_sequences': len(sequences),
            'unique_sequences': len(unique_sequences),
            'duplicate_sequences': len(sequences) - len(unique_sequences),
            'length_distribution': length_stats,
            'amino_acid_frequencies': aa_frequencies,
            'amino_acid_gini_coefficient': gini
        }

def test_evaluation_metrics():
    """æµ‹è¯•è¯„ä¼°æŒ‡æ ‡"""
    logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•ä¸“ä¸šç”Ÿç‰©å­¦è¯„ä¼°æŒ‡æ ‡...")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = SimpleEvaluator(device)
    
    # æµ‹è¯•åºåˆ—
    test_sequences = [
        "KWKLFKKIEKVGQNIRDGIIKAGPAVAVVGQATQIAK",  # æŠ—èŒè‚½
        "FLPIIAKFFSKVM",  # æŠ—çœŸèŒè‚½
        "GLLSKLWKKVFKAFKKFLKK",  # æŠ—ç—…æ¯’è‚½
        "ACDEFGHIKLMNPQRSTVWY",  # åŒ…å«æ‰€æœ‰æ°¨åŸºé…¸
        "AAAAAAAAAAAAAAAAAAAA",  # å•ä¸€æ°¨åŸºé…¸
        "KWKLFKKIEKVGQNIR",  # çŸ­åºåˆ—
        "KWKLFKKIEKVGQNIRDGIIKAGPAVAVVGQATQIAKWKLFKKIEKVGQNIR"  # é•¿åºåˆ—
    ]
    
    logger.info(f"æµ‹è¯•åºåˆ—æ•°é‡: {len(test_sequences)}")
    
    # å‚è€ƒåºåˆ—
    reference_sequences = [
        "KWKLFKKIEKVGQNIRDGIIKAGPAVAVVGQATQIAK",
        "FLPIIAKFFSKVM",
        "GLLSKLWKKVFKAFKKFLKK",
        "ACDEFGHIKLMNPQRSTVWY"
    ]
    
    # è¿è¡Œæ‰€æœ‰è¯„ä¼°
    results = {}
    
    # 1. ä¼ªå›°æƒ‘åº¦
    try:
        results['pseudo_perplexity'] = evaluator.evaluate_pseudo_perplexity(test_sequences)
    except Exception as e:
        logger.warning(f"ä¼ªå›°æƒ‘åº¦è®¡ç®—å¤±è´¥: {e}")
        results['pseudo_perplexity'] = {'mean_pseudo_perplexity': 0.0, 'std_pseudo_perplexity': 0.0}
    
    # 2. Shannonç†µ
    try:
        results['shannon_entropy'] = evaluator.evaluate_shannon_entropy(test_sequences)
    except Exception as e:
        logger.warning(f"Shannonç†µè®¡ç®—å¤±è´¥: {e}")
        results['shannon_entropy'] = {'mean_sequence_entropy': 0.0, 'overall_entropy': 0.0}
    
    # 3. ä¸ç¨³å®šæ€§æŒ‡æ•°
    try:
        results['instability_index'] = evaluator.evaluate_instability_index(test_sequences)
    except Exception as e:
        logger.warning(f"ä¸ç¨³å®šæ€§æŒ‡æ•°è®¡ç®—å¤±è´¥: {e}")
        results['instability_index'] = {'mean_instability_index': 0.0, 'std_instability_index': 0.0}
    
    # 4. BLOSUM62ç›¸ä¼¼æ€§
    try:
        results['blosum62_similarity'] = evaluator.evaluate_similarity_to_training(
            test_sequences, reference_sequences
        )
    except Exception as e:
        logger.warning(f"BLOSUM62ç›¸ä¼¼æ€§è®¡ç®—å¤±è´¥: {e}")
        results['blosum62_similarity'] = {'mean_similarity_score': 0.0, 'std_similarity_score': 0.0}
    
    # 5. å¤šæ ·æ€§åˆ†æ
    try:
        results['diversity_analysis'] = evaluator.evaluate_diversity_metrics(test_sequences)
    except Exception as e:
        logger.warning(f"å¤šæ ·æ€§åˆ†æå¤±è´¥: {e}")
        results['diversity_analysis'] = {'uniqueness_ratio': 0.0}
    
    # æ‰“å°ç»“æœ
    logger.info("\n" + "="*80)
    logger.info("ğŸ¯ ä¸“ä¸šç”Ÿç‰©å­¦è¯„ä¼°æŒ‡æ ‡æµ‹è¯•ç»“æœ")
    logger.info("="*80)
    
    # 1. ä¼ªå›°æƒ‘åº¦
    if 'pseudo_perplexity' in results:
        pp = results['pseudo_perplexity']
        logger.info(f"ğŸ§® ä¼ªå›°æƒ‘åº¦ (Pseudo-Perplexity):")
        logger.info(f"   å¹³å‡å€¼: {pp.get('mean_pseudo_perplexity', 0):.4f} Â± {pp.get('std_pseudo_perplexity', 0):.4f}")
        logger.info(f"   æœ‰æ•ˆåºåˆ—: {pp.get('valid_sequences', 0)}")
    
    # 2. Shannonä¿¡æ¯ç†µ
    if 'shannon_entropy' in results:
        se = results['shannon_entropy']
        logger.info(f"ğŸ“Š Shannonä¿¡æ¯ç†µ:")
        logger.info(f"   åºåˆ—å¹³å‡ç†µ: {se.get('mean_sequence_entropy', 0):.4f} Â± {se.get('std_sequence_entropy', 0):.4f}")
        logger.info(f"   æ•´ä½“ç†µ: {se.get('overall_entropy', 0):.4f} / {se.get('max_possible_entropy', 4.32):.2f}")
    
    # 3. ä¸ç¨³å®šæ€§æŒ‡æ•°
    if 'instability_index' in results:
        ii = results['instability_index']
        logger.info(f"ğŸ§ª ä¸ç¨³å®šæ€§æŒ‡æ•° (Instability Index):")
        logger.info(f"   å¹³å‡å€¼: {ii.get('mean_instability_index', 0):.4f} Â± {ii.get('std_instability_index', 0):.4f}")
        stable = ii.get('stable_peptides', 0)
        unstable = ii.get('unstable_peptides', 0)
        total = stable + unstable
        if total > 0:
            logger.info(f"   ç¨³å®šè‚½ (â‰¤40): {stable}/{total} ({stable/total*100:.1f}%)")
            logger.info(f"   ä¸ç¨³å®šè‚½ (>40): {unstable}/{total} ({unstable/total*100:.1f}%)")
    
    # 4. BLOSUM62ç›¸ä¼¼æ€§
    if 'blosum62_similarity' in results:
        bs = results['blosum62_similarity']
        logger.info(f"ğŸ” BLOSUM62ç›¸ä¼¼æ€§å¾—åˆ†:")
        logger.info(f"   å¹³å‡ç›¸ä¼¼æ€§: {bs.get('mean_similarity_score', 0):.4f} Â± {bs.get('std_similarity_score', 0):.4f}")
        if 'max_similarity_score' in bs:
            logger.info(f"   æœ€é«˜ç›¸ä¼¼æ€§: {bs['max_similarity_score']:.4f}")
            logger.info(f"   æœ€ä½ç›¸ä¼¼æ€§: {bs['min_similarity_score']:.4f}")
    
    # 5. å¤šæ ·æ€§åˆ†æ
    if 'diversity_analysis' in results:
        da = results['diversity_analysis']
        logger.info(f"ğŸ“ˆ å¤šæ ·æ€§åˆ†æ:")
        logger.info(f"   å”¯ä¸€æ€§æ¯”ä¾‹: {da.get('uniqueness_ratio', 0):.4f}")
        logger.info(f"   æ€»åºåˆ—æ•°: {da.get('total_sequences', 0)}")
        logger.info(f"   å”¯ä¸€åºåˆ—æ•°: {da.get('unique_sequences', 0)}")
        logger.info(f"   é‡å¤åºåˆ—æ•°: {da.get('duplicate_sequences', 0)}")
        
        if 'length_distribution' in da:
            ld = da['length_distribution']
            logger.info(f"   é•¿åº¦åˆ†å¸ƒ: {ld.get('mean_length', 0):.1f} Â± {ld.get('std_length', 0):.1f}")
            logger.info(f"   é•¿åº¦èŒƒå›´: {ld.get('min_length', 0)}-{ld.get('max_length', 0)}")
        
        gini = da.get('amino_acid_gini_coefficient', 0)
        logger.info(f"   æ°¨åŸºé…¸åˆ†å¸ƒå‡åŒ€æ€§ (Gini): {gini:.4f} (0=å‡åŒ€, 1=ä¸å‡åŒ€)")
    
    logger.info("ğŸ‰ ä¸“ä¸šç”Ÿç‰©å­¦è¯„ä¼°æŒ‡æ ‡æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    test_evaluation_metrics() 