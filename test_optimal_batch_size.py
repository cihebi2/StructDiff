#!/usr/bin/env python3
"""
æ‰¹æ¬¡å¤§å°ä¼˜åŒ–æµ‹è¯•è„šæœ¬
ç”¨äºæ‰¾åˆ°æœ€ä¼˜çš„æ‰¹æ¬¡å¤§å°ä»¥æå‡GPUåˆ©ç”¨ç‡
"""

import os
import sys
import time
import torch
import torch.nn as nn
import gc
import json
from datetime import datetime
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/home/qlyu/sequence/StructDiff-7.0.0')

from src.models.struct_diff import StructDiff
from src.data.peptide_dataset import PeptideDataset
from src.data.collator import PeptideCollator
from src.utils.esmfold_wrapper import ESMFoldWrapper
from src.utils.config import Config

def create_test_batch(batch_size, dataset, collator):
    """åˆ›å»ºæµ‹è¯•æ‰¹æ¬¡"""
    # ä»æ•°æ®é›†ä¸­éšæœºé€‰æ‹©æ ·æœ¬
    indices = torch.randperm(len(dataset))[:batch_size]
    samples = [dataset[i] for i in indices]
    
    # ä½¿ç”¨collatorå¤„ç†æ‰¹æ¬¡
    batch = collator(samples)
    
    # ç§»åŠ¨åˆ°GPU
    for key in batch:
        if torch.is_tensor(batch[key]):
            batch[key] = batch[key].cuda()
    
    return batch

def test_batch_sizes():
    """æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°çš„æ€§èƒ½"""
    
    print("ğŸš€ å¼€å§‹æ‰¹æ¬¡å¤§å°ä¼˜åŒ–æµ‹è¯•...")
    print("=" * 60)
    
    # åˆå§‹åŒ–é…ç½®
    config = Config()
    config.model.structure_prediction_enabled = True
    config.training.batch_size = 2  # åŸºç¡€æ‰¹æ¬¡å¤§å°
    
    # åˆå§‹åŒ–ESMFold
    print("ğŸ“¥ åˆå§‹åŒ–ESMFold...")
    esmfold_wrapper = ESMFoldWrapper(device="cuda:1")
    
    # åˆå§‹åŒ–æ¨¡å‹
    print("ğŸ”§ åˆå§‹åŒ–æ¨¡å‹...")
    model = StructDiff(config.model)
    model.cuda()
    model.train()
    
    # å¼ºåˆ¶è®¾ç½®å…±äº«ESMFoldå®ä¾‹
    model.esmfold_wrapper = esmfold_wrapper
    
    # åˆå§‹åŒ–æ•°æ®é›†
    print("ğŸ“Š åˆå§‹åŒ–æ•°æ®é›†...")
    train_dataset = PeptideDataset(
        data_path=config.data.train_data_path,
        max_length=config.data.max_length,
        structure_prediction_enabled=config.model.structure_prediction_enabled
    )
    
    collator = PeptideCollator(
        max_length=config.data.max_length,
        structure_prediction_enabled=config.model.structure_prediction_enabled
    )
    
    # åˆå§‹åŒ–ä¼˜åŒ–å™¨å’ŒæŸå¤±ç¼©æ”¾å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    scaler = GradScaler()
    
    # æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°
    batch_sizes = [2, 4, 6, 8, 10, 12, 16]
    results = {}
    
    print("\nğŸ§ª å¼€å§‹æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°...")
    print("-" * 60)
    
    for bs in batch_sizes:
        print(f"\nğŸ“ æµ‹è¯•æ‰¹æ¬¡å¤§å°: {bs}")
        
        try:
            # æ¸…ç†GPUå†…å­˜
            torch.cuda.empty_cache()
            gc.collect()
            
            # è®°å½•åˆå§‹å†…å­˜
            initial_memory = torch.cuda.memory_allocated() / 1e9
            
            # åˆ›å»ºæµ‹è¯•æ‰¹æ¬¡
            test_batch = create_test_batch(bs, train_dataset, collator)
            
            # åˆ›å»ºéšæœºæ—¶é—´æ­¥
            timesteps = torch.randint(0, 1000, (bs,), device='cuda')
            
            # æµ‹è¯•å‰å‘ä¼ æ’­
            start_time = time.time()
            
            with autocast():
                outputs = model(
                    sequences=test_batch['sequences'],
                    attention_mask=test_batch['attention_mask'],
                    timesteps=timesteps,
                    structures=test_batch.get('structures'),
                    conditions=test_batch.get('conditions'),
                    return_loss=True
                )
                loss = outputs['total_loss']
            
            forward_time = time.time() - start_time
            
            # æµ‹è¯•åå‘ä¼ æ’­
            backward_start = time.time()
            scaler.scale(loss).backward()
            backward_time = time.time() - backward_start
            
            total_time = forward_time + backward_time
            peak_memory = torch.cuda.memory_allocated() / 1e9
            memory_used = peak_memory - initial_memory
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            throughput = bs / total_time
            samples_per_second = bs / total_time
            
            results[bs] = {
                'forward_time': forward_time,
                'backward_time': backward_time,
                'total_time': total_time,
                'memory_used': memory_used,
                'peak_memory': peak_memory,
                'throughput': throughput,
                'samples_per_second': samples_per_second,
                'loss': loss.item()
            }
            
            print(f"  âœ… æˆåŠŸ")
            print(f"  â±ï¸  æ€»æ—¶é—´: {total_time:.2f}s (å‰å‘: {forward_time:.2f}s, åå‘: {backward_time:.2f}s)")
            print(f"  ğŸ’¾ å†…å­˜ä½¿ç”¨: {memory_used:.1f}GB (å³°å€¼: {peak_memory:.1f}GB)")
            print(f"  ğŸš€ ååé‡: {throughput:.2f} samples/s")
            print(f"  ğŸ“‰ æŸå¤±: {loss.item():.6f}")
            
            # æ¸…ç†æ¢¯åº¦
            optimizer.zero_grad()
            
            # æ£€æŸ¥å†…å­˜æ˜¯å¦è¶…å‡ºé™åˆ¶
            if peak_memory > 22:  # ä¿æŒ2GBå®‰å…¨è¾¹ç•Œ
                print(f"  âš ï¸  å†…å­˜ä½¿ç”¨è¿‡é«˜ ({peak_memory:.1f}GB > 22GB)")
                
        except RuntimeError as e:
            if "out of memory" in str(e):
                print(f"  âŒ å†…å­˜ä¸è¶³: {str(e)}")
                results[bs] = {'error': 'OOM', 'message': str(e)}
                break
            else:
                print(f"  âŒ å…¶ä»–é”™è¯¯: {str(e)}")
                results[bs] = {'error': 'Other', 'message': str(e)}
                break
    
    # åˆ†æç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœåˆ†æ")
    print("=" * 60)
    
    valid_results = {k: v for k, v in results.items() if 'error' not in v}
    
    if valid_results:
        # æ‰¾åˆ°æœ€ä¼˜æ‰¹æ¬¡å¤§å°
        best_throughput = max(valid_results.values(), key=lambda x: x['throughput'])
        best_bs_throughput = [k for k, v in valid_results.items() if v['throughput'] == best_throughput['throughput']][0]
        
        # æ‰¾åˆ°å†…å­˜æ•ˆç‡æœ€é«˜çš„æ‰¹æ¬¡å¤§å°
        memory_efficient = min(valid_results.values(), key=lambda x: x['memory_used'] / x['throughput'])
        best_bs_memory = [k for k, v in valid_results.items() if v == memory_efficient][0]
        
        print(f"\nğŸ† æœ€ä¼˜æ‰¹æ¬¡å¤§å°åˆ†æ:")
        print(f"  ğŸš€ æœ€é«˜ååé‡: æ‰¹æ¬¡å¤§å° {best_bs_throughput} ({best_throughput['throughput']:.2f} samples/s)")
        print(f"  ğŸ’¾ å†…å­˜æ•ˆç‡æœ€é«˜: æ‰¹æ¬¡å¤§å° {best_bs_memory} ({memory_efficient['throughput']:.2f} samples/s, {memory_efficient['memory_used']:.1f}GB)")
        
        # æ¨èæ‰¹æ¬¡å¤§å°
        recommended_bs = best_bs_throughput
        if valid_results[best_bs_throughput]['peak_memory'] > 20:
            # å¦‚æœæœ€é«˜ååé‡çš„æ‰¹æ¬¡å¤§å°å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œé€‰æ‹©å†…å­˜æ•ˆç‡é«˜çš„
            recommended_bs = best_bs_memory
        
        print(f"\nğŸ’¡ æ¨èæ‰¹æ¬¡å¤§å°: {recommended_bs}")
        print(f"  - ååé‡: {valid_results[recommended_bs]['throughput']:.2f} samples/s")
        print(f"  - å†…å­˜ä½¿ç”¨: {valid_results[recommended_bs]['memory_used']:.1f}GB")
        print(f"  - æ€»æ—¶é—´: {valid_results[recommended_bs]['total_time']:.2f}s")
        
        # é¢„æœŸæ€§èƒ½æå‡
        current_bs = 2
        if current_bs in valid_results and recommended_bs in valid_results:
            current_perf = valid_results[current_bs]['throughput']
            new_perf = valid_results[recommended_bs]['throughput']
            improvement = (new_perf / current_perf - 1) * 100
            
            print(f"\nğŸ“ˆ é¢„æœŸæ€§èƒ½æå‡:")
            print(f"  - å½“å‰æ‰¹æ¬¡å¤§å° {current_bs}: {current_perf:.2f} samples/s")
            print(f"  - æ¨èæ‰¹æ¬¡å¤§å° {recommended_bs}: {new_perf:.2f} samples/s")
            print(f"  - æ€§èƒ½æå‡: {improvement:.1f}%")
    
    # è¯¦ç»†ç»“æœè¡¨æ ¼
    print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ:")
    print("-" * 100)
    print(f"{'æ‰¹æ¬¡å¤§å°':<8} {'æ€»æ—¶é—´(s)':<10} {'å†…å­˜(GB)':<10} {'ååé‡(s/s)':<12} {'çŠ¶æ€':<10}")
    print("-" * 100)
    
    for bs, result in results.items():
        if 'error' in result:
            print(f"{bs:<8} {'N/A':<10} {'N/A':<10} {'N/A':<12} {result['error']:<10}")
        else:
            print(f"{bs:<8} {result['total_time']:<10.2f} {result['memory_used']:<10.1f} {result['throughput']:<12.2f} {'æˆåŠŸ':<10}")
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"batch_size_optimization_results_{timestamp}.json"
    
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    return results

if __name__ == "__main__":
    try:
        results = test_batch_sizes()
        print("\nâœ… æ‰¹æ¬¡å¤§å°ä¼˜åŒ–æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc() 