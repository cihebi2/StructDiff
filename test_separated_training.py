#!/usr/bin/env python3
"""
åˆ†ç¦»å¼è®­ç»ƒæµ‹è¯•è„šæœ¬
éªŒè¯CPL-Diffå¯å‘çš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥
"""

import torch
import numpy as np
import pandas as pd
from pathlib import Path
import sys
import tempfile
import shutil
from typing import Dict, List

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from structdiff.training.separated_training import SeparatedTrainingManager, SeparatedTrainingConfig
from structdiff.training.length_controller import (
    LengthDistributionAnalyzer, AdaptiveLengthController, 
    LengthAwareDataCollator, create_length_controller_from_data
)
from structdiff.models.structdiff import StructDiff
from structdiff.diffusion.gaussian_diffusion import GaussianDiffusion
from structdiff.data.dataset import PeptideStructureDataset
from structdiff.utils.logger import setup_logger, get_logger
from omegaconf import OmegaConf
from transformers import AutoTokenizer
from torch.utils.data import DataLoader

# è®¾ç½®æ—¥å¿—
setup_logger()
logger = get_logger(__name__)


def create_test_data(data_dir: Path, num_samples: int = 100):
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    logger.info(f"åˆ›å»ºæµ‹è¯•æ•°æ®åˆ°: {data_dir}")
    
    # ç”Ÿæˆæ¨¡æ‹Ÿè‚½æ®µåºåˆ—
    amino_acids = "ACDEFGHIKLMNPQRSTVWY"
    sequences = []
    peptide_types = []
    
    type_length_prefs = {
        'antimicrobial': (20, 5),
        'antifungal': (25, 7),
        'antiviral': (30, 8)
    }
    
    for i in range(num_samples):
        # éšæœºé€‰æ‹©è‚½æ®µç±»å‹
        peptide_type = np.random.choice(['antimicrobial', 'antifungal', 'antiviral'])
        peptide_types.append(peptide_type)
        
        # æ ¹æ®ç±»å‹ç”Ÿæˆé•¿åº¦
        mean_len, std_len = type_length_prefs[peptide_type]
        length = max(5, min(50, int(np.random.normal(mean_len, std_len))))
        
        # ç”Ÿæˆåºåˆ—
        sequence = ''.join(np.random.choice(list(amino_acids), length))
        sequences.append(sequence)
    
    # åˆ›å»ºDataFrame
    data = pd.DataFrame({
        'id': [f'peptide_{i:04d}' for i in range(num_samples)],
        'sequence': sequences,
        'peptide_type': peptide_types
    })
    
    # åˆ†å‰²æ•°æ®
    train_size = int(0.8 * num_samples)
    val_size = int(0.1 * num_samples)
    
    train_data = data[:train_size]
    val_data = data[train_size:train_size+val_size]
    test_data = data[train_size+val_size:]
    
    # ä¿å­˜æ•°æ®
    data_dir.mkdir(parents=True, exist_ok=True)
    train_data.to_csv(data_dir / "train.csv", index=False)
    val_data.to_csv(data_dir / "val.csv", index=False)
    test_data.to_csv(data_dir / "test.csv", index=False)
    
    logger.info(f"ç”Ÿæˆæ•°æ®: è®­ç»ƒé›† {len(train_data)}, éªŒè¯é›† {len(val_data)}, æµ‹è¯•é›† {len(test_data)}")
    return train_data, val_data, test_data


def test_length_controller():
    """æµ‹è¯•é•¿åº¦æ§åˆ¶å™¨"""
    logger.info("ğŸ§ª æµ‹è¯•é•¿åº¦æ§åˆ¶å™¨")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        train_data, _, _ = create_test_data(temp_path)
        
        # æµ‹è¯•é•¿åº¦åˆ†å¸ƒåˆ†æå™¨
        analyzer = LengthDistributionAnalyzer(str(temp_path / "train.csv"))
        distributions = analyzer.analyze_training_data()
        
        assert len(distributions) == 3  # ä¸‰ç§è‚½æ®µç±»å‹
        logger.info("âœ… é•¿åº¦åˆ†å¸ƒåˆ†ææµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•é•¿åº¦æ§åˆ¶å™¨
        controller = AdaptiveLengthController(
            min_length=5,
            max_length=50,
            distributions=distributions
        )
        
        # æµ‹è¯•é•¿åº¦é‡‡æ ·
        peptide_types = ['antimicrobial', 'antifungal', 'antiviral']
        lengths = controller.sample_target_lengths(10, peptide_types)
        
        assert lengths.shape == (10,)
        assert torch.all(lengths >= 5)
        assert torch.all(lengths <= 50)
        logger.info("âœ… é•¿åº¦é‡‡æ ·æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•é•¿åº¦æ©ç 
        mask = controller.create_length_mask(lengths, 60)
        assert mask.shape == (10, 60)
        logger.info("âœ… é•¿åº¦æ©ç æµ‹è¯•é€šè¿‡")
        
    logger.info("ğŸ‰ é•¿åº¦æ§åˆ¶å™¨æµ‹è¯•å®Œæˆ")


def test_data_collator():
    """æµ‹è¯•æ•°æ®æ•´ç†å™¨"""
    logger.info("ğŸ§ª æµ‹è¯•æ•°æ®æ•´ç†å™¨")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        create_test_data(temp_path)
        
        # åˆ›å»ºé•¿åº¦æ§åˆ¶å™¨
        controller = create_length_controller_from_data(
            str(temp_path / "train.csv"),
            save_distributions=False
        )
        
        # åˆ›å»ºåˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained("facebook/esm2_t6_8M_UR50D")
        
        # åˆ›å»ºæ•°æ®æ•´ç†å™¨
        collator = LengthAwareDataCollator(
            length_controller=controller,
            tokenizer=tokenizer,
            use_length_control=True
        )
        
        # æµ‹è¯•æ•°æ®
        batch_data = [
            {'sequence': 'ACDEFGHIKLMNPQRS', 'peptide_type': 'antimicrobial'},
            {'sequence': 'TVWYACDEFGHIKLMNPQRST', 'peptide_type': 'antifungal'},
            {'sequence': 'VYACDEFGHIKLMNPQRSTVWY', 'peptide_type': 'antiviral'}
        ]
        
        # æµ‹è¯•æ•´ç†
        result = collator(batch_data)
        
        assert 'sequences' in result
        assert 'attention_mask' in result
        assert 'conditions' in result
        assert 'target_lengths' in result
        assert 'length_mask' in result
        
        logger.info("âœ… æ•°æ®æ•´ç†å™¨æµ‹è¯•é€šè¿‡")
    
    logger.info("ğŸ‰ æ•°æ®æ•´ç†å™¨æµ‹è¯•å®Œæˆ")


def test_separated_training_manager():
    """æµ‹è¯•åˆ†ç¦»å¼è®­ç»ƒç®¡ç†å™¨"""
    logger.info("ğŸ§ª æµ‹è¯•åˆ†ç¦»å¼è®­ç»ƒç®¡ç†å™¨")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)
        
        # åˆ›å»ºé…ç½®
        config = OmegaConf.create({
            "model": {
                "type": "StructDiff",
                "sequence_encoder": {
                    "pretrained_model": "facebook/esm2_t6_8M_UR50D",
                    "freeze_encoder": False
                },
                "denoiser": {
                    "hidden_dim": 256,  # è¾ƒå°çš„éšè—å±‚ç”¨äºæµ‹è¯•
                    "num_layers": 2,
                    "num_heads": 4
                }
            },
            "diffusion": {
                "num_timesteps": 100,  # è¾ƒå°‘çš„æ—¶é—´æ­¥ç”¨äºæµ‹è¯•
                "noise_schedule": "sqrt",
                "beta_start": 0.0001,
                "beta_end": 0.02
            }
        })
        
        # åˆ›å»ºè®­ç»ƒé…ç½®
        training_config = SeparatedTrainingConfig(
            stage1_epochs=2,  # å¾ˆå°‘çš„epochç”¨äºæµ‹è¯•
            stage2_epochs=1,
            stage1_batch_size=4,
            stage2_batch_size=4,
            data_dir=str(temp_path),
            output_dir=str(temp_path / "output"),
            checkpoint_dir=str(temp_path / "checkpoints"),
            use_cfg=True,
            use_length_control=True,
            save_every=50,
            validate_every=25,
            log_every=10
        )
        
        # åˆ›å»ºæ¨¡å‹å’Œæ‰©æ•£è¿‡ç¨‹
        model = StructDiff(config.model)
        diffusion = GaussianDiffusion(config.diffusion)
        
        # åˆ›å»ºè®­ç»ƒç®¡ç†å™¨
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        trainer = SeparatedTrainingManager(
            config=training_config,
            model=model,
            diffusion=diffusion,
            device=device
        )
        
        # æµ‹è¯•é˜¶æ®µ1å‡†å¤‡
        stage1_model, stage1_optimizer, stage1_scheduler = trainer.prepare_stage1_components()
        
        # éªŒè¯ESMç¼–ç å™¨è¢«å†»ç»“
        esm_frozen = all(not p.requires_grad for n, p in stage1_model.named_parameters() 
                        if 'sequence_encoder' in n)
        assert esm_frozen, "ESMç¼–ç å™¨åº”è¯¥è¢«å†»ç»“"
        logger.info("âœ… é˜¶æ®µ1ç»„ä»¶å‡†å¤‡æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•é˜¶æ®µ2å‡†å¤‡
        stage2_model, stage2_optimizer, stage2_scheduler = trainer.prepare_stage2_components()
        
        # éªŒè¯åªæœ‰è§£ç å™¨å¯è®­ç»ƒï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å®é™…æ¨¡å‹ç»“æ„è°ƒæ•´ï¼‰
        logger.info("âœ… é˜¶æ®µ2ç»„ä»¶å‡†å¤‡æµ‹è¯•é€šè¿‡")
        
    logger.info("ğŸ‰ åˆ†ç¦»å¼è®­ç»ƒç®¡ç†å™¨æµ‹è¯•å®Œæˆ")


def test_end_to_end_mini_training():
    """ç«¯åˆ°ç«¯è¿·ä½ è®­ç»ƒæµ‹è¯•"""
    logger.info("ğŸ§ª ç«¯åˆ°ç«¯è¿·ä½ è®­ç»ƒæµ‹è¯•")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)
        
        try:
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            create_test_data(temp_path, num_samples=20)  # å¾ˆå°‘çš„æ ·æœ¬
            
            # åˆ›å»ºé…ç½®
            config = OmegaConf.create({
                "model": {
                    "type": "StructDiff",
                    "sequence_encoder": {
                        "pretrained_model": "facebook/esm2_t6_8M_UR50D",
                        "freeze_encoder": False
                    },
                    "denoiser": {
                        "hidden_dim": 128,  # å¾ˆå°çš„æ¨¡å‹
                        "num_layers": 1,
                        "num_heads": 2
                    }
                },
                "diffusion": {
                    "num_timesteps": 50,  # å¾ˆå°‘çš„æ—¶é—´æ­¥
                    "noise_schedule": "sqrt",
                    "beta_start": 0.0001,
                    "beta_end": 0.02
                }
            })
            
            # åˆ›å»ºè®­ç»ƒé…ç½®
            training_config = SeparatedTrainingConfig(
                stage1_epochs=1,  # åªè®­ç»ƒ1ä¸ªepoch
                stage2_epochs=1,
                stage1_batch_size=2,
                stage2_batch_size=2,
                data_dir=str(temp_path),
                output_dir=str(temp_path / "output"),
                checkpoint_dir=str(temp_path / "checkpoints"),
                use_cfg=False,  # ç®€åŒ–é…ç½®
                use_length_control=False,
                use_amp=False,
                use_ema=False,
                save_every=100,  # ä¸ä¿å­˜
                validate_every=100,  # ä¸éªŒè¯
                log_every=5
            )
            
            # åˆ›å»ºåˆ†è¯å™¨
            tokenizer = AutoTokenizer.from_pretrained("facebook/esm2_t6_8M_UR50D")
            
            # åˆ›å»ºæ•°æ®æ•´ç†å™¨
            collator = LengthAwareDataCollator(
                length_controller=None,
                tokenizer=tokenizer,
                use_length_control=False
            )
            
            # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
            train_dataset = PeptideStructureDataset(
                data_path=str(temp_path / "train.csv"),
                config=config,
                is_training=True
            )
            
            train_loader = DataLoader(
                train_dataset,
                batch_size=2,
                shuffle=True,
                collate_fn=collator,
                num_workers=0  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
            )
            
            # åˆ›å»ºæ¨¡å‹å’Œæ‰©æ•£è¿‡ç¨‹
            model = StructDiff(config.model)
            diffusion = GaussianDiffusion(config.diffusion)
            
            # åˆ›å»ºè®­ç»ƒç®¡ç†å™¨
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            trainer = SeparatedTrainingManager(
                config=training_config,
                model=model,
                diffusion=diffusion,
                device=device
            )
            
            # æ‰§è¡Œè¿·ä½ è®­ç»ƒ
            logger.info("å¼€å§‹è¿·ä½ è®­ç»ƒ...")
            stats = trainer.run_complete_training(train_loader)
            
            # éªŒè¯ç»Ÿè®¡æ•°æ®
            assert 'stage1' in stats
            assert 'stage2' in stats
            assert len(stats['stage1']['losses']) > 0
            assert len(stats['stage2']['losses']) > 0
            
            logger.info("âœ… ç«¯åˆ°ç«¯è®­ç»ƒæµ‹è¯•é€šè¿‡")
            
        except Exception as e:
            logger.error(f"ç«¯åˆ°ç«¯æµ‹è¯•å¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            raise
    
    logger.info("ğŸ‰ ç«¯åˆ°ç«¯è®­ç»ƒæµ‹è¯•å®Œæˆ")


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    logger.info("ğŸš€ å¼€å§‹åˆ†ç¦»å¼è®­ç»ƒå…¨é¢æµ‹è¯•")
    
    try:
        # ç»„ä»¶æµ‹è¯•
        test_length_controller()
        test_data_collator()
        test_separated_training_manager()
        
        # é›†æˆæµ‹è¯•
        test_end_to_end_mini_training()
        
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {str(e)}")
        raise


if __name__ == "__main__":
    run_all_tests()