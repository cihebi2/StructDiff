#!/usr/bin/env python3
"""
å¿«é€Ÿæµ‹è¯•åˆ†ç¦»å¼è®­ç»ƒåŠŸèƒ½
ä½¿ç”¨æœ€å°é…ç½®éªŒè¯åŸºæœ¬è®­ç»ƒæµç¨‹
"""

import os
import sys
import torch
import torch.nn as nn
from pathlib import Path
from omegaconf import OmegaConf

# è®¾ç½®ç¯å¢ƒ
os.environ['CUDA_VISIBLE_DEVICES'] = '2'  # ä½¿ç”¨GPU 2

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from structdiff.training.separated_training import SeparatedTrainingManager, SeparatedTrainingConfig
from structdiff.models.structdiff import StructDiff
from structdiff.diffusion.gaussian_diffusion import GaussianDiffusion
from structdiff.data.dataset import PeptideStructureDataset
from structdiff.training.length_controller import LengthAwareDataCollator
from structdiff.utils.logger import setup_logger, get_logger
from torch.utils.data import DataLoader
from transformers import AutoTokenizer

setup_logger(level="INFO")
logger = get_logger(__name__)

def create_minimal_config():
    """åˆ›å»ºæœ€å°é…ç½®"""
    return OmegaConf.create({
        "model": {
            "type": "StructDiff",
            "sequence_encoder": {
                "pretrained_model": "facebook/esm2_t6_8M_UR50D",
                "freeze_encoder": False,
                "use_lora": False  # ç¦ç”¨LoRAç®€åŒ–
            },
            "structure_encoder": {
                "type": "multi_scale",
                "hidden_dim": 128,  # å‡å°ç»´åº¦
                "use_esmfold": False  # ç¦ç”¨ESMFold
            },
            "denoiser": {
                "hidden_dim": 320,
                "num_layers": 4,    # å‡å°‘å±‚æ•°
                "num_heads": 4,     # å‡å°‘å¤´æ•°
                "dropout": 0.1,
                "use_cross_attention": False  # ç®€åŒ–äº¤å‰æ³¨æ„åŠ›
            },
            "sequence_decoder": {
                "hidden_dim": 320,
                "num_layers": 2,    # å‡å°‘è§£ç å™¨å±‚æ•°
                "vocab_size": 33,
                "dropout": 0.1
            }
        },
        "diffusion": {
            "num_timesteps": 100,  # å‡å°‘æ—¶é—´æ­¥
            "noise_schedule": "sqrt",
            "beta_start": 0.0001,
            "beta_end": 0.02
        }
    })

def create_minimal_training_config():
    """åˆ›å»ºæœ€å°è®­ç»ƒé…ç½®"""
    return SeparatedTrainingConfig(
        # é˜¶æ®µ1é…ç½®
        stage1_epochs=2,           # å¾ˆå°‘çš„epochsç”¨äºæµ‹è¯•
        stage1_lr=1e-4,
        stage1_batch_size=2,       # æœ€å°æ‰¹æ¬¡
        stage1_gradient_clip=1.0,
        stage1_warmup_steps=10,
        
        # é˜¶æ®µ2é…ç½®
        stage2_epochs=1,
        stage2_lr=5e-5,
        stage2_batch_size=2,
        stage2_gradient_clip=0.5,
        stage2_warmup_steps=5,
        
        # å…¶ä»–é…ç½®
        use_amp=False,             # ç¦ç”¨æ··åˆç²¾åº¦é¿å…å¤æ‚æ€§
        use_ema=False,             # ç¦ç”¨EMA
        save_every=1000,           # ä¸ä¿å­˜æ£€æŸ¥ç‚¹
        validate_every=50,         # å‡å°‘éªŒè¯é¢‘ç‡
        log_every=5,               # å¢åŠ æ—¥å¿—é¢‘ç‡
        
        # ç¦ç”¨é«˜çº§åŠŸèƒ½
        use_length_control=False,
        use_cfg=False,
        
        # è·¯å¾„
        data_dir="./data/processed",
        output_dir="./outputs/test_separated",
        checkpoint_dir="./outputs/test_separated/checkpoints",
        
        # ç¦ç”¨è¯„ä¼°
        enable_evaluation=False
    )

def create_simple_data_loader(config, training_config, tokenizer, split='train'):
    """åˆ›å»ºç®€åŒ–çš„æ•°æ®åŠ è½½å™¨"""
    dataset = PeptideStructureDataset(
        data_path=f"./data/processed/{split}.csv",
        config=config,
        is_training=(split == 'train')
    )
    
    # ç®€åŒ–çš„collatorï¼Œä¸ä½¿ç”¨é•¿åº¦æ§åˆ¶
    def simple_collate_fn(batch):
        sequences = []
        attention_masks = []
        
        for item in batch:
            sequences.append(item['sequences'])
            attention_masks.append(item['attention_mask'])
        
        return {
            'sequences': torch.stack(sequences),
            'attention_mask': torch.stack(attention_masks),
            'structures': None,  # ç®€åŒ–ï¼Œä¸ä½¿ç”¨ç»“æ„ç‰¹å¾
            'conditions': None   # ç®€åŒ–ï¼Œä¸ä½¿ç”¨æ¡ä»¶
        }
    
    data_loader = DataLoader(
        dataset,
        batch_size=training_config.stage1_batch_size,
        shuffle=(split == 'train'),
        num_workers=0,  # ç¦ç”¨å¤šè¿›ç¨‹
        collate_fn=simple_collate_fn
    )
    
    return data_loader

def test_separated_training():
    """æµ‹è¯•åˆ†ç¦»å¼è®­ç»ƒ"""
    logger.info("ğŸš€ å¼€å§‹å¿«é€Ÿåˆ†ç¦»å¼è®­ç»ƒæµ‹è¯•")
    
    # æ£€æŸ¥GPU
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    try:
        # åˆ›å»ºé…ç½®
        config = create_minimal_config()
        training_config = create_minimal_training_config()
        
        logger.info("âœ“ é…ç½®åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºåˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(
            config.model.sequence_encoder.pretrained_model
        )
        logger.info("âœ“ åˆ†è¯å™¨åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºæ¨¡å‹
        model = StructDiff(config.model)
        model = model.to(device)
        logger.info(f"âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # åˆ›å»ºæ‰©æ•£è¿‡ç¨‹
        diffusion = GaussianDiffusion(
            num_timesteps=config.diffusion.num_timesteps,
            noise_schedule=config.diffusion.noise_schedule,
            beta_start=config.diffusion.beta_start,
            beta_end=config.diffusion.beta_end
        )
        logger.info("âœ“ æ‰©æ•£è¿‡ç¨‹åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = create_simple_data_loader(config, training_config, tokenizer, 'train')
        val_loader = create_simple_data_loader(config, training_config, tokenizer, 'val')
        logger.info(f"âœ“ æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸï¼Œè®­ç»ƒæ ·æœ¬æ•°: {len(train_loader.dataset)}")
        
        # åˆ›å»ºè®­ç»ƒç®¡ç†å™¨
        trainer = SeparatedTrainingManager(
            config=training_config,
            model=model,
            diffusion=diffusion,
            device=str(device),
            tokenizer=tokenizer
        )
        logger.info("âœ“ è®­ç»ƒç®¡ç†å™¨åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•ä¸€ä¸ªè®­ç»ƒæ‰¹æ¬¡
        logger.info("ğŸ§ª æµ‹è¯•å•ä¸ªè®­ç»ƒæ‰¹æ¬¡...")
        
        # å‡†å¤‡é˜¶æ®µ1ç»„ä»¶
        model, optimizer, scheduler = trainer.prepare_stage1_components()
        
        # è·å–ä¸€ä¸ªæ‰¹æ¬¡
        train_iter = iter(train_loader)
        batch = next(train_iter)
        
        # æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤
        step_stats = trainer.stage1_training_step(batch, model, optimizer)
        logger.info(f"âœ“ é˜¶æ®µ1è®­ç»ƒæ­¥éª¤æˆåŠŸï¼ŒæŸå¤±: {step_stats['loss']:.4f}")
        
        # æµ‹è¯•éªŒè¯
        val_stats = trainer.validate_stage1(val_loader, model)
        logger.info(f"âœ“ é˜¶æ®µ1éªŒè¯æˆåŠŸï¼ŒéªŒè¯æŸå¤±: {val_stats['val_loss']:.4f}")
        
        # å¿«é€Ÿæµ‹è¯•é˜¶æ®µ2å‡†å¤‡
        logger.info("ğŸ§ª æµ‹è¯•é˜¶æ®µ2ç»„ä»¶å‡†å¤‡...")
        model, optimizer2, scheduler2 = trainer.prepare_stage2_components()
        logger.info("âœ“ é˜¶æ®µ2ç»„ä»¶å‡†å¤‡æˆåŠŸ")
        
        # å¦‚æœä¸€åˆ‡æ­£å¸¸ï¼Œè¿è¡Œå°‘é‡epochsçš„å®Œæ•´è®­ç»ƒ
        logger.info("ğŸš€ å¼€å§‹å¿«é€Ÿå®Œæ•´è®­ç»ƒæµ‹è¯•...")
        final_stats = trainer.run_complete_training(train_loader, val_loader)
        
        logger.info("ğŸ‰ åˆ†ç¦»å¼è®­ç»ƒæµ‹è¯•å®Œæˆï¼")
        logger.info("è®­ç»ƒç»Ÿè®¡:")
        for stage, stats in final_stats.items():
            logger.info(f"  {stage}:")
            if 'losses' in stats and stats['losses']:
                logger.info(f"    æœ€ç»ˆæŸå¤±: {stats['losses'][-1]:.4f}")
            if 'val_losses' in stats and stats['val_losses']:
                logger.info(f"    æœ€ç»ˆéªŒè¯æŸå¤±: {stats['val_losses'][-1]:.4f}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

if __name__ == "__main__":
    success = test_separated_training()
    if success:
        logger.info("âœ… åˆ†ç¦»å¼è®­ç»ƒåŠŸèƒ½éªŒè¯æˆåŠŸï¼")
        print("\n" + "="*60)
        print("ğŸ‰ åˆ†ç¦»å¼è®­ç»ƒç³»ç»Ÿå·²å°±ç»ªï¼")
        print("ğŸ’¡ ç°åœ¨å¯ä»¥ä½¿ç”¨å®Œæ•´é…ç½®è¿›è¡Œç”Ÿäº§è®­ç»ƒ")
        print("="*60)
    else:
        logger.error("âŒ åˆ†ç¦»å¼è®­ç»ƒåŠŸèƒ½éªŒè¯å¤±è´¥")
        sys.exit(1) 