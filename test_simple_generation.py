#!/usr/bin/env python3
"""
ç®€åŒ–çš„ç”Ÿæˆæµ‹è¯• - æµ‹è¯•å½“å‰è®­ç»ƒå¥½çš„æ¨¡å‹
"""

import torch
import numpy as np
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print("ğŸ”„ æµ‹è¯•æ¨¡å‹åŠ è½½...")
    
    try:
        from structdiff.models.structdiff import StructDiff
        from omegaconf import OmegaConf
        
        # åˆ›å»ºé…ç½®ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        config = OmegaConf.create({
            'sequence_encoder': {
                'pretrained_model': 'facebook/esm2_t6_8M_UR50D',
                'freeze_encoder': False,
                'use_lora': True,
                'lora_rank': 16,
                'lora_alpha': 32,
                'lora_dropout': 0.1
            },
            'structure_encoder': {
                'hidden_dim': 256,
                'num_layers': 3,
                'use_esmfold': False,
                'use_coordinates': False,
                'use_distances': False,
                'use_angles': False,
                'use_secondary_structure': True
            },
            'denoiser': {
                'hidden_dim': 320,
                'num_layers': 6,
                'num_heads': 8,
                'dropout': 0.1,
                'use_cross_attention': False,  # æ³¨æ„ï¼šè®­ç»ƒæ—¶ç¦ç”¨äº†
                'use_cfg': True,
                'cfg_dropout': 0.1
            },
            'data': {'max_length': 512}
        })
        
        print("âœ“ é…ç½®åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºæ¨¡å‹
        model = StructDiff(config)
        print(f"âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {model.count_parameters():,}")
        
        return model, config
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def test_checkpoint_loading(model, checkpoint_path):
    """æµ‹è¯•æ£€æŸ¥ç‚¹åŠ è½½"""
    print(f"ğŸ”„ æµ‹è¯•æ£€æŸ¥ç‚¹åŠ è½½: {checkpoint_path}")
    
    try:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        if not Path(checkpoint_path).exists():
            print(f"âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            return None
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
        
        # å¤„ç†ä¸åŒçš„æ£€æŸ¥ç‚¹æ ¼å¼
        if 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
            print("âœ“ æ£€æŸ¥ç‚¹æ ¼å¼: model_state_dict")
        elif 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
            print("âœ“ æ£€æŸ¥ç‚¹æ ¼å¼: state_dict")
        else:
            state_dict = checkpoint
            print("âœ“ æ£€æŸ¥ç‚¹æ ¼å¼: ç›´æ¥çŠ¶æ€å­—å…¸")
        
        # åŠ è½½åˆ°æ¨¡å‹
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
        
        if missing_keys:
            print(f"âš ï¸ ç¼ºå¤±é”®: {len(missing_keys)} ä¸ª")
            for key in missing_keys[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"   - {key}")
        
        if unexpected_keys:
            print(f"âš ï¸ å¤šä½™é”®: {len(unexpected_keys)} ä¸ª")
            for key in unexpected_keys[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"   - {key}")
        
        model = model.to(device)
        model.eval()
        
        print("âœ“ æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ")
        return model
        
    except Exception as e:
        print(f"âŒ æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def test_tokenizer():
    """æµ‹è¯•åˆ†è¯å™¨"""
    print("ğŸ”„ æµ‹è¯•åˆ†è¯å™¨...")
    
    try:
        from transformers import AutoTokenizer
        
        tokenizer = AutoTokenizer.from_pretrained('facebook/esm2_t6_8M_UR50D')
        
        # æµ‹è¯•ç¼–ç è§£ç 
        test_seq = "MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIYLRSLGYNIVATPRGYVLAGG"
        encoded = tokenizer.encode(test_seq, add_special_tokens=True)
        decoded = tokenizer.decode(encoded, skip_special_tokens=True)
        
        print(f"âœ“ åŸå§‹åºåˆ—: {test_seq[:30]}...")
        print(f"âœ“ ç¼–ç é•¿åº¦: {len(encoded)}")
        print(f"âœ“ è§£ç åºåˆ—: {decoded[:30]}...")
        print(f"âœ“ è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")
        
        return tokenizer
        
    except Exception as e:
        print(f"âŒ åˆ†è¯å™¨æµ‹è¯•å¤±è´¥: {e}")
        return None

def test_sequence_decoding(model, tokenizer, device):
    """æµ‹è¯•åºåˆ—è§£ç åŠŸèƒ½"""
    print("ğŸ”„ æµ‹è¯•åºåˆ—è§£ç ...")
    
    try:
        # åˆ›å»ºéšæœºåµŒå…¥
        batch_size = 1
        seq_len = 32  # åŒ…æ‹¬CLSå’ŒSEP
        hidden_dim = model.seq_hidden_dim
        
        # éšæœºåµŒå…¥
        embeddings = torch.randn(batch_size, seq_len, hidden_dim, device=device)
        attention_mask = torch.ones(batch_size, seq_len, device=device)
        
        print(f"âœ“ åµŒå…¥å½¢çŠ¶: {embeddings.shape}")
        print(f"âœ“ æ©ç å½¢çŠ¶: {attention_mask.shape}")
        
        # è§£ç 
        with torch.no_grad():
            sequences = model._decode_embeddings(embeddings, attention_mask)
        
        print(f"âœ“ è§£ç ç»“æœ: {sequences}")
        print(f"âœ“ ç”Ÿæˆåºåˆ—æ•°é‡: {len(sequences)}")
        
        if sequences and sequences[0]:
            print(f"âœ“ ç¬¬ä¸€ä¸ªåºåˆ—: {sequences[0]}")
            print(f"âœ“ åºåˆ—é•¿åº¦: {len(sequences[0])}")
        
        return sequences
        
    except Exception as e:
        print(f"âŒ åºåˆ—è§£ç å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def test_simple_generation(model, device):
    """æµ‹è¯•ç®€å•ç”Ÿæˆ"""
    print("ğŸ”„ æµ‹è¯•ç®€å•ç”Ÿæˆ...")
    
    try:
        # ç®€å•çš„å‰å‘ä¼ æ’­æµ‹è¯•
        batch_size = 1
        seq_len = 32
        
        # åˆ›å»ºè™šæ‹Ÿè¾“å…¥
        sequences = torch.zeros(batch_size, seq_len, device=device, dtype=torch.long)
        attention_mask = torch.ones(batch_size, seq_len, device=device)
        timesteps = torch.randint(0, 1000, (batch_size,), device=device)
        conditions = {'peptide_type': torch.tensor([0], device=device, dtype=torch.long)}
        
        print(f"âœ“ è¾“å…¥å½¢çŠ¶ - åºåˆ—: {sequences.shape}")
        print(f"âœ“ è¾“å…¥å½¢çŠ¶ - æ©ç : {attention_mask.shape}")
        print(f"âœ“ æ—¶é—´æ­¥: {timesteps}")
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            outputs = model(
                sequences=sequences,
                attention_mask=attention_mask,
                timesteps=timesteps,
                conditions=conditions,
                return_loss=False
            )
        
        print(f"âœ“ è¾“å‡ºé”®: {list(outputs.keys())}")
        
        if 'denoised_embeddings' in outputs:
            denoised = outputs['denoised_embeddings']
            print(f"âœ“ å»å™ªåµŒå…¥å½¢çŠ¶: {denoised.shape}")
            
            # å°è¯•è§£ç 
            try:
                sequences = model._decode_embeddings(denoised, attention_mask)
                print(f"âœ“ ç”Ÿæˆåºåˆ—: {sequences}")
            except Exception as decode_error:
                print(f"âš ï¸ è§£ç å¤±è´¥: {decode_error}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç®€å•ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ§ª ç®€åŒ–æ¨¡å‹ç”Ÿæˆæµ‹è¯•")
    print("=" * 60)
    
    # 1. æµ‹è¯•æ¨¡å‹åŠ è½½
    model, config = test_model_loading()
    if model is None:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæµ‹è¯•ç»ˆæ­¢")
        return
    
    # 2. æµ‹è¯•æ£€æŸ¥ç‚¹åŠ è½½
    checkpoint_path = "./outputs/structdiff_fixed/best_model.pt"
    model = test_checkpoint_loading(model, checkpoint_path)
    if model is None:
        print("âŒ æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥ï¼Œæµ‹è¯•ç»ˆæ­¢")
        return
    
    device = next(model.parameters()).device
    print(f"âœ“ æ¨¡å‹è®¾å¤‡: {device}")
    
    # 3. æµ‹è¯•åˆ†è¯å™¨
    tokenizer = test_tokenizer()
    if tokenizer is None:
        print("âŒ åˆ†è¯å™¨æµ‹è¯•å¤±è´¥")
        return
    
    # 4. æµ‹è¯•åºåˆ—è§£ç 
    sequences = test_sequence_decoding(model, tokenizer, device)
    
    # 5. æµ‹è¯•ç®€å•ç”Ÿæˆ
    success = test_simple_generation(model, device)
    
    print("\n" + "=" * 60)
    if success:
        print("ğŸ‰ åŸºç¡€æµ‹è¯•å®Œæˆï¼æ¨¡å‹å¯ä»¥æ­£å¸¸å·¥ä½œ")
        print("ğŸ“ ç»“è®º: å½“å‰è®­ç»ƒçš„ç®€åŒ–æ¨¡å‹åŠŸèƒ½æ­£å¸¸")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
    print("=" * 60)

if __name__ == "__main__":
    main() 