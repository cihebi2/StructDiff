#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„è®­ç»ƒè„šæœ¬
ç®€åŒ–ç‰ˆæœ¬ï¼Œç”¨äºéªŒè¯ä¿®å¤æ˜¯å¦æœ‰æ•ˆ
"""

import os
import sys
import torch
from pathlib import Path
from omegaconf import OmegaConf

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥ä¿®å¤åçš„æ¨¡å—
from fix_esmfold_patch import apply_esmfold_patch
from structdiff.models.structdiff import StructDiff
from structdiff.data.dataset import PeptideStructureDataset
from structdiff.data.collator import PeptideStructureCollator
from structdiff.utils.logger import setup_logger, get_logger

def test_fix():
    """æµ‹è¯•ä¿®å¤æ˜¯å¦æœ‰æ•ˆ"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•ä¿®å¤åçš„è®­ç»ƒä»£ç ...")
    
    # åº”ç”¨ESMFoldè¡¥ä¸
    apply_esmfold_patch()
    print("âœ“ ESMFoldè¡¥ä¸åº”ç”¨æˆåŠŸ")
    
    # è®¾ç½®æ—¥å¿—
    setup_logger()
    logger = get_logger(__name__)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½é…ç½®
    config_path = "configs/peptide_esmfold_config.yaml"
    if not os.path.exists(config_path):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return False
    
    config = OmegaConf.load(config_path)
    print("âœ“ é…ç½®åŠ è½½æˆåŠŸ")
    
    try:
        # æµ‹è¯•æ•°æ®åŠ è½½å™¨
        print("ğŸ“Š æµ‹è¯•æ•°æ®åŠ è½½å™¨...")
        
        # åˆ›å»ºå°å‹æµ‹è¯•æ•°æ®é›†
        train_dataset = PeptideStructureDataset(
            config.data.train_path,
            config,
            is_training=True,
            shared_esmfold=None  # æš‚æ—¶ä¸ä½¿ç”¨ESMFold
        )
        
        # ä½¿ç”¨å‰5ä¸ªæ ·æœ¬è¿›è¡Œæµ‹è¯•
        from torch.utils.data import Subset
        test_subset = Subset(train_dataset, range(min(5, len(train_dataset))))
        
        collator = PeptideStructureCollator(config)
        
        from torch.utils.data import DataLoader
        test_loader = DataLoader(
            test_subset,
            batch_size=2,
            shuffle=False,
            num_workers=0,
            collate_fn=collator
        )
        
        print(f"âœ“ æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸï¼Œæµ‹è¯•æ ·æœ¬æ•°: {len(test_subset)}")
        
        # æµ‹è¯•æ‰¹æ¬¡å¤„ç†
        print("ğŸ”„ æµ‹è¯•æ‰¹æ¬¡å¤„ç†...")
        for i, batch in enumerate(test_loader):
            print(f"  æ‰¹æ¬¡ {i+1}:")
            print(f"    sequences: {batch['sequences'].shape}")
            print(f"    attention_mask: {batch['attention_mask'].shape}")
            
            if 'structures' in batch:
                print(f"    structures: {len(batch['structures'])} keys")
                for key, value in batch['structures'].items():
                    if isinstance(value, torch.Tensor):
                        print(f"      {key}: {value.shape}")
            
            if i >= 2:  # åªæµ‹è¯•å‰3ä¸ªæ‰¹æ¬¡
                break
        
        print("âœ“ æ‰¹æ¬¡å¤„ç†æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–ï¼ˆä¸ä½¿ç”¨ESMFoldï¼‰
        print("ğŸ—ï¸ æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–...")
        
        # ä¸´æ—¶ç¦ç”¨ESMFold
        original_use_esmfold = config.model.structure_encoder.get('use_esmfold', False)
        config.model.structure_encoder.use_esmfold = False
        config.data.use_predicted_structures = False
        
        model = StructDiff(config).to(device)
        print(f"âœ“ æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        print("â© æµ‹è¯•å‰å‘ä¼ æ’­...")
        model.eval()
        
        with torch.no_grad():
            # è·å–ä¸€ä¸ªæ‰¹æ¬¡è¿›è¡Œæµ‹è¯•
            test_batch = next(iter(test_loader))
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            test_batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                         for k, v in test_batch.items()}
            
            # åˆ›å»ºæ—¶é—´æ­¥
            batch_size = test_batch['sequences'].shape[0]
            timesteps = torch.randint(0, config.diffusion.num_timesteps, (batch_size,), device=device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(
                sequences=test_batch['sequences'],
                attention_mask=test_batch['attention_mask'],
                timesteps=timesteps,
                structures=test_batch.get('structures'),
                conditions=test_batch.get('conditions'),
                return_loss=True
            )
            
            print(f"âœ“ å‰å‘ä¼ æ’­æˆåŠŸ")
            print(f"  è¾“å‡ºé”®: {list(outputs.keys())}")
            
            if 'total_loss' in outputs:
                print(f"  æ€»æŸå¤±: {outputs['total_loss'].item():.4f}")
            
            if 'denoised_embeddings' in outputs:
                print(f"  å»å™ªåµŒå…¥å½¢çŠ¶: {outputs['denoised_embeddings'].shape}")
        
        # æ¢å¤åŸå§‹é…ç½®
        config.model.structure_encoder.use_esmfold = original_use_esmfold
        
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ä¿®å¤æœ‰æ•ˆï¼")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        print(traceback.format_exc())
        return False

if __name__ == "__main__":
    success = test_fix()
    if success:
        print("\nâœ… ä¿®å¤éªŒè¯æˆåŠŸï¼ç°åœ¨å¯ä»¥é‡æ–°è¿è¡Œè®­ç»ƒè„šæœ¬ã€‚")
        print("æ¨èå‘½ä»¤:")
        print("python scripts/train_peptide_esmfold.py --config configs/peptide_esmfold_config.yaml --debug")
    else:
        print("\nâŒ ä¿®å¤éªŒè¯å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")
    
    sys.exit(0 if success else 1)