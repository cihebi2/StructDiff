# train_classification.py - æŠ—èŒè‚½åˆ†ç±»è®­ç»ƒè„šæœ¬
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from omegaconf import OmegaConf
from tqdm import tqdm
import os
import sys
import gc
import json
import logging
import argparse
from pathlib import Path
from datetime import datetime
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix
import matplotlib.pyplot as plt
import matplotlib
# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œé¿å…ä¸­æ–‡æ˜¾ç¤ºè­¦å‘Š
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']  # æŒ‡å®šé»˜è®¤å­—ä½“
matplotlib.rcParams['axes.unicode_minus'] = False  # è§£å†³ä¿å­˜å›¾åƒæ˜¯è´Ÿå·'-'æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜
import seaborn as sns

# è®¾ç½®è·¯å¾„ï¼Œç¡®ä¿èƒ½å¯¼å…¥é¡¹ç›®æ¨¡å—
sys.path.append('/home/qlyu/StructDiff')

class AMPClassifier(nn.Module):
    """æŠ—èŒè‚½åˆ†ç±»å™¨"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # åºåˆ—ç¼–ç å™¨ - ä½¿ç”¨ESM2
        from transformers import EsmModel, EsmTokenizer
        self.tokenizer = EsmTokenizer.from_pretrained(config.model.sequence_encoder.pretrained_model)
        self.sequence_encoder = EsmModel.from_pretrained(config.model.sequence_encoder.pretrained_model)
        
        # å†»ç»“encoderå‚æ•°ï¼ˆå¯é€‰ï¼‰
        if config.model.sequence_encoder.get('freeze_encoder', False):
            for param in self.sequence_encoder.parameters():
                param.requires_grad = False
        
        # è·å–ç¼–ç å™¨çš„éšè—ç»´åº¦
        encoder_dim = self.sequence_encoder.config.hidden_size
        
        # åˆ†ç±»å¤´
        classifier_layers = []
        hidden_dim = config.model.classifier.hidden_dim
        num_layers = config.model.classifier.get('num_layers', 2)
        dropout = config.model.classifier.get('dropout', 0.1)
        
        # è¾“å…¥å±‚
        classifier_layers.append(nn.Linear(encoder_dim, hidden_dim))
        classifier_layers.append(nn.ReLU())
        classifier_layers.append(nn.Dropout(dropout))
        
        # éšè—å±‚
        for _ in range(num_layers - 1):
            classifier_layers.append(nn.Linear(hidden_dim, hidden_dim))
            classifier_layers.append(nn.ReLU())
            classifier_layers.append(nn.Dropout(dropout))
        
        # è¾“å‡ºå±‚
        num_classes = config.task.num_classes
        classifier_layers.append(nn.Linear(hidden_dim, num_classes))
        
        self.classifier = nn.Sequential(*classifier_layers)
        
        print(f"âœ“ åˆå§‹åŒ–åˆ†ç±»å™¨: {num_classes} ç±»")
        print(f"  ç¼–ç å™¨ç»´åº¦: {encoder_dim}")
        print(f"  åˆ†ç±»å™¨éšè—ç»´åº¦: {hidden_dim}")
        print(f"  åˆ†ç±»å™¨å±‚æ•°: {num_layers}")
    
    def forward(self, sequences, attention_mask=None, labels=None, weights=None):
        """å‰å‘ä¼ æ’­"""
        # ç¼–ç åºåˆ—
        encoder_outputs = self.sequence_encoder(
            input_ids=sequences,
            attention_mask=attention_mask,
            return_dict=True
        )
        
        # ä½¿ç”¨CLS tokençš„è¡¨ç¤ºæˆ–è€…å¹³å‡æ± åŒ–
        if hasattr(encoder_outputs, 'pooler_output') and encoder_outputs.pooler_output is not None:
            sequence_representation = encoder_outputs.pooler_output
        else:
            # ä½¿ç”¨å¹³å‡æ± åŒ–
            hidden_states = encoder_outputs.last_hidden_state
            if attention_mask is not None:
                # åŠ æƒå¹³å‡ï¼ˆå¿½ç•¥paddingéƒ¨åˆ†ï¼‰
                attention_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()
                sum_embeddings = torch.sum(hidden_states * attention_mask_expanded, 1)
                sum_mask = torch.clamp(attention_mask_expanded.sum(1), min=1e-9)
                sequence_representation = sum_embeddings / sum_mask
            else:
                sequence_representation = hidden_states.mean(dim=1)
        
        # åˆ†ç±»
        logits = self.classifier(sequence_representation)
        
        outputs = {'logits': logits}
        
        # è®¡ç®—æŸå¤±
        if labels is not None:
            if weights is not None:
                # åŠ æƒäº¤å‰ç†µæŸå¤±
                loss_fct = nn.CrossEntropyLoss(reduction='none')
                losses = loss_fct(logits, labels)
                loss = (losses * weights).mean()
            else:
                loss_fct = nn.CrossEntropyLoss()
                loss = loss_fct(logits, labels)
            
            outputs['loss'] = loss
            outputs['classification_loss'] = loss
        
        return outputs
    
    def count_parameters(self):
        """è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        return {
            'total': total_params,
            'trainable': trainable_params,
            'frozen': total_params - trainable_params
        }

class AMPDataset(torch.utils.data.Dataset):
    """æŠ—èŒè‚½æ•°æ®é›†"""
    
    def __init__(self, csv_path, tokenizer, config, is_training=True):
        self.config = config
        self.tokenizer = tokenizer
        self.is_training = is_training
        
        # è¯»å–æ•°æ®
        self.data = pd.read_csv(csv_path)
        print(f"åŠ è½½æ•°æ®: {len(self.data)} æ ·æœ¬")
        
        # æ£€æŸ¥å¿…è¦åˆ—
        required_cols = ['sequence', 'label']
        for col in required_cols:
            if col not in self.data.columns:
                raise ValueError(f"æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {col}")
        
        # åºåˆ—é•¿åº¦è¿‡æ»¤
        max_length = config.data.get('max_length', 100)
        min_length = config.data.get('min_length', 5)
        
        original_len = len(self.data)
        seq_lengths = self.data['sequence'].str.len()
        self.data = self.data[(seq_lengths >= min_length) & (seq_lengths <= max_length)]
        filtered_len = len(self.data)
        
        if filtered_len < original_len:
            print(f"è¿‡æ»¤åºåˆ—é•¿åº¦: {original_len} -> {filtered_len}")
        
        # æ ‡ç­¾å’Œæƒé‡
        self.labels = self.data['label'].values
        if 'weight' in self.data.columns:
            self.weights = self.data['weight'].values
        else:
            self.weights = np.ones(len(self.data))
        
        print(f"æ ‡ç­¾åˆ†å¸ƒ: {dict(zip(*np.unique(self.labels, return_counts=True)))}")
        print(f"æƒé‡åˆ†å¸ƒ: {dict(zip(*np.unique(self.weights, return_counts=True)))}")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        sequence = row['sequence']
        label = row['label']
        weight = self.weights[idx]
        
        # æ•°æ®å¢å¼ºï¼ˆä»…è®­ç»ƒæ—¶ï¼‰
        if self.is_training and self.config.data.augmentation.get('enable', False):
            sequence = self._augment_sequence(sequence)
        
        return {
            'sequence': sequence,
            'label': label,
            'weight': weight
        }
    
    def _augment_sequence(self, sequence):
        """ç®€å•çš„åºåˆ—æ•°æ®å¢å¼º"""
        # éšæœºæ©ç›–éƒ¨åˆ†æ°¨åŸºé…¸
        if np.random.random() < self.config.data.augmentation.get('mask_prob', 0):
            seq_list = list(sequence)
            mask_positions = np.random.choice(
                len(seq_list),
                size=max(1, int(len(seq_list) * 0.1)),
                replace=False
            )
            for pos in mask_positions:
                seq_list[pos] = '<mask>'
            sequence = ''.join(seq_list)
        
        return sequence

def collate_fn(batch, tokenizer, config):
    """æ•°æ®æ‰¹å¤„ç†å‡½æ•°"""
    sequences = [item['sequence'] for item in batch]
    labels = torch.tensor([item['label'] for item in batch], dtype=torch.long)
    weights = torch.tensor([item['weight'] for item in batch], dtype=torch.float)
    
    # ç¼–ç åºåˆ—
    max_length = config.data.get('max_length', 100)
    encoded = tokenizer(
        sequences,
        padding=True,
        truncation=True,
        max_length=max_length,
        return_tensors='pt'
    )
    
    return {
        'sequences': encoded['input_ids'],
        'attention_mask': encoded['attention_mask'],
        'labels': labels,
        'weights': weights
    }

def calculate_metrics(predictions, labels, weights=None, num_classes=3):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    # åŸºæœ¬æŒ‡æ ‡
    accuracy = accuracy_score(labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, predictions, average='weighted', zero_division=0
    )
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(labels, predictions)
    
    # ROC AUC (å¤šåˆ†ç±»)
    try:
        # éœ€è¦é¢„æµ‹æ¦‚ç‡æ¥è®¡ç®—AUC
        auc_roc = None  # è¿™é‡Œéœ€è¦logitsæ¥è®¡ç®—
    except:
        auc_roc = None
    
    metrics = {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'confusion_matrix': cm,
        'auc_roc': auc_roc
    }
    
    # æ¯ç±»æŒ‡æ ‡
    per_class_precision, per_class_recall, per_class_f1, support = precision_recall_fscore_support(
        labels, predictions, average=None, zero_division=0
    )
    
    for i in range(num_classes):
        metrics[f'precision_class_{i}'] = per_class_precision[i] if i < len(per_class_precision) else 0
        metrics[f'recall_class_{i}'] = per_class_recall[i] if i < len(per_class_recall) else 0
        metrics[f'f1_class_{i}'] = per_class_f1[i] if i < len(per_class_f1) else 0
        metrics[f'support_class_{i}'] = support[i] if i < len(support) else 0
    
    return metrics

def train_epoch(model, train_loader, optimizer, device, config, epoch, writer=None, logger=None):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    
    total_loss = 0.0
    all_predictions = []
    all_labels = []
    all_weights = []
    
    pbar = tqdm(train_loader, desc=f"Epoch {epoch+1} è®­ç»ƒ")
    
    for batch_idx, batch in enumerate(pbar):
        # ç§»åŠ¨åˆ°è®¾å¤‡
        sequences = batch['sequences'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        weights = batch['weights'].to(device) if config.task.get('use_weights', False) else None
        
        # å‰å‘ä¼ æ’­
        outputs = model(
            sequences=sequences,
            attention_mask=attention_mask,
            labels=labels,
            weights=weights
        )
        
        loss = outputs['loss']
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        if config.training.get('max_grad_norm', 0) > 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), config.training.max_grad_norm)
        
        optimizer.step()
        
        # ç»Ÿè®¡
        total_loss += loss.item()
        
        # é¢„æµ‹
        with torch.no_grad():
            logits = outputs['logits']
            predictions = torch.argmax(logits, dim=-1)
            
            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            if weights is not None:
                all_weights.extend(weights.cpu().numpy())
        
        # æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix({'loss': f"{loss.item():.4f}"})
        
        # è®°å½•åˆ°TensorBoard
        if writer:
            global_step = epoch * len(train_loader) + batch_idx
            writer.add_scalar('Train/Loss', loss.item(), global_step)
    
    # è®¡ç®—epochæŒ‡æ ‡
    avg_loss = total_loss / len(train_loader)
    metrics = calculate_metrics(all_predictions, all_labels, all_weights, config.task.num_classes)
    metrics['loss'] = avg_loss
    
    return metrics

def validate_model(model, val_loader, device, config, epoch, writer=None, logger=None):
    """éªŒè¯æ¨¡å‹"""
    model.eval()
    
    total_loss = 0.0
    all_predictions = []
    all_labels = []
    all_weights = []
    all_logits = []
    
    with torch.no_grad():
        pbar = tqdm(val_loader, desc="éªŒè¯")
        
        for batch in pbar:
            # ç§»åŠ¨åˆ°è®¾å¤‡
            sequences = batch['sequences'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            weights = batch['weights'].to(device) if config.task.get('use_weights', False) else None
            
            # å‰å‘ä¼ æ’­
            outputs = model(
                sequences=sequences,
                attention_mask=attention_mask,
                labels=labels,
                weights=weights
            )
            
            loss = outputs['loss']
            logits = outputs['logits']
            
            total_loss += loss.item()
            
            # é¢„æµ‹
            predictions = torch.argmax(logits, dim=-1)
            
            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_logits.extend(F.softmax(logits, dim=-1).cpu().numpy())
            if weights is not None:
                all_weights.extend(weights.cpu().numpy())
            
            pbar.set_postfix({'val_loss': f"{loss.item():.4f}"})
    
    # è®¡ç®—æŒ‡æ ‡
    avg_loss = total_loss / len(val_loader)
    metrics = calculate_metrics(all_predictions, all_labels, all_weights, config.task.num_classes)
    metrics['loss'] = avg_loss
    
    # è®¡ç®—AUCï¼ˆå¤šåˆ†ç±»ï¼‰
    try:
        all_logits = np.array(all_logits)
        all_labels_np = np.array(all_labels)
        if len(np.unique(all_labels_np)) > 2:
            # å¤šåˆ†ç±»AUC
            from sklearn.preprocessing import label_binarize
            y_test_bin = label_binarize(all_labels_np, classes=list(range(config.task.num_classes)))
            auc_scores = []
            for i in range(config.task.num_classes):
                if np.sum(y_test_bin[:, i]) > 0:  # ç¡®ä¿ç±»åˆ«å­˜åœ¨
                    auc = roc_auc_score(y_test_bin[:, i], all_logits[:, i])
                    auc_scores.append(auc)
            if auc_scores:
                metrics['auc_roc'] = np.mean(auc_scores)
    except Exception as e:
        if logger:
            logger.warning(f"è®¡ç®—AUCå¤±è´¥: {e}")
    
    # è®°å½•åˆ°TensorBoard
    if writer:
        writer.add_scalar('Val/Loss', avg_loss, epoch)
        writer.add_scalar('Val/Accuracy', metrics['accuracy'], epoch)
        writer.add_scalar('Val/F1', metrics['f1'], epoch)
        
        # æ··æ·†çŸ©é˜µ
        if 'confusion_matrix' in metrics:
            fig, ax = plt.subplots(figsize=(8, 6))
            sns.heatmap(metrics['confusion_matrix'], annot=True, fmt='d', ax=ax, 
                       xticklabels=config.task.class_names,
                       yticklabels=config.task.class_names)
            ax.set_xlabel('é¢„æµ‹')
            ax.set_ylabel('çœŸå®')
            ax.set_title(f'Epoch {epoch+1} æ··æ·†çŸ©é˜µ')
            writer.add_figure('Val/Confusion_Matrix', fig, epoch)
            plt.close(fig)
    
    return metrics

def main():
    parser = argparse.ArgumentParser(description='æŠ—èŒè‚½åˆ†ç±»è®­ç»ƒ')
    parser.add_argument('--config', type=str, default='configs/classification_config.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='outputs/classification',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--data_file', type=str, 
                       default='/home/qlyu/StructDiff/data/train/mulit_peptide_val.csv',
                       help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = OmegaConf.load(args.config)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—
    log_file = output_dir / f"training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    logger = logging.getLogger(__name__)
    
    logger.info("ğŸš€ å¼€å§‹æŠ—èŒè‚½åˆ†ç±»è®­ç»ƒ")
    logger.info(f"é…ç½®æ–‡ä»¶: {args.config}")
    logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
    logger.info(f"æ•°æ®æ–‡ä»¶: {args.data_file}")
    
    # å‡†å¤‡æ•°æ®
    logger.info("ğŸ“Š å‡†å¤‡æ•°æ®...")
    
    # å¦‚æœprocessedæ•°æ®ä¸å­˜åœ¨ï¼Œå…ˆå¤„ç†åŸå§‹æ•°æ®
    if not (Path("data/processed/train.csv").exists() and Path("data/processed/val.csv").exists()):
        logger.info("å¤„ç†åŸå§‹æ•°æ®...")
        from prepare_classification_data import prepare_classification_data
        train_path, val_path, test_path = prepare_classification_data(
            input_file=args.data_file,
            output_dir="data/processed"
        )
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ¨¡å‹
    logger.info("ğŸ”§ åˆ›å»ºæ¨¡å‹...")
    model = AMPClassifier(config).to(device)
    
    param_info = model.count_parameters()
    logger.info(f"æ¨¡å‹å‚æ•°: æ€»è®¡ {param_info['total']:,}, å¯è®­ç»ƒ {param_info['trainable']:,}")
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    logger.info("ğŸ“š åˆ›å»ºæ•°æ®é›†...")
    
    # è·å–tokenizer
    tokenizer = model.tokenizer
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = AMPDataset("data/processed/train.csv", tokenizer, config, is_training=True)
    val_dataset = AMPDataset("data/processed/val.csv", tokenizer, config, is_training=False)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    from functools import partial
    collate_fn_with_config = partial(collate_fn, tokenizer=tokenizer, config=config)
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.data.batch_size,
        shuffle=True,
        collate_fn=collate_fn_with_config,
        num_workers=config.data.get('num_workers', 4),
        pin_memory=config.data.get('pin_memory', True)
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.data.batch_size,
        shuffle=False,
        collate_fn=collate_fn_with_config,
        num_workers=config.data.get('num_workers', 4),
        pin_memory=config.data.get('pin_memory', True)
    )
    
    logger.info(f"è®­ç»ƒæ ·æœ¬: {len(train_dataset)}, éªŒè¯æ ·æœ¬: {len(val_dataset)}")
    logger.info(f"è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}, éªŒè¯æ‰¹æ¬¡: {len(val_loader)}")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.training.optimizer.lr,
        weight_decay=config.training.optimizer.weight_decay,
        betas=config.training.optimizer.betas
    )
    
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = None
    if config.training.get('scheduler'):
        if config.training.scheduler.name == 'cosine':
            from torch.optim.lr_scheduler import CosineAnnealingLR
            scheduler = CosineAnnealingLR(
                optimizer,
                T_max=config.training.num_epochs,
                eta_min=config.training.scheduler.min_lr
            )
    
    # TensorBoard
    writer = SummaryWriter(output_dir / "tensorboard")
    
    # è®­ç»ƒå¾ªç¯
    logger.info(f"ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ {config.training.num_epochs} epochs...")
    
    best_val_f1 = 0.0
    patience_counter = 0
    patience = config.training.get('early_stopping', {}).get('patience', 10)
    
    for epoch in range(config.training.num_epochs):
        logger.info(f"\nğŸ“… Epoch {epoch + 1}/{config.training.num_epochs}")
        
        # è®­ç»ƒ
        train_metrics = train_epoch(model, train_loader, optimizer, device, config, epoch, writer, logger)
        
        logger.info(f"è®­ç»ƒ - Loss: {train_metrics['loss']:.4f}, "
                   f"Acc: {train_metrics['accuracy']:.4f}, "
                   f"F1: {train_metrics['f1']:.4f}")
        
        # éªŒè¯
        if (epoch + 1) % config.training.get('validate_every', 1) == 0:
            val_metrics = validate_model(model, val_loader, device, config, epoch, writer, logger)
            
            logger.info(f"éªŒè¯ - Loss: {val_metrics['loss']:.4f}, "
                       f"Acc: {val_metrics['accuracy']:.4f}, "
                       f"F1: {val_metrics['f1']:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_metrics['f1'] > best_val_f1:
                best_val_f1 = val_metrics['f1']
                patience_counter = 0
                
                # ä¿å­˜æ¨¡å‹
                checkpoint = {
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'config': config,
                    'val_metrics': val_metrics,
                    'train_metrics': train_metrics
                }
                
                torch.save(checkpoint, output_dir / "best_model.pth")
                logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (F1: {best_val_f1:.4f})")
                
                # ä¿å­˜è¯¦ç»†ç»“æœ
                results = {
                    'epoch': epoch,
                    'train_metrics': {k: float(v) if isinstance(v, (int, float, np.number)) else str(v) 
                                    for k, v in train_metrics.items() if k != 'confusion_matrix'},
                    'val_metrics': {k: float(v) if isinstance(v, (int, float, np.number)) else str(v) 
                                  for k, v in val_metrics.items() if k != 'confusion_matrix'}
                }
                
                with open(output_dir / "best_results.json", 'w') as f:
                    json.dump(results, f, indent=2)
                    
            else:
                patience_counter += 1
                
            # æ—©åœ
            if patience_counter >= patience:
                logger.info(f"ğŸ›‘ æ—©åœ (patience: {patience})")
                break
        
        # å­¦ä¹ ç‡è°ƒåº¦
        if scheduler:
            scheduler.step()
        
        # æ¸…ç†å†…å­˜
        if epoch % 5 == 0:
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    # å…³é—­èµ„æº
    writer.close()
    
    logger.info("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    logger.info(f"ğŸ“Š æœ€ä½³F1åˆ†æ•°: {best_val_f1:.4f}")
    logger.info(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")

if __name__ == "__main__":
    main() 