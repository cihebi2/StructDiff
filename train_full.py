# train_full.py - å®Œæ•´çš„å¤§è§„æ¨¡StructDiffè®­ç»ƒè„šæœ¬
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler
from torch.utils.tensorboard import SummaryWriter
from omegaconf import OmegaConf
from tqdm import tqdm
import os
import sys
import gc
import json
import time
import logging
import argparse
from pathlib import Path
from datetime import datetime
import numpy as np
import random

# å¯¼å…¥ ESMFold è¡¥ä¸ä»¥ä¿®å¤å…¼å®¹æ€§é—®é¢˜
from fix_esmfold_patch import apply_esmfold_patch

from structdiff.models.structdiff import StructDiff
from structdiff.data.dataset import PeptideStructureDataset
from structdiff.data.collator import PeptideStructureCollator

def setup_logging(output_dir, rank=0):
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    if rank == 0:
        log_dir = Path(output_dir) / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        
        # é…ç½®æ—¥å¿—æ ¼å¼
        log_format = '%(asctime)s - %(levelname)s - %(message)s'
        log_file = log_dir / f"training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        # åŒæ—¶è¾“å‡ºåˆ°æ–‡ä»¶å’Œæ§åˆ¶å°
        logging.basicConfig(
            level=logging.INFO,
            format=log_format,
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler(sys.stdout)
            ]
        )
        
        return logging.getLogger(__name__)
    else:
        # éä¸»è¿›ç¨‹åªè¾“å‡ºåˆ°æ§åˆ¶å°
        logging.basicConfig(level=logging.WARNING)
        return logging.getLogger(__name__)

def setup_distributed():
    """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒ"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ["RANK"])
        world_size = int(os.environ['WORLD_SIZE'])
        gpu = int(os.environ['LOCAL_RANK'])
        
        torch.cuda.set_device(gpu)
        dist.init_process_group(backend='nccl', init_method='env://')
        
        return rank, world_size, gpu
    else:
        return 0, 1, 0

def setup_seed(seed):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def setup_esmfold_patch():
    """è®¾ç½® ESMFold è¡¥ä¸"""
    apply_esmfold_patch()

def clear_memory():
    """æ¸…ç† GPU å†…å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()

def move_to_device(obj, device):
    """é€’å½’åœ°å°†å¯¹è±¡ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
    if isinstance(obj, torch.Tensor):
        return obj.to(device)
    elif isinstance(obj, dict):
        return {k: move_to_device(v, device) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [move_to_device(item, device) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(move_to_device(item, device) for item in obj)
    else:
        return obj

def save_checkpoint(model, optimizer, scheduler, epoch, loss, config, output_dir, 
                   is_best=False, rank=0, is_distributed=False):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    if rank == 0:
        checkpoint_dir = Path(output_dir) / "checkpoints"
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # è·å–æ¨¡å‹çŠ¶æ€å­—å…¸
        if is_distributed:
            model_state_dict = model.module.state_dict()
        else:
            model_state_dict = model.state_dict()
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model_state_dict,
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
            'loss': loss,
            'config': config,
            'timestamp': datetime.now().isoformat()
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        latest_path = checkpoint_dir / "checkpoint_latest.pth"
        torch.save(checkpoint, latest_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = checkpoint_dir / "checkpoint_best.pth"
            torch.save(checkpoint, best_path)
        
        # ä¿å­˜å®šæœŸæ£€æŸ¥ç‚¹
        if epoch % 10 == 0:
            epoch_path = checkpoint_dir / f"checkpoint_epoch_{epoch}.pth"
            torch.save(checkpoint, epoch_path)
        
        return str(latest_path)
    return None

def load_checkpoint(model, optimizer, scheduler, checkpoint_path, rank=0, is_distributed=False):
    """åŠ è½½æ£€æŸ¥ç‚¹"""
    if os.path.exists(checkpoint_path):
        if rank == 0:
            print(f"æ­£åœ¨åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        # åŠ è½½æ¨¡å‹çŠ¶æ€
        if is_distributed:
            model.module.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint['model_state_dict'])
        
        # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # åŠ è½½è°ƒåº¦å™¨çŠ¶æ€
        if scheduler and checkpoint['scheduler_state_dict']:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        start_epoch = checkpoint['epoch'] + 1
        best_loss = checkpoint.get('loss', float('inf'))
        
        if rank == 0:
            print(f"âœ“ æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸï¼Œä» epoch {start_epoch} å¼€å§‹")
        
        return start_epoch, best_loss
    else:
        return 0, float('inf')

def create_optimizer(model, config):
    """åˆ›å»ºä¼˜åŒ–å™¨"""
    if config.training.optimizer.name.lower() == 'adamw':
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config.training.optimizer.lr,
            weight_decay=config.training.optimizer.weight_decay,
            betas=config.training.optimizer.get('betas', (0.9, 0.999)),
            eps=config.training.optimizer.get('eps', 1e-8)
        )
    elif config.training.optimizer.name.lower() == 'adam':
        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=config.training.optimizer.lr,
            weight_decay=config.training.optimizer.weight_decay
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {config.training.optimizer.name}")
    
    return optimizer

def create_scheduler(optimizer, config, total_steps):
    """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
    if not config.training.get('scheduler', None):
        return None
    
    scheduler_name = config.training.scheduler.name.lower()
    
    if scheduler_name == 'cosine':
        from torch.optim.lr_scheduler import CosineAnnealingLR
        scheduler = CosineAnnealingLR(
            optimizer,
            T_max=config.training.num_epochs,
            eta_min=config.training.scheduler.get('min_lr', 1e-6)
        )
    elif scheduler_name == 'cosine_warmup':
        from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
        scheduler = CosineAnnealingWarmRestarts(
            optimizer,
            T_0=config.training.scheduler.get('warmup_epochs', 10),
            T_mult=config.training.scheduler.get('restart_mult', 2),
            eta_min=config.training.scheduler.get('min_lr', 1e-6)
        )
    elif scheduler_name == 'linear_warmup':
        from transformers import get_linear_schedule_with_warmup
        warmup_steps = int(total_steps * config.training.scheduler.get('warmup_ratio', 0.1))
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=total_steps
        )
    else:
        return None
    
    return scheduler

def train_epoch(model, train_loader, optimizer, scheduler, device, config, 
                epoch, writer=None, rank=0, world_size=1, logger=None):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_loss = 0.0
    total_diffusion_loss = 0.0
    total_structure_loss = 0.0
    successful_batches = 0
    total_batches = len(train_loader)
    
    # åˆ†å¸ƒå¼é‡‡æ ·å™¨è®¾ç½®
    if hasattr(train_loader.sampler, 'set_epoch'):
        train_loader.sampler.set_epoch(epoch)
    
    # è¿›åº¦æ¡ï¼ˆä»…ä¸»è¿›ç¨‹æ˜¾ç¤ºï¼‰
    if rank == 0:
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1} è®­ç»ƒä¸­")
    else:
        pbar = train_loader
    
    optimizer.zero_grad()
    
    for batch_idx, batch_raw in enumerate(pbar):
        try:
            # æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
            batch = move_to_device(batch_raw, device)
            
            # æ£€æŸ¥å¿…è¦å­—æ®µ
            if 'sequences' not in batch or 'attention_mask' not in batch:
                if logger:
                    logger.warning(f"Batch {batch_idx} ç¼ºå°‘å¿…è¦å­—æ®µï¼Œè·³è¿‡")
                continue
            
            # é‡‡æ ·æ—¶é—´æ­¥
            batch_size = batch['sequences'].shape[0]
            timesteps = torch.randint(
                0, config.diffusion.num_timesteps,
                (batch_size,), device=device
            )
            
            # å‡†å¤‡ç»“æ„æ•°æ®
            structures = None
            if config.data.get('use_predicted_structures', False) and 'structures' in batch:
                structures = batch['structures']
            
            # å‰å‘ä¼ æ’­
            outputs = model(
                sequences=batch['sequences'],
                attention_mask=batch['attention_mask'],
                timesteps=timesteps,
                structures=structures,
                return_loss=True
            )
            
            # è·å¾—æŸå¤±
            loss = outputs['total_loss']
            diffusion_loss = outputs.get('diffusion_loss', torch.tensor(0.0))
            structure_loss = outputs.get('structure_loss', torch.tensor(0.0))
            
            # æ¢¯åº¦ç´¯ç§¯ç¼©æ”¾
            loss = loss / config.training.gradient_accumulation_steps
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦æ›´æ–°
            if (batch_idx + 1) % config.training.gradient_accumulation_steps == 0:
                # æ¢¯åº¦è£å‰ª
                if config.training.get('max_grad_norm', 0) > 0:
                    torch.nn.utils.clip_grad_norm_(
                        model.parameters(), 
                        config.training.max_grad_norm
                    )
                
                # ä¼˜åŒ–å™¨æ­¥éª¤
                optimizer.step()
                optimizer.zero_grad()
                
                # è°ƒåº¦å™¨æ­¥éª¤ï¼ˆå¦‚æœæ˜¯åŸºäºæ­¥éª¤çš„ï¼‰
                if scheduler and config.training.scheduler.get('step_based', False):
                    scheduler.step()
            
            # ç»Ÿè®¡æ›´æ–°
            actual_loss = loss.item() * config.training.gradient_accumulation_steps
            total_loss += actual_loss
            total_diffusion_loss += diffusion_loss.item()
            total_structure_loss += structure_loss.item()
            successful_batches += 1
            
            # æ›´æ–°è¿›åº¦æ¡ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
            if rank == 0:
                current_lr = optimizer.param_groups[0]['lr']
                pbar.set_postfix({
                    'loss': f"{actual_loss:.4f}",
                    'diff': f"{diffusion_loss.item():.4f}",
                    'struct': f"{structure_loss.item():.4f}",
                    'lr': f"{current_lr:.2e}",
                    'mem': f"{torch.cuda.memory_allocated() / 1e9:.1f}GB" if torch.cuda.is_available() else "CPU"
                })
            
            # è®°å½•åˆ°TensorBoard
            if writer and rank == 0:
                global_step = epoch * total_batches + batch_idx
                writer.add_scalar('Train/Loss_Total', actual_loss, global_step)
                writer.add_scalar('Train/Loss_Diffusion', diffusion_loss.item(), global_step)
                writer.add_scalar('Train/Loss_Structure', structure_loss.item(), global_step)
                writer.add_scalar('Train/Learning_Rate', optimizer.param_groups[0]['lr'], global_step)
                writer.add_scalar('Train/GPU_Memory_GB', torch.cuda.memory_allocated() / 1e9, global_step)
            
            # å®šæœŸæ¸…ç†å†…å­˜
            if batch_idx % 100 == 0:
                clear_memory()
                
        except Exception as e:
            if logger:
                logger.error(f"è®­ç»ƒæ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}")
            else:
                print(f"è®­ç»ƒæ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}")
            
            # æ¸…ç†å¹¶ç»§ç»­
            optimizer.zero_grad()
            clear_memory()
            continue
    
    # æœ€ç»ˆæ¢¯åº¦æ›´æ–°ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if total_batches % config.training.gradient_accumulation_steps != 0:
        if config.training.get('max_grad_norm', 0) > 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), config.training.max_grad_norm)
        optimizer.step()
        optimizer.zero_grad()
    
    # è°ƒåº¦å™¨æ­¥éª¤ï¼ˆå¦‚æœæ˜¯åŸºäºepochçš„ï¼‰
    if scheduler and not config.training.scheduler.get('step_based', False):
        scheduler.step()
    
    # è®¡ç®—å¹³å‡æŸå¤±
    if successful_batches > 0:
        avg_metrics = {
            'total_loss': total_loss / successful_batches,
            'diffusion_loss': total_diffusion_loss / successful_batches,
            'structure_loss': total_structure_loss / successful_batches,
            'successful_batches': successful_batches,
            'total_batches': total_batches
        }
    else:
        avg_metrics = {
            'total_loss': 0.0,
            'diffusion_loss': 0.0,
            'structure_loss': 0.0,
            'successful_batches': 0,
            'total_batches': total_batches
        }
    
    return avg_metrics

def validate_model(model, val_loader, device, config, epoch, writer=None, rank=0, logger=None):
    """éªŒè¯æ¨¡å‹"""
    model.eval()
    
    total_loss = 0.0
    total_diffusion_loss = 0.0
    total_structure_loss = 0.0
    successful_batches = 0
    
    with torch.no_grad():
        if rank == 0:
            pbar = tqdm(val_loader, desc="éªŒè¯ä¸­")
        else:
            pbar = val_loader
        
        for batch_idx, batch_raw in enumerate(pbar):
            try:
                # æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
                batch = move_to_device(batch_raw, device)
                
                # æ£€æŸ¥å¿…è¦å­—æ®µ
                if 'sequences' not in batch or 'attention_mask' not in batch:
                    continue
                
                # é‡‡æ ·æ—¶é—´æ­¥
                batch_size = batch['sequences'].shape[0]
                timesteps = torch.randint(
                    0, config.diffusion.num_timesteps,
                    (batch_size,), device=device
                )
                
                # å‡†å¤‡ç»“æ„æ•°æ®
                structures = None
                if config.data.get('use_predicted_structures', False) and 'structures' in batch:
                    structures = batch['structures']
                
                # å‰å‘ä¼ æ’­
                outputs = model(
                    sequences=batch['sequences'],
                    attention_mask=batch['attention_mask'],
                    timesteps=timesteps,
                    structures=structures,
                    return_loss=True
                )
                
                # è·å¾—æŸå¤±
                loss = outputs['total_loss']
                diffusion_loss = outputs.get('diffusion_loss', torch.tensor(0.0))
                structure_loss = outputs.get('structure_loss', torch.tensor(0.0))
                
                # ç»Ÿè®¡
                total_loss += loss.item()
                total_diffusion_loss += diffusion_loss.item()
                total_structure_loss += structure_loss.item()
                successful_batches += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                if rank == 0:
                    pbar.set_postfix({
                        'val_loss': f"{loss.item():.4f}",
                        'mem': f"{torch.cuda.memory_allocated() / 1e9:.1f}GB" if torch.cuda.is_available() else "CPU"
                    })
                
            except Exception as e:
                if logger:
                    logger.warning(f"éªŒè¯æ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}")
                clear_memory()
                continue
    
    # è®¡ç®—å¹³å‡æŸå¤±
    if successful_batches > 0:
        avg_metrics = {
            'total_loss': total_loss / successful_batches,
            'diffusion_loss': total_diffusion_loss / successful_batches,
            'structure_loss': total_structure_loss / successful_batches
        }
    else:
        avg_metrics = {
            'total_loss': float('inf'),
            'diffusion_loss': float('inf'),
            'structure_loss': float('inf')
        }
    
    # è®°å½•åˆ°TensorBoard
    if writer and rank == 0:
        writer.add_scalar('Val/Loss_Total', avg_metrics['total_loss'], epoch)
        writer.add_scalar('Val/Loss_Diffusion', avg_metrics['diffusion_loss'], epoch)
        writer.add_scalar('Val/Loss_Structure', avg_metrics['structure_loss'], epoch)
    
    return avg_metrics

def create_shared_esmfold(config, device, logger=None):
    """åˆ›å»ºå…±äº«çš„ESMFoldå®ä¾‹"""
    use_esmfold = config.model.structure_encoder.get('use_esmfold', False)
    use_structures = config.data.get('use_predicted_structures', False)
    
    shared_esmfold = None
    if use_esmfold and use_structures:
        if logger:
            logger.info("æ­£åœ¨åˆ›å»ºå…±äº«çš„ ESMFold å®ä¾‹...")
        try:
            from structdiff.models.esmfold_wrapper import ESMFoldWrapper
            shared_esmfold = ESMFoldWrapper(device=device)
            if shared_esmfold.available:
                if logger:
                    logger.info("âœ“ å…±äº« ESMFold å®ä¾‹åˆ›å»ºæˆåŠŸ")
                    if torch.cuda.is_available():
                        logger.info(f"ESMFold åŠ è½½å GPU å†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1e9:.1f}GB")
            else:
                if logger:
                    logger.warning("âŒ å…±äº« ESMFold å®ä¾‹åˆ›å»ºå¤±è´¥")
                shared_esmfold = None
        except Exception as e:
            if logger:
                logger.error(f"åˆ›å»ºå…±äº« ESMFold å®ä¾‹å¤±è´¥: {e}")
            shared_esmfold = None
    
    return shared_esmfold

def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='StructDiff å¤§è§„æ¨¡è®­ç»ƒè„šæœ¬')
    parser.add_argument('--config', type=str, default='configs/test_train.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='outputs',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--resume', type=str, default=None,
                       help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--local_rank', type=int, default=0,
                       help='æœ¬åœ°è¿›ç¨‹æ’å')
    args = parser.parse_args()
    
    # è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒ
    rank, world_size, gpu = setup_distributed()
    
    # è®¾ç½®è®¾å¤‡
    if torch.cuda.is_available():
        device = torch.device(f'cuda:{gpu}')
        torch.cuda.set_device(device)
    else:
        device = torch.device('cpu')
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(output_dir, rank)
    
    if rank == 0:
        logger.info(f"å¼€å§‹ StructDiff å¤§è§„æ¨¡è®­ç»ƒ")
        logger.info(f"é…ç½®æ–‡ä»¶: {args.config}")
        logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
        logger.info(f"åˆ†å¸ƒå¼è®¾ç½®: rank={rank}, world_size={world_size}")
    
    # åº”ç”¨ESMFoldè¡¥ä¸
    setup_esmfold_patch()
    
    # åŠ è½½é…ç½®
    config = OmegaConf.load(args.config)
    
    # è®¾ç½®éšæœºç§å­
    if config.get('seed', None):
        setup_seed(config.seed)
    
    # åˆ›å»ºå…±äº«ESMFoldå®ä¾‹
    shared_esmfold = create_shared_esmfold(config, device, logger)
    
    # åˆ›å»ºæ¨¡å‹
    if rank == 0:
        logger.info("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
    
    try:
        # ä¸´æ—¶ç¦ç”¨æ¨¡å‹å†…éƒ¨ESMFoldåŠ è½½ä»¥é¿å…é‡å¤
        original_use_esmfold = config.model.structure_encoder.get('use_esmfold', False)
        if shared_esmfold and shared_esmfold.available:
            config.model.structure_encoder.use_esmfold = False
        
        model = StructDiff(config).to(device)
        
        # æ¢å¤é…ç½®å¹¶è®¾ç½®å…±äº«å®ä¾‹
        config.model.structure_encoder.use_esmfold = original_use_esmfold
        if shared_esmfold and shared_esmfold.available:
            model.structure_encoder.esmfold = shared_esmfold
            model.structure_encoder.use_esmfold = True
        
        if rank == 0:
            logger.info(f"æ¨¡å‹å‚æ•°æ•°é‡: {model.count_parameters():,}")
            if torch.cuda.is_available():
                logger.info(f"æ¨¡å‹åŠ è½½å GPU å†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1e9:.1f}GB")
        
    except Exception as e:
        logger.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # åˆ†å¸ƒå¼åŒ…è£…
    is_distributed = world_size > 1
    if is_distributed:
        model = DDP(model, device_ids=[gpu], find_unused_parameters=True)
        if rank == 0:
            logger.info("âœ“ æ¨¡å‹å·²åŒ…è£…ä¸ºåˆ†å¸ƒå¼æ¨¡å‹")
    
    # åˆ›å»ºæ•°æ®é›†
    if rank == 0:
        logger.info("æ­£åœ¨åŠ è½½æ•°æ®é›†...")
    
    try:
        train_dataset = PeptideStructureDataset(
            config.data.train_path,
            config,
            is_training=True,
            shared_esmfold=shared_esmfold
        )
        
        val_dataset = None
        if os.path.exists(config.data.get('val_path', '')):
            val_dataset = PeptideStructureDataset(
                config.data.val_path,
                config,
                is_training=False,
                shared_esmfold=shared_esmfold
            )
        
        if rank == 0:
            logger.info(f"è®­ç»ƒæ•°æ®é›†å¤§å°: {len(train_dataset):,}")
            if val_dataset:
                logger.info(f"éªŒè¯æ•°æ®é›†å¤§å°: {len(val_dataset):,}")
        
    except Exception as e:
        logger.error(f"æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    collator = PeptideStructureCollator(config)
    
    # è®­ç»ƒæ•°æ®åŠ è½½å™¨
    if is_distributed:
        train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)
    else:
        train_sampler = None
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.training.batch_size,
        shuffle=(train_sampler is None),
        sampler=train_sampler,
        collate_fn=collator,
        num_workers=config.data.get('num_workers', 4),
        pin_memory=config.data.get('pin_memory', True),
        drop_last=True
    )
    
    # éªŒè¯æ•°æ®åŠ è½½å™¨
    val_loader = None
    if val_dataset:
        if is_distributed:
            val_sampler = DistributedSampler(val_dataset, num_replicas=world_size, rank=rank, shuffle=False)
        else:
            val_sampler = None
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=config.training.get('val_batch_size', config.training.batch_size),
            shuffle=False,
            sampler=val_sampler,
            collate_fn=collator,
            num_workers=config.data.get('num_workers', 4),
            pin_memory=config.data.get('pin_memory', True)
        )
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = create_optimizer(model, config)
    
    # åˆ›å»ºè°ƒåº¦å™¨
    total_steps = len(train_loader) * config.training.num_epochs // config.training.gradient_accumulation_steps
    scheduler = create_scheduler(optimizer, config, total_steps)
    
    # è®¾ç½®TensorBoardï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    writer = None
    if rank == 0:
        log_dir = output_dir / "tensorboard"
        writer = SummaryWriter(log_dir)
        logger.info(f"TensorBoard æ—¥å¿—ç›®å½•: {log_dir}")
    
    # åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    start_epoch = 0
    best_val_loss = float('inf')
    if args.resume:
        start_epoch, best_val_loss = load_checkpoint(
            model, optimizer, scheduler, args.resume, rank, is_distributed
        )
    
    # ä¿å­˜é…ç½®æ–‡ä»¶
    if rank == 0:
        config_save_path = output_dir / "config.yaml"
        OmegaConf.save(config, config_save_path)
        logger.info(f"é…ç½®æ–‡ä»¶å·²ä¿å­˜åˆ°: {config_save_path}")
    
    # è®­ç»ƒå¾ªç¯
    if rank == 0:
        logger.info(f"å¼€å§‹è®­ç»ƒ {config.training.num_epochs} ä¸ª epochs...")
        logger.info(f"æ‰¹é‡å¤§å°: {config.training.batch_size}")
        logger.info(f"æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {config.training.gradient_accumulation_steps}")
        logger.info(f"æœ‰æ•ˆæ‰¹é‡å¤§å°: {config.training.batch_size * config.training.gradient_accumulation_steps * world_size}")
        logger.info(f"æ€»è®­ç»ƒæ­¥æ•°: {total_steps}")
    
    for epoch in range(start_epoch, config.training.num_epochs):
        if rank == 0:
            logger.info(f"\nğŸš€ Epoch {epoch + 1}/{config.training.num_epochs}")
        
        # è®­ç»ƒ
        train_metrics = train_epoch(
            model, train_loader, optimizer, scheduler, device, config,
            epoch, writer, rank, world_size, logger
        )
        
        if rank == 0:
            logger.info(
                f"è®­ç»ƒæŸå¤± - æ€»è®¡: {train_metrics['total_loss']:.4f}, "
                f"æ‰©æ•£: {train_metrics['diffusion_loss']:.4f}, "
                f"ç»“æ„: {train_metrics['structure_loss']:.4f}, "
                f"æˆåŠŸæ‰¹æ¬¡: {train_metrics['successful_batches']}/{train_metrics['total_batches']}"
            )
        
        # éªŒè¯
        val_metrics = None
        if val_loader and (epoch + 1) % config.training.get('validate_every', 1) == 0:
            val_metrics = validate_model(model, val_loader, device, config, epoch, writer, rank, logger)
            
            if rank == 0:
                logger.info(
                    f"éªŒè¯æŸå¤± - æ€»è®¡: {val_metrics['total_loss']:.4f}, "
                    f"æ‰©æ•£: {val_metrics['diffusion_loss']:.4f}, "
                    f"ç»“æ„: {val_metrics['structure_loss']:.4f}"
                )
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        is_best = val_metrics and val_metrics['total_loss'] < best_val_loss
        if is_best:
            best_val_loss = val_metrics['total_loss']
        
        save_checkpoint(
            model, optimizer, scheduler, epoch,
            val_metrics['total_loss'] if val_metrics else train_metrics['total_loss'],
            config, output_dir, is_best, rank, is_distributed
        )
        
        # å®šæœŸæ¸…ç†å†…å­˜
        clear_memory()
    
    # å…³é—­èµ„æº
    if writer:
        writer.close()
    
    if is_distributed:
        dist.destroy_process_group()
    
    if rank == 0:
        logger.info("ğŸ‰ è®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    main() 