#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆçš„StructDiffè®­ç»ƒè„šæœ¬
åŸºäºæˆåŠŸçš„è°ƒè¯•ç»“æœï¼Œæ·»åŠ è¯¦ç»†çš„è¿›åº¦è¾“å‡ºå’Œé”™è¯¯å¤„ç†
"""

import os
import sys
import time
import torch
import numpy as np
from pathlib import Path
from omegaconf import OmegaConf
from torch.utils.data import DataLoader
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from structdiff.models.structdiff import StructDiff
from structdiff.diffusion.gaussian_diffusion import GaussianDiffusion
from structdiff.data.dataset import PeptideStructureDataset

def custom_collate_fn(batch):
    """è‡ªå®šä¹‰collateå‡½æ•°ï¼Œå¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—"""
    # è·å–æœ€å¤§é•¿åº¦
    max_len = max(item['sequences'].shape[0] for item in batch)
    
    # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
    sequences = []
    attention_masks = []
    labels = []
    
    for item in batch:
        seq = item['sequences']
        # æˆªæ–­æˆ–å¡«å……åˆ°å›ºå®šé•¿åº¦
        if seq.shape[0] > max_len:
            seq = seq[:max_len]
        elif seq.shape[0] < max_len:
            # ä½¿ç”¨pad_token_idå¡«å……ï¼ˆé€šå¸¸æ˜¯0ï¼‰
            pad_length = max_len - seq.shape[0]
            seq = torch.cat([seq, torch.zeros(pad_length, dtype=seq.dtype)], dim=0)
        
        sequences.append(seq)
        
        # åˆ›å»ºattention mask
        attention_mask = torch.ones(max_len, dtype=torch.bool)
        if item['sequences'].shape[0] < max_len:
            attention_mask[item['sequences'].shape[0]:] = False
        attention_masks.append(attention_mask)
        
        labels.append(item['label'])
    
    return {
        'sequences': torch.stack(sequences),
        'attention_mask': torch.stack(attention_masks),
        'peptide_type': torch.stack(labels)
    }

def create_model_config():
    """åˆ›å»ºç»è¿‡éªŒè¯çš„æ¨¡å‹é…ç½®"""
    return OmegaConf.create({
        "sequence_encoder": {
            "pretrained_model": "facebook/esm2_t6_8M_UR50D",
            "freeze_encoder": False,  # å…è®¸å¾®è°ƒ
            "use_lora": True,  # ä½¿ç”¨LoRA
            "lora_rank": 16,
            "lora_alpha": 32,
            "lora_dropout": 0.1
        },
        "structure_encoder": {
            "use_esmfold": False,  # å®Œå…¨ç¦ç”¨ç»“æ„ç‰¹å¾
            "hidden_dim": 256
        },
        "denoiser": {
            "hidden_dim": 320,  # åŒ¹é…ESM2_t6_8M
            "num_layers": 6,    # å¢åŠ å±‚æ•°
            "num_heads": 8,     # å¢åŠ æ³¨æ„åŠ›å¤´
            "dropout": 0.1,
            "use_cross_attention": False  # ç¦ç”¨ç»“æ„äº¤å‰æ³¨æ„åŠ›
        }
    })

def create_data_config():
    """åˆ›å»ºæ•°æ®é…ç½®"""
    return OmegaConf.create({
        "data": {
            "max_length": 50,
            "use_predicted_structures": False
        },
        "model": {
            "sequence_encoder": {
                "pretrained_model": "facebook/esm2_t6_8M_UR50D"
            },
            "structure_encoder": {
                "use_esmfold": False
            }
        }
    })

def train_epoch(model, dataloader, diffusion, optimizer, device, epoch, total_epochs):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0.0
    num_batches = len(dataloader)
    
    start_time = time.time()
    
    for batch_idx, batch in enumerate(dataloader):
        batch_start_time = time.time()
        
        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
        sequences = batch['sequences'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        conditions = {'peptide_type': batch['peptide_type'].to(device)}
        
        # è·å–åºåˆ—åµŒå…¥
        with torch.no_grad():
            embeddings = model.sequence_encoder(sequences, attention_mask)
            embeddings = embeddings.last_hidden_state[:, 1:-1, :]
        
        # å‰å‘æ‰©æ•£è¿‡ç¨‹ï¼šæ·»åŠ å™ªå£°
        batch_size, seq_len, hidden_dim = embeddings.shape
        timesteps = torch.randint(0, diffusion.num_timesteps, (batch_size,), device=device)
        noise = torch.randn_like(embeddings)
        noisy_embeddings = diffusion.q_sample(embeddings, timesteps, noise)
        
        # é¢„æµ‹å™ªå£°
        optimizer.zero_grad()
        predicted_noise = model.denoiser(
            noisy_embeddings=noisy_embeddings,
            timesteps=timesteps,
            attention_mask=attention_mask[:, 1:-1],
            structure_features=None,
            conditions=conditions
        )[0]
        
        # è®¡ç®—æŸå¤±
        loss = torch.nn.functional.mse_loss(predicted_noise, noise)
        
        # åå‘ä¼ æ’­
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        total_loss += loss.item()
        batch_time = time.time() - batch_start_time
        
        # å®æ—¶æ—¥å¿—è¾“å‡º
        if batch_idx % 10 == 0:
            elapsed_time = time.time() - start_time
            progress = (batch_idx + 1) / num_batches
            eta = elapsed_time / progress * (1 - progress) if progress > 0 else 0
            
            print(f"Epoch {epoch+1}/{total_epochs} | "
                  f"Batch {batch_idx+1}/{num_batches} ({progress*100:.1f}%) | "
                  f"Loss: {loss.item():.6f} | "
                  f"Time: {batch_time:.2f}s | "
                  f"ETA: {eta/60:.1f}min")
            sys.stdout.flush()
    
    avg_loss = total_loss / num_batches
    epoch_time = time.time() - start_time
    print(f"Epoch {epoch+1} å®Œæˆ | å¹³å‡æŸå¤±: {avg_loss:.6f} | æ—¶é—´: {epoch_time/60:.2f}åˆ†é’Ÿ")
    return avg_loss

def validate(model, dataloader, diffusion, device):
    """éªŒè¯æ¨¡å‹"""
    model.eval()
    total_loss = 0.0
    num_batches = len(dataloader)
    
    print("å¼€å§‹éªŒè¯...")
    with torch.no_grad():
        for batch_idx, batch in enumerate(dataloader):
            sequences = batch['sequences'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            conditions = {'peptide_type': batch['peptide_type'].to(device)}
            
            # è·å–åºåˆ—åµŒå…¥
            embeddings = model.sequence_encoder(sequences, attention_mask)
            embeddings = embeddings.last_hidden_state[:, 1:-1, :]
            
            # å‰å‘æ‰©æ•£
            batch_size, seq_len, hidden_dim = embeddings.shape
            timesteps = torch.randint(0, diffusion.num_timesteps, (batch_size,), device=device)
            noise = torch.randn_like(embeddings)
            noisy_embeddings = diffusion.q_sample(embeddings, timesteps, noise)
            
            # é¢„æµ‹å™ªå£°
            predicted_noise = model.denoiser(
                noisy_embeddings=noisy_embeddings,
                timesteps=timesteps,
                attention_mask=attention_mask[:, 1:-1],
                structure_features=None,
                conditions=conditions
            )[0]
            
            # è®¡ç®—æŸå¤±
            loss = torch.nn.functional.mse_loss(predicted_noise, noise)
            total_loss += loss.item()
            
            if batch_idx % 50 == 0:
                print(f"éªŒè¯è¿›åº¦: {batch_idx+1}/{num_batches}")
                sys.stdout.flush()
    
    avg_loss = total_loss / num_batches
    print(f"éªŒè¯å®Œæˆ | å¹³å‡æŸå¤±: {avg_loss:.6f}")
    return avg_loss

def main():
    parser = argparse.ArgumentParser(description='ä¿®å¤ç‰ˆStructDiffè®­ç»ƒ')
    parser.add_argument('--epochs', type=int, default=5, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=4, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=1e-5, help='å­¦ä¹ ç‡')
    parser.add_argument('--device', type=str, default='cuda', help='è®­ç»ƒè®¾å¤‡')
    parser.add_argument('--output-dir', type=str, default='./outputs/structdiff_fixed', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸš€ å¼€å§‹StructDiffè®­ç»ƒ (ä¿®å¤ç‰ˆ)")
    print("=" * 60)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    print(f"âœ“ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    print(f"âœ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    
    # åˆ›å»ºé…ç½®
    print("ğŸ“‹ åˆ›å»ºé…ç½®...")
    model_config = create_model_config()
    data_config = create_data_config()
    print("âœ“ é…ç½®åˆ›å»ºå®Œæˆ")
    
    # åˆ›å»ºæ•°æ®é›†
    print("ğŸ“Š åˆ›å»ºæ•°æ®é›†...")
    sys.stdout.flush()
    
    train_dataset = PeptideStructureDataset(
        data_path="data/processed/train.csv",
        config=data_config,
        is_training=True
    )
    
    val_dataset = PeptideStructureDataset(
        data_path="data/processed/val.csv", 
        config=data_config,
        is_training=False
    )
    
    print(f"âœ“ è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"âœ“ éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("ğŸ“¥ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=0,
        pin_memory=False,
        collate_fn=custom_collate_fn
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=False,
        collate_fn=custom_collate_fn
    )
    print(f"âœ“ æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼Œæ¯ä¸ªepoch {len(train_loader)} ä¸ªæ‰¹æ¬¡")
    
    # åˆ›å»ºæ¨¡å‹
    print("ğŸ§  åˆ›å»ºæ¨¡å‹...")
    sys.stdout.flush()
    model = StructDiff(model_config).to(device)
    print(f"âœ“ æ¨¡å‹åˆ›å»ºå®Œæˆï¼Œå‚æ•°æ•°é‡: {model.count_parameters():,}")
    
    # åˆ›å»ºæ‰©æ•£è¿‡ç¨‹
    print("ğŸŒ€ åˆ›å»ºæ‰©æ•£è¿‡ç¨‹...")
    diffusion = GaussianDiffusion(
        num_timesteps=1000,
        noise_schedule="cosine",
        beta_start=0.0001,
        beta_end=0.02
    )
    print("âœ“ æ‰©æ•£è¿‡ç¨‹åˆ›å»ºå®Œæˆ")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    print("âš™ï¸ åˆ›å»ºä¼˜åŒ–å™¨...")
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=args.lr,
        weight_decay=0.01
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=args.epochs, eta_min=1e-6
    )
    print(f"âœ“ ä¼˜åŒ–å™¨åˆ›å»ºå®Œæˆï¼Œå­¦ä¹ ç‡: {args.lr}")
    
    # è®­ç»ƒå¾ªç¯
    print("\n" + "=" * 60)
    print("ğŸ‹ï¸ å¼€å§‹è®­ç»ƒå¾ªç¯")
    print("=" * 60)
    
    best_val_loss = float('inf')
    
    for epoch in range(args.epochs):
        epoch_start_time = time.time()
        print(f"\nğŸ“ˆ Epoch {epoch+1}/{args.epochs}")
        print("-" * 40)
        
        # è®­ç»ƒ
        train_loss = train_epoch(model, train_loader, diffusion, optimizer, device, epoch, args.epochs)
        
        # éªŒè¯
        val_loss = validate(model, val_loader, diffusion, device)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        current_lr = scheduler.get_last_lr()[0]
        
        # è®°å½•
        epoch_time = time.time() - epoch_start_time
        print(f"\nğŸ“Š Epoch {epoch+1} æ€»ç»“:")
        print(f"  è®­ç»ƒæŸå¤±: {train_loss:.6f}")
        print(f"  éªŒè¯æŸå¤±: {val_loss:.6f}")
        print(f"  å­¦ä¹ ç‡: {current_lr:.2e}")
        print(f"  ç”¨æ—¶: {epoch_time/60:.2f} åˆ†é’Ÿ")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_loss,
                'val_loss': val_loss,
                'config': model_config
            }
            torch.save(checkpoint, os.path.join(args.output_dir, 'best_model.pt'))
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: {val_loss:.6f}")
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % 2 == 0:
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_loss,
                'val_loss': val_loss,
                'config': model_config
            }
            torch.save(checkpoint, os.path.join(args.output_dir, f'checkpoint_epoch_{epoch}.pt'))
        
        sys.stdout.flush()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ’¯ æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {args.output_dir}")
    print("=" * 60)

if __name__ == "__main__":
    main() 